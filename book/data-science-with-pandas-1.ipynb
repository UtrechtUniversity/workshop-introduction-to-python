{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Science with Pandas (part 1: Introduction and basic statistics)\"\n",
    "format: html\n",
    "execute: \n",
    "  enabled: true\n",
    "  error: true\n",
    "  freeze: auto\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analysis heavily depends on the characteristics of the chosen research subject, but we can still identify  main common data analysis steps as follows:\n",
    "- Define the problem or objective (research question);\n",
    "- Design/perform experiments and collect data;\n",
    "- Data Exploration and Visualization;\n",
    "- Data Cleaning and Preparation;\n",
    "- Data Analysis and Modelling;\n",
    "- Result Interpretation;\n",
    "- Reporting and Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While tasks like finding a relevant research question and interpreting data are challenges for the critical and analytical skills of the single researcher, all the tasks related to data manipulation, from data reduction and statistical analysis to modelling and visualization, require a systematic use of a quite large variety of software. No need to say that implementing such software yourself from scratch would require a huge amount of time, without even mentioning all the issues related to software mantainance. Luckily for us, people have already devoloped software with specific tools for data analysis. These tools are organised into *packages* or *libraries*, they usually are openly available (according to the programming language you use), and they keep to be mantained and developed by a huge community of users. In particular, for python programmers, the most popular data analysis library is Pandas.\n",
    "\n",
    "The python library <a url=\"https://pandas.pydata.org/about/\">**Pandas**</a> is a popular open-source data analysis and data manupulation library for Python that started to be developed in 2008. The library is based on two main data structures: *Series* (1D, similar to numpy arrays) and *DataFrame* (2D labeled arrays). Why shall we use Series and DataFrame when we can work with numpy arrays and matrices? Because Pandas has been designed specifically for data manipulation and analysis tasks, providing tools and functions for data cleaning, data transformation, data aggregation, and data visualization, among other data-related tasks. Adopting Pandas, we can run through all the main technical tasks in our data analysis pipeline mainly using a single python library. \n",
    "\n",
    "Pandas is widely used in data science, machine learning, and data analysis tasks, as it provides powerful tools for data handling and manipulation, making it easier to work with a large variety of data. Furthermore, it well integrates with other Python libraries for data analysis, machine learning, and statistical analysis, such as NumPy, Scikit-Learn, and StatsModels. \n",
    "\n",
    "Overall, Pandas is a powerful and flexible library for data analysis and manipulation in Python, widely used in various domains including data science, finance, business, and research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will explore the main features of Pandas going through the main steps of standard data analysis. For this purpose, we will be using data from the <a url=\"https://figshare.com/articles/dataset/Portal_Project_Teaching_Database/1314459\">Portal Project Teaching Database</a>: real world example of life-history, population, and ecological data and, occasionally, small ad hoc dataset to exaplain DataFrame operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start our journey into Pandas functionalities, there are some preliminary operations to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas library is not native of python (it is not automatically loaded when you start running python), it needs to be installed and loaded. Assuming you already installed it, let's start importing the Pandas library and checking our installed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path of our data files will be specified relatively to the main project folder, so,  in order to properly upload our dataset, it's important to check we are working in there. In order to do that, we will load another library, os, containing all sort of tools to interact with our operating system. The function ```os.getcwd()```, in particular, returns the current working directory (cwd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the current local directory is <local_dir>/workshop-introduction-to-python, where <local_dir> is whatever directory you chose to download and unzip the course material, you are in the right place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the relative path of our data into a variable and let's check if the data file actually exists using the function ```os.path.exists()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_file = '../course_materials/data/surveys.csv'\n",
    "print(os.path.exists(data_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the result is True, we are all set up to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data and quick look (data exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first operation we will perform is loading our data into a Pandas **DataFrame** (from now on, only DataFrame). A DataFrame is a Pandas *core object*, i.e. a key component that provides the primary functionality or core features of the entire library. We can convert our data into DataFrame using Pandas functions, they can read a quite large variety of formats like the *comma-separated values* (CSV) file format. In CSV files data values are separated by a comma (\",\"), but files where values are separated by a semicolon (\";\"), space (\" \") or tab (\"\\t\") can also have a .csv extention. Once specified the spacer, Pandas can read all these files with the function ```pd.read_csv()```. The default spacer is a comma, so in our specific case there is no need to specify the separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df = pd.read_csv(data_file)\n",
    "\n",
    "print(type(data_file))\n",
    "print(type(surveys_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable ```data_file``` is a string (a word) specifying the location of our file. We use it as the first argument of the function ```pd.read_csv()``` that, indeed, reads the file and returns a DataFrame. In Jupyter Notebook or Jupyter Lab you can visualise the DataFrame simply writing its name in a code cell and running the cell (in the same way you would display the value of any variable). Let's have a look at our just created DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the DataFrame we can finally understand what a DataFrame actually is: a 2-dimensional data structure storing different types of variables in columns. Ever DataFrame has an Index (starting from 0) referring to the rows of the table. Columns can be addressed either by their names or by their index, where 0 corresponds to the very first column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter does something else for us, it does not show the entire DataFrame as it is too big, instead it shows the first and last 5 rows separated by ... . Simply typing the name of our DataFrame in Jupyter is a great way to start looking at how our data looks like, get familiar with all the columns names, values, and type, and, most importantly, to check for the presence of the infamous NaN (not a number), as these not numeric values are the worst enemies of data analysis scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TRY IT YOURSELF:</b> Type the following commands and check out the outputs. Can you tell what each command does? What is the difference between commands with and without parenthesis?\n",
    "</div>\n",
    "\n",
    "```python\n",
    "surveys_df.shape\n",
    "surveys_df.columns\n",
    "surveys_df.index\n",
    "surveys_df.dtypes\n",
    "surveys_df.head(<try_various_integers_here>)\n",
    "surveys_df.tail(<try_various_integers_here>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to perform some basic statistics on our dataset. There is nothing to worry about, Pandas has been specifucally designed for data analysis, so Pandas datastructures have plenty of attributes and methods for our needs (you can always visualise detailed information about attributes and methods of any python object running ```help(<object>)```). Let's first visualise all the column names in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(surveys_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the loop statements we learned about this morning to print the DataFrame column one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for column in surveys_df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the column *weight* in our DataFrame and let's run some statistics on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = surveys_df['weight']\n",
    "print(type(weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*weight* is another Pandas core object, a **Series**. You can think at a Series as a python list, or numpy array, with extra powers. One of these extra powers allows us to make a simple plot, just to have a first glance at the data. As this is the first plot of this session, we specify the command ```%matplotlib inline```, in this way jupyter lab or notebook will print the plot in the current notebook instead of opening a new window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "weight.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice how easy it was to obtain a summary plot of a column of our DataFrame? We can repeat the same for every column with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df['hindfoot_length'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TRY IT YOURSELF:</b> Perform some basic statistics on the weight column. Can you tell what each method listed below does? Look at our explorative plot, does the statistics make sense?\n",
    "</div>\n",
    "\n",
    "```python\n",
    "weight.min()\n",
    "weight.max()\n",
    "weight.mean()\n",
    "weight.std()\n",
    "weight.counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each Series we have, instead of running the methods above one by one, we can obtain a statistical summary using the method ```.describe()```. Let's get a statistical summary for the weight column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.describe()``` is just one of the many Series and DataFrame methods. There are methods to remove duplicates, to sort values in a particular order, to filter data according to custom criteria, etc (you can always check the Pandas documentation for more information). One of these methods, ```.nunique()```, returns the number of unique elements in a Series. Comparing this number with the length of the Series allows us to check if all the values of a column are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(surveys_df['plot_id']))\n",
    "print(surveys_df['plot_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the column plot_id has only 24 unique elements, while the total number of elements (computed applying the function ```len()```) is 35549. We definetively have repeated values in that column. Checking if there are repeating elements in a DataFrame column is one of the most important operation in data analysis and introduces the next topic of this session: grouping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ef4342b9cd1753b48d34a4dec30b358168f7f31d59fdf5ff32eb29bf3d04187b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
