{
 "cells": [
  {
   "cell_type": "raw",
   "id": "14f401b8-2d1d-4a01-a3c8-ed2c59e66d29",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)\"\n",
    "format: html\n",
    "execute:   \n",
    "  enabled: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd23ed0",
   "metadata": {},
   "source": [
    "## Recap: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "surveys_df = pd.read_csv('../surveys.csv')\n",
    "surveys_df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39194cc3",
   "metadata": {},
   "source": [
    "We often want to calculate summary statistics grouped by subsets or attributes within fields of our data. For example, we might want to calculate the average weight of all individuals per site.\n",
    "\n",
    "For exampe we want to know how heavy our samples are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e3f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['weight'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643637d8",
   "metadata": {},
   "source": [
    "We can also extract one specific metric if we wish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['weight'].min()\n",
    "surveys_df['weight'].max()\n",
    "surveys_df['weight'].mean()\n",
    "surveys_df['weight'].std()\n",
    "surveys_df['weight'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef65745",
   "metadata": {},
   "source": [
    "Now assume we want to get those statistics for the two groups \"male\" and \"female\".\n",
    "If we want to summarize data by one or more variables, we can use Pandas’ .groupby method. Once we’ve created a groupby DataFrame, we can quickly calculate summary statistics by a group of our choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb8311",
   "metadata": {},
   "source": [
    "## Selecting data using column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8f0b6",
   "metadata": {},
   "source": [
    "In the morning session (link) we saw how to get specific values from dictionaries using keys. We can do the same with dataframes, in fact we have already accessed the values in a column by the column name. In this section we will discover how to select values, slices of data and subsets of a dataframe.\n",
    "There are two ways of selecting columns, we have already used the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f57013",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['species_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df.species_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8bbe52",
   "metadata": {},
   "source": [
    "How can we now create a dataframe that only consists of the two columns *plot_id* and *species_id*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df[['plot_id', 'species_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd34ed",
   "metadata": {},
   "source": [
    "Why the double *[[..]]*? What is the difference between `surveys_df['plot_id']` and `surveys_df[['plot_id']]`? Let us have a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba845b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(surveys_df['plot_id']))\n",
    "print(type(surveys_df[['plot_id']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751ae41",
   "metadata": {},
   "source": [
    "The dataframe is organised as a dictionary with the column names as keys for column values and row numbers as keys for the values stored in a row. `surveys_df['plot_id']` will give us the value behind the key *plot_id*, in our case the series of numbers. When we ask for the values behind *plot_id* **and** *species_id* we need to give the dataframe a list of column names like we did with `surveys_df[['plot_id', 'species_id']]`.\n",
    "When we pass a list of column names to a dataframe, Pandas will execute for us the following code so that we do not have to worry about that any longer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93707720",
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = surveys_df['plot_id']\n",
    "col2 = surveys_df['species_id']\n",
    "aggregatedData = pd.DataFrame(dict(col1 = col1, col2 = col2))\n",
    "aggregatedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcb581f",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Swap the order of column names in `surveys_df[['plot_id', 'species_id']]`\n",
    "- Repeat one of the column names like `surveys_df[['plot_id', 'plot_id', 'species_id']]`\n",
    "How does the results look like and why?\n",
    "- Which error occurrs in `surveys_df['plot_id', 'species_id']` and why?\n",
    "- Which error occurrs in `surveys_df['speciess']`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950cf64",
   "metadata": {},
   "source": [
    "## Slicing subsets of rows\n",
    "Slicing using the `[]` operator selects a set of rows and/or columns from a DataFrame. To slice out a set of rows, you use the following syntax: `data[start:stop]`. When slicing in pandas the start bound is included in the output. The stop bound is one step BEYOND the row you want to select. So if you want to select rows 0, 1 and 2 your code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4725da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "surveys_df[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f07738",
   "metadata": {},
   "source": [
    "We can select specific ranges of our data in both the row and column directions using either label or integer-based indexing. The respective functions for that are called `loc` (label-based indexing) and `iloc` (integer-based indexing).\n",
    "\n",
    "Let's have a look at `iloc` first. In the example below we select the first three entries and the columns month, day and year (columns 2, 3 and 4 if we start counting at 1), like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc[row slicing, column slicing]\n",
    "surveys_df.iloc[0:3, 1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17168c31",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Remember:</b> In Python integer indexing starts with 0 and, when slicing using a continous range of indices, data corresponding to the last index is NOT included.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3121a6",
   "metadata": {},
   "source": [
    "We can achieve the same with the function `loc`. Here we need to know the names of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3612540",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df.loc[0:3, ['month', 'day', 'year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1144b",
   "metadata": {},
   "source": [
    "And there is a third way: We first select the columns by their names as in the previous section and then select the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e4a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df[['month', 'day', 'year']][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c31b9",
   "metadata": {},
   "source": [
    "Let us further explore the `loc` and  `iloc` functions as they are more powerful. Have a look at the examples below and predict their outcome before hitting enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b77914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns for rows of index values 0 and 10\n",
    "surveys_df.loc[[0, 10], :]\n",
    "\n",
    "# What does this do?\n",
    "surveys_df.loc[0, ['species_id', 'plot_id', 'weight']]\n",
    "\n",
    "# What happens when you type the code below?\n",
    "surveys_df.loc[[0, 10, 35549], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffcd6f",
   "metadata": {},
   "source": [
    "We can also extract single values from our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.iloc[row, column]\n",
    "surveys_df.iloc[2, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b576b",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "What happens when you execute:\n",
    "- `surveys_df[0:1]`\n",
    "- `surveys_df[:4]`\n",
    "- `surveys_df[:-1]`\n",
    "\n",
    "What happens when you call:\n",
    "- `surveys_df.iloc[0:4, 1:4]`\n",
    "- `surveys_df.loc[0:4, 1:4]`\n",
    "How are the two commands different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b4c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebeacab0",
   "metadata": {},
   "source": [
    "### Summary: Selecting slices, rows and columns\n",
    "In the first two methods we extract the column specifying its name. The third method is substantially identical to the first one as the 6th (index 5) element of the Series ```surveys_df.columns``` is species_id. The fourth method uses the method ```iloc``` to select *all* the rows of the 6th column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6958e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#By name\n",
    "# --------------------------------------\n",
    "#Method1\n",
    "plot_id_1 = surveys_df['species_id']\n",
    "\n",
    "#Method2\n",
    "plot_id_2 = surveys_df.species_id\n",
    "# --------------------------------------\n",
    "\n",
    "#By location\n",
    "# --------------------------------------\n",
    "#Method3\n",
    "plot_id_3 = surveys_df[surveys_df.columns[5]]\n",
    "\n",
    "#Method4\n",
    "plot_id_4 = surveys_df.iloc[:,5]\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc54c12",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TRY IT YOURSELF:</b> What happens when you call the following commands? How are the two commands different?\n",
    "</div>\n",
    "\n",
    "- ```surveys_df.iloc[0:4, 1:4]```;\n",
    "- ```surveys_df.loc[0:4, 1:4]```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e35fa",
   "metadata": {},
   "source": [
    "### Subsetting Data according to user-defined criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee3780",
   "metadata": {},
   "source": [
    "We can extract subsets of our DataFrame following the general syntax ```data_frame[<condition_on_data>]``` <condition_on_data> is a conditional statement on the DataFrame content itself. You may think at the conditional statement as a question or query you ask to your DataFrame. Here there are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af48aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the data collected in the year 2002?\n",
    "surveys_df[surveys_df.year == 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21c366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What are the data NOT collected in the year 2002?\n",
    "surveys_df[surveys_df.year != 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcf0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What are the data NOT collected in the year 2002? (different syntax)\n",
    "surveys_df[~(surveys_df.year == 2002)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597ccce",
   "metadata": {},
   "source": [
    "Our filtering conditions may be very specific, they can target different columns in the DataFrame, and they can be combined using the logical operator \"&\" which means **and**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f3290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What are the data collected between 2000 and 2002 on female species?\n",
    "surveys_df[(surveys_df.year >= 2000) & (surveys_df.year <= 2002) & (surveys_df.sex == 'F')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf34282",
   "metadata": {},
   "source": [
    "We have also an operator for **or**.\n",
    "Below we filter for rows with collected data on female species in the year 2000 or 2002.\n",
    "\"Give me all data where sex is Female and data is collected in 2000 or 2002\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df[(surveys_df.sex == 'F') & ((surveys_df.year >= 2000) | (surveys_df.year <= 2002))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f22b518",
   "metadata": {},
   "source": [
    "**Note** that logical operators sometimes need to be grouped with `()`. The operator **and** binds the two statements around it stronger than the operator **or**.\n",
    "In our example if we would leave out the extra `()` we would get all rows of samples that are female and gathered in 2000 plus all samples that were collected in 2002 (irrespective of whether they are femal, male or not set).\n",
    "\n",
    "\n",
    "Try what happens if you leave out the extra set of `()` in the statement above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ecdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41ec961e",
   "metadata": {},
   "source": [
    "The method ```isin()``` allows to specify a range of \"permitted\" values for a certain column. Here it follows another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013eb804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df[(surveys_df.year == 2000) & (surveys_df.sex == 'F') & (surveys_df.month.isin([1,3,4]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12cdaf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TRY IT YOURSELF:</b> \n",
    "    <ol>\n",
    "    <li> Create a new DataFrame that only contains observations with sex values that are not female or male. Print the number of rows in this new DataFrame. Verify the result by comparing the number of rows in the new DataFrame with the number of rows in the surveys DataFrame where sex is NaN (hint: there is a function isnull).</li>\n",
    "    <li>Create a new DataFrame that contains only observations that are of sex male or female and where weight values are greater than 0.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd42879",
   "metadata": {},
   "source": [
    "## DataFrame Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf67fe",
   "metadata": {},
   "source": [
    "A simple exploration of our DataFrame showed us that there are columns full of invalid values (NaN). One of the most important preliminary operations of data analysis is cleaning your data set, i.e. \"getting rid\" of non numerical values. Now that we mastered selecting, slicing, and subsetting, we can easily clean our DataFrame with few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any invalid values in the weight column?\n",
    "n_tot = len(surveys_df)\n",
    "n_null_weight = len(surveys_df[pd.isnull(surveys_df.weight)])\n",
    "n_pos_weight  = len(surveys_df[surveys_df.weight > 0])\n",
    "\n",
    "print('Total number of rows:',n_tot)\n",
    "print('Number of null weight rows:',n_null_weight)\n",
    "print('Number of positive weight rows:',n_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc6128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49c83573",
   "metadata": {},
   "source": [
    "As you can see, out of 35549 weight measurements, 3266 are not usable. The remaining 32283 values are positive, so usable, values. What happens if we compute the mean weight ignoring the fact that there are not numeric values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb5d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ave_weight = surveys_df.weight.mean()\n",
    "print(ave_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a09567e",
   "metadata": {},
   "source": [
    "A smooth run, without errors or warnings. As we said several times, Pandas is a library designed for data analysis and when performing data analysis it is very common to deal with not numeric values. In particular, the ```.mean()``` method has an argument called *skipna* that when set TRUE (default value, so we do not need to specify it) excludes NaN values. This means that, in this case, Pandas simply ignores whatever it is not numeric and it performs computations only on numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6b511",
   "metadata": {},
   "source": [
    "If we are not happy with Pandas default behaviour, we can manually decide which value to assigni to NA/null values. One possible choice is setting them to zero. To do that, we just need to apply the method ```.fillna(<value>)```, where <value> is the number we want to substitute to the NA/null value (in our case, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_weight1 = surveys_df.weight.fillna(0)\n",
    "cleaned_weight_ave1 = cleaned_weight1.mean()\n",
    "print(cleaned_weight_ave1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8bad6",
   "metadata": {},
   "source": [
    "You probably noticed that compared to our previous mean computation, the result is different. This is because the mean is now computed on data with many more zeros compared to the previous one and, as a result, the value of the computed mean is smaller.\n",
    "Conscious of this problem, we may now choose a more appropriate value to \"fill\" our NaN values. How about we use the \"clean\" mean of our first computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f46b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_weight2 = surveys_df.weight.fillna(surveys_df.weight.mean())\n",
    "cleaned_weight_ave2 = cleaned_weight2.mean()\n",
    "print(cleaned_weight_ave2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c0be8",
   "metadata": {},
   "source": [
    "This time we obtain exactly the same result of our first computation, this is because we substituted the NA/null values with a mean computed excluding the NA/null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b321b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TRY IT YOURSELF:</b> Compute the average weight of data after having cleaned the weight and the sex column.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8ba70-5aba-4c52-96e4-f4cda4c47268",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f430af",
   "metadata": {},
   "source": [
    "We often want to calculate summary statistics grouped by subsets or attributes within fields of our data. For example, we might want to calculate the average weight of all individuals per site.\n",
    "\n",
    "We can calculate basic statistics for all records in a single column using the syntax below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['weight'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b283e86",
   "metadata": {},
   "source": [
    "We can achieve the same with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['weight'].min()\n",
    "surveys_df['weight'].max()\n",
    "surveys_df['weight'].mean()\n",
    "surveys_df['weight'].std()\n",
    "surveys_df['weight'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a12270",
   "metadata": {},
   "source": [
    "But if we want to summarize by one or more variables, for example sex, we can use Pandas’ `.groupby` method. Once we’ve created a groupby DataFrame, we can quickly calculate summary statistics by a group of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169df8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = surveys_df.groupby('sex')\n",
    "grouped_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb649e4",
   "metadata": {},
   "source": [
    "The output is a bit overwhelming. Let's just have a look at one statistical value, the mean, to understand what is happening here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f88c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b0c18",
   "metadata": {},
   "source": [
    "We see that the data is divided into two groups, one group where the value in the column *sex* equals \"F\" and another group where the value in the column *sex* equals \"M\". The statistics is then calculated for all samples in that specific group for each of the columns in the dataframe. Note that samples annotated with sex equals NaN and column values with NaN are left out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb33c2e",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Let's see in which plots animals get more food. Calculate the average weight per plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733aba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = surveys_df.groupby(...)\n",
    "grouped_data[...].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71794bfb",
   "metadata": {},
   "source": [
    "## Structure of a groupby object\n",
    "We can investigate which rows are assigned to which group as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5920fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(grouped_data.groups)) # dictionary\n",
    "print(\"Plot ids: \", grouped_data.groups.keys()) # keys are the unique values of the column we grouped by\n",
    "print(\"Rows belonging to plot id \", 1, \": \", grouped_data.groups[1]) # values are row indexes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9e651",
   "metadata": {},
   "source": [
    "## Grouping by multiple columns\n",
    "Now let's have a look at a more complex grouping example. We want an overview statistics of the weight of all females and males by plot id. So in fact we want to group by *sex* and by *plot_id* at the same time.\n",
    "\n",
    "This will give us exactly 48 groups for our survey data:\n",
    "- female, plot id = 1\n",
    "- female, plot id = 2\n",
    "- ...\n",
    "- female, plot id = 24\n",
    "- male, plot id = 1\n",
    "- ...\n",
    "- male, plot id = 24\n",
    "\n",
    "Why 48 groups? We have 24 unique values for *plot_id*. Per plot we have two groups of samples, female and male. Hence, the grouping returns 48 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4525cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = surveys_df.groupby(['sex', 'plot_id'])\n",
    "grouped_data[\"weight\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccb9da",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Investigate the group keys and row indexes for this more complex grouping example. \n",
    "Why are there more than 48 groups?\n",
    "What happened to the third group and why dos it not turn up in our statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee674ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15193645",
   "metadata": {},
   "source": [
    "## Counting and plotting\n",
    "Another very useful outcome of grouping is the possibility of performing selective counting. For example, let's see how to count the number of records per species. We just need to remember that each species has a unique ID and that records are identified by another ID stored in the column record ID. We will first group our data according to the species ID and then, for each group, we will count the number of records. Several consecutive operations that, once again, Pandas allows us to execute in a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83034cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "species_counts = surveys_df.groupby('species_id')['record_id'].count()\n",
    "print(type(grouped_species_counts))\n",
    "species_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401bd39",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can also plot the information for better overview. We will learn more about plotting after the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e1d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "species_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c925fa",
   "metadata": {},
   "source": [
    "## Summary grouping\n",
    "Grouping is one of the most common operation in data analysis. Data often consists of different measurements on the same samples. In many cases we are not only interested in one particular measurement but in the cross product of measurements. In the picture below we labeled samples with green lines, blue dots and red lines. We are now interested how these three different groups relate to each other given the all other measurements in the dataframe. Pandas' groupby function gives us the means to compare these three groups with several built-in statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaad2c1-a90f-43b1-9950-948209792458",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Grouping sketch](pictures/grouping.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1ab75",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Would it make sense to group our data frame by the column *weight*? Why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
