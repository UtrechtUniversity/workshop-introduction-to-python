{
 "cells": [
  {
   "cell_type": "raw",
   "id": "14f401b8-2d1d-4a01-a3c8-ed2c59e66d29",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)\"\n",
    "format: html\n",
    "execute:   \n",
    "  enabled: true\n",
    "  error: true\n",
    "  freeze: auto\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd23ed0",
   "metadata": {},
   "source": [
    "## Recap: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "surveys_df = pd.read_csv('../course_materials/data/surveys.csv')\n",
    "surveys_df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39194cc3",
   "metadata": {},
   "source": [
    "In addition to learning about characteristics of our dataset as a whole, we may be interested in analyzing parts (subsets) of our data.\n",
    "For exampe we want to know how heavy our samples are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e3f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['weight'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643637d8",
   "metadata": {},
   "source": [
    "We can also extract one specific metric if we wish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['weight'].min()\n",
    "surveys_df['weight'].max()\n",
    "surveys_df['weight'].mean()\n",
    "surveys_df['weight'].std()\n",
    "surveys_df['weight'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb8311",
   "metadata": {},
   "source": [
    "## Selecting data using column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8f0b6",
   "metadata": {},
   "source": [
    "In the [morning session](introduction_to_python_3.ipynb) we saw how to get specific values from dictionaries using keys. We can do the same with DataFrames, in fact we have already accessed the values in a column by the column name. In this section we will discover how to select values, slices of data and subsets of a DataFrame.\n",
    "There are two ways of selecting columns, we have already used the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f57013",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['species_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df.species_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8bbe52",
   "metadata": {},
   "source": [
    "How can we now create a DataFrame that only consists of the two columns *plot_id* and *species_id*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df[['plot_id', 'species_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd34ed",
   "metadata": {},
   "source": [
    "Why the double *[[..]]*? What is the difference between `surveys_df['plot_id']` and `surveys_df[['plot_id']]`? Let us have a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba845b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(surveys_df['plot_id']))\n",
    "print(type(surveys_df[['plot_id']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751ae41",
   "metadata": {},
   "source": [
    "The DataFrame is organised as a dictionary with the column names as keys and row numbers as keys for the values stored in a row. `surveys_df['plot_id']` will give us the value behind the key *plot_id*, in our case the series of numbers. When we ask for the values behind *plot_id* **and** *species_id* we need to give the DataFrame a *list* of column names like we did with `surveys_df[['plot_id', 'species_id']]`.\n",
    "When we pass a list of column names to a DataFrame, Pandas will execute for us the following code so that we do not have to worry about that any longer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93707720",
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = surveys_df['plot_id']\n",
    "col2 = surveys_df['species_id']\n",
    "aggregatedData = pd.DataFrame(dict(col1 = col1, col2 = col2))\n",
    "aggregatedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950cf64",
   "metadata": {},
   "source": [
    "## Slicing subsets of rows\n",
    "Slicing using the `[]` operator selects a set of rows and/or columns from a DataFrame. To slice out a set of rows, you use the following syntax: `data[start:stop]`. When slicing in pandas the start bound is included in the output. The stop bound is not included. The slicing stops _before_ the stop bound.\n",
    "So if you want to select rows 0, 1 and 2 your code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4725da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "surveys_df[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f07738",
   "metadata": {},
   "source": [
    "We can select specific ranges of our data in both the row and column directions using either label or integer-based indexing. The respective functions for that are called `loc` (label-based indexing) and `iloc` (integer-based indexing).\n",
    "\n",
    "Let's have a look at `iloc` first. where we use the index of a row and/or column to select it. In the example below we select the first three entries and the columns month, day and year (the second, third and fourth column, remember indexing starts at 0 on Python). The first range of numbers selects the rows, the second the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc[row slicing, column slicing]\n",
    "surveys_df.iloc[0:3, 1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3121a6",
   "metadata": {},
   "source": [
    "We can achieve the same with the function `loc`, only instead of column indices, we use the column labels this time. So, we need to know the names of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3612540",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df.loc[0:3, ['month', 'day', 'year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1144b",
   "metadata": {},
   "source": [
    "And there is a third way: In a forst step we select the columns by their names `surveys_df[['month', 'day', 'year']]`. From the resulting DataFrame we then, in a second step, select the first three rows `[0:3]`. Putting the two steps together, the code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e4a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df[['month', 'day', 'year']][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c31b9",
   "metadata": {},
   "source": [
    "### Interactive Part\n",
    "Let us further explore the `loc` and  `iloc` functions as they are more powerful. Have a look at the examples below and predict their outcome before hitting enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b77914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns for rows of index values 0 and 10\n",
    "surveys_df.loc[[0, 10], :]\n",
    "\n",
    "# What does this do?\n",
    "surveys_df.loc[0, ['species_id', 'plot_id', 'weight']]\n",
    "\n",
    "# What happens when you type the code below?\n",
    "surveys_df.loc[[0, 10, 35549], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffcd6f",
   "metadata": {},
   "source": [
    "We can also extract single values from our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.iloc[row, column]\n",
    "surveys_df.iloc[2, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeacab0",
   "metadata": {},
   "source": [
    "### Summary: Selecting slices, rows and columns\n",
    "In the first two methods we extract the column specifying its name. The third method is essentially identical to the first one as the 6th (index 5) element of the Series ```surveys_df.columns``` is *species_id*. The fourth method uses the method ```iloc``` to select *all* the rows of the 6th column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6958e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By name\n",
    "# --------------------------------------\n",
    "# Method1\n",
    "plot_id_1 = surveys_df['species_id']\n",
    "\n",
    "# Method2\n",
    "plot_id_2 = surveys_df.species_id\n",
    "# --------------------------------------\n",
    "\n",
    "# By location\n",
    "# --------------------------------------\n",
    "# Method3\n",
    "plot_id_3 = surveys_df[surveys_df.columns[5]]\n",
    "\n",
    "# Method4\n",
    "plot_id_4 = surveys_df.iloc[:,5]\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbc17a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise 3 to 5</b>\n",
    "    \n",
    "Now go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercises 3 to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e35fa",
   "metadata": {},
   "source": [
    "### Subsetting Data according to user-defined criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee3780",
   "metadata": {},
   "source": [
    "We can extract subsets of our DataFrame following the general syntax ```data_frame[<condition_on_data>]``` <condition_on_data> is a conditional statement on the DataFrame content itself. You may think at the conditional statement as a question or query you ask to your DataFrame. Here there are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af48aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the data collected in the year 2002?\n",
    "surveys_df[surveys_df.year == 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21c366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What are the data NOT collected in the year 2002?\n",
    "surveys_df[surveys_df.year != 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcf0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What are the data NOT collected in the year 2002? (different syntax)\n",
    "surveys_df[~(surveys_df.year == 2002)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597ccce",
   "metadata": {},
   "source": [
    "Our filtering conditions may be very specific, they can target different columns in the DataFrame, and they can be combined using the logical operator \"&\" which means **and**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f3290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What are the data collected between 2000 and 2002 on female species?\n",
    "surveys_df[(surveys_df.year >= 2000) & (surveys_df.year <= 2002) & (surveys_df.sex == 'F')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf34282",
   "metadata": {},
   "source": [
    "We have also an operator for **or**.\n",
    "Below we filter for rows with collected data on female species in the year 2000 or 2002.\n",
    "\"Give me all data where sex is Female and data is collected in 2000 or 2002\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec961e",
   "metadata": {},
   "source": [
    "The method ```isin()``` allows to specify a range of \"permitted\" values for a certain column. Here it follows another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013eb804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df[(surveys_df.year == 2000) & (surveys_df.sex == 'F') & (surveys_df.month.isin([1,3,4]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd42879",
   "metadata": {},
   "source": [
    "## DataFrame Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf67fe",
   "metadata": {},
   "source": [
    "A simple exploration of our DataFrame showed us that there are columns full of invalid values (NaN). One of the most important preliminary operations of data analysis is cleaning your data set, i.e. \"getting rid\" of non-numerical or non-character values. we want to make sure that our data only contains meaningful values. \n",
    "\n",
    "Now that we mastered selecting, slicing, and subsetting, we can easily clean our DataFrame with few lines of code. Let us have a look at the function *isnull*. It is a Pandas function which we imported at the beginning with `import pandas as pd`. Now we can call the functin like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd708256",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d40c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull([1, 2, 3, '', dict(), None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340df6a",
   "metadata": {},
   "source": [
    "We can pass single values or array-like values to the function. The function will then check for us whether each value is `NaN` (Not a Number) or `None` and return a boolean array.\n",
    "Note, that values like the empty string (a strin without any characters in it) or an empty dictionary etc will not count as `null` value, they do have a type, they only do not contain any values but they are something. \n",
    "`null` values in python are only `NaN` and `None`. When you read in tabular data into a DataFrame empty cells will be shown as `NaN`. `None` stands for the type *NoneType*, which we will not dive into further in this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948108a",
   "metadata": {},
   "source": [
    "With all that kowledge we can now detect `null` values in the column *weight* and do something about it. Let us have a look how many `null` values we can find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f09fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of the full dataframe: \", len(surveys_df))\n",
    "pd.isnull(surveys_df.weight) # boolean array indicating where null values are found\n",
    "surveys_df[pd.isnull(surveys_df.weight)] # all lines that have a null value in the column weight\n",
    "len(surveys_df[pd.isnull(surveys_df.weight)]) # length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c83573",
   "metadata": {},
   "source": [
    "As you can see, in our whole dataset 3266 weight values are not usable. We need to do something with those values.\n",
    "\n",
    "Another thing that would not make sense are negative weights. Let's check whether the remaining 32283 values in the *weight* column are positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(surveys_df[surveys_df.weight > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf62355",
   "metadata": {},
   "source": [
    "As we see, we have 32283 non-negative *weighht* values. The remaining 3266 values in the *weight* column are not set, so they are `null`. How can we impute the values? Let us have a look at the average weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb5d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df.weight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a09567e",
   "metadata": {},
   "source": [
    "A smooth run, without errors or warnings. As we said several times, Pandas is a library designed for data analysis and when performing data analysis it is very common to deal with not numeric values. In particular, the ```.mean()``` method has an argument called *skipna* that when set `True` (default value, so we do not need to specify it) excludes NaN values. This means that, in this case, Pandas simply ignores whatever it is not numeric and it performs computations only on numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6b511",
   "metadata": {},
   "source": [
    "If we are not happy with Pandas default behaviour, we can manually decide which value to assign to cells that contain `null` values. One possible choice is setting them to zero. To do that, we just need to apply the method ```.fillna(<value>)```, where `<value>` is the number we want to substitute to the `null` value with (in our case, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_weight1 = surveys_df.weight.fillna(0)\n",
    "cleaned_weight_ave1 = cleaned_weight1.mean()\n",
    "print(cleaned_weight_ave1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8bad6",
   "metadata": {},
   "source": [
    "You see that when filling the `null` values with 0, the average weight decreases. This is because the mean is now computed on data with many more zeros compared to the previous one.\n",
    "Conscious of this problem, we may now choose a more appropriate value to \"fill\" our `null` values. How about we use the \"clean\" mean of our first computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f46b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_weight2 = surveys_df.weight.fillna(surveys_df.weight.mean())\n",
    "cleaned_weight_ave2 = cleaned_weight2.mean()\n",
    "print(cleaned_weight_ave2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c0be8",
   "metadata": {},
   "source": [
    "This time we obtain exactly the same result of our first computation, this is because we substituted the `null` values with a mean computed excluding the `null` values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b321b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise 6 and 7</b>\n",
    "\n",
    "Now go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercises 6 and 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8ba70-5aba-4c52-96e4-f4cda4c47268",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f430af",
   "metadata": {},
   "source": [
    "We often want to calculate summary statistics grouped by subsets or attributes within fields of our data. For example, we might want to calculate the average weight of all individuals per site.\n",
    "\n",
    "As we have seen above we can calculate basic statistics for all records in a single column using the syntax below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['weight'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a12270",
   "metadata": {},
   "source": [
    "If we want to summarize by one or more variables, for example sex, we can use Pandas’ `.groupby()` method. Once we’ve created a groupby DataFrame, we can quickly calculate summary statistics by a group of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169df8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = surveys_df.groupby('sex')\n",
    "grouped_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb649e4",
   "metadata": {},
   "source": [
    "The output is a bit overwhelming. Let's just have a look at one statistical value, the mean, to understand what is happening here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f88c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b0c18",
   "metadata": {},
   "source": [
    "We see that the data is divided into two groups, one group where the value in the column *sex* equals \"F\" and another group where the value in the column *sex* equals \"M\". The statistics is then calculated for all samples in that specific group for each of the columns in the dataframe. Note that samples annotated with sex equals NaN and column values with NaN are left out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733aba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = surveys_df.groupby(...)\n",
    "grouped_data[...].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71794bfb",
   "metadata": {},
   "source": [
    "## Structure of a groupby object\n",
    "We can investigate which rows are assigned to which group as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5920fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(grouped_data.groups)) # dictionary\n",
    "print(\"Plot ids: \", grouped_data.groups.keys()) # keys are the unique values of the column we grouped by\n",
    "print(\"Rows belonging to plot id \", 1, \": \", grouped_data.groups[1]) # values are row indexes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9e651",
   "metadata": {},
   "source": [
    "## Grouping by multiple columns\n",
    "Now let's have a look at a more complex grouping example. We want an overview statistics of the weight of all females and males by plot id. So in fact we want to group by *sex* and by *plot_id* at the same time.\n",
    "\n",
    "This will give us exactly 48 groups for our survey data:\n",
    "- female, plot id = 1\n",
    "- female, plot id = 2\n",
    "- ...\n",
    "- female, plot id = 24\n",
    "- male, plot id = 1\n",
    "- ...\n",
    "- male, plot id = 24\n",
    "\n",
    "Why 48 groups? We have 24 unique values for *plot_id*. Per plot we have two groups of samples, female and male. Hence, the grouping returns 48 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4525cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = surveys_df.groupby(['sex', 'plot_id'])\n",
    "grouped_data[\"weight\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15193645",
   "metadata": {},
   "source": [
    "## Counting and plotting\n",
    "Another very useful outcome of grouping is the possibility of performing selective counting. For example, let's see how to count the number of records per species. We just need to remember that each species has a unique ID and that records are identified by another ID stored in the column record ID. We will first group our data according to the species ID and then, for each group, we will count the number of records. Several consecutive operations that, once again, Pandas allows us to execute in a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83034cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "species_counts = surveys_df.groupby('species_id')['record_id'].count()\n",
    "print(type(grouped_species_counts))\n",
    "species_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401bd39",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can also plot the information for better overview. We will learn more about plotting after the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e1d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "species_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c925fa",
   "metadata": {},
   "source": [
    "## Summary grouping\n",
    "Grouping is one of the most common operation in data analysis. Data often consists of different measurements on the same samples. In many cases we are not only interested in one particular measurement but in the cross product of measurements. In the picture below we labeled samples with green lines, blue dots and red lines. We are now interested how these three different groups relate to each other given the all other measurements in the dataframe. Pandas' groupby function gives us the means to compare these three groups with several built-in statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaad2c1-a90f-43b1-9950-948209792458",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Grouping sketch](images/grouping.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1ab75",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise 8 to 10</b>\n",
    "    \n",
    "Now go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercises 8 to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b47cf",
   "metadata": {},
   "source": [
    "After you finished the exercises please come back to this document and continue with the [following chapter](data-science-with-pandas-3.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
