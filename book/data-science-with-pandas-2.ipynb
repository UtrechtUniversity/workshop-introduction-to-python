{
 "cells": [
  {
   "cell_type": "raw",
   "id": "14f401b8-2d1d-4a01-a3c8-ed2c59e66d29",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)\"\n",
    "format: html\n",
    "execute:   \n",
    "  enabled: true\n",
    "  error: true\n",
    "  freeze: auto\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8ba70-5aba-4c52-96e4-f4cda4c47268",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80329228-c30a-4474-8b94-624035453e3d",
   "metadata": {},
   "source": [
    "Grouping is one of the most common operation in data analysis. Data often consists of measurements and our data files contain not only the measurement values but also all kind of characteristics/specifications relative to those measurement. When we load our data into a Pandas DataFrame, we usually have measurements in one column and all their different characteristics organized in the other columns. Such characteristics are rapresented by the red-dotted, green-dashed, and blu-long-dashed lines in the picture below. As you can see, in the Pandas DataFrame these measurement characteristics have no particular order.\n",
    "\n",
    "Assuming you managed to arrange your data in a single DataFrame, you may be interested in computing statistics for *subsets* of data, where data subsets can be defined according to our measurement characteristics. If we think, for example, at rain measurements, these may be collected in different wind and temperature conditions, in different places, in different times, and by different researchers. All these characteristics represent variables that can be used for grouping: we can group data by date, to check how much it rained on a certain date, or by place, to check how much it rained in a specific town, or by both place and date. It is important to know that the TOTAL information in our DataFrame will be always the same, but *organising* the data by different charactersitics may help us to see patterns that otherwise would remain hidden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaad2c1-a90f-43b1-9950-948209792458",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Grouping sketch](pictures/grouping.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca5b4c-ee2f-4122-abd0-21e7e89b9271",
   "metadata": {},
   "source": [
    "In our specific case, we might be interested in calculating the average weight of all individuals per site or per sex. Once again, Pandas methods allow us to perform such operations with few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31425c3-7d07-4524-81fc-f410283566d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_data = surveys_df.groupby('sex')\n",
    "print(type(grouped_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9dde6-e356-454b-92a9-b7ccef099e94",
   "metadata": {},
   "source": [
    "The result of applying the method ```groupby()``` is not a \"classic\" Pandas DataFrame, but another Pandas object, a *DataFrameGroupBy*. This tells us that this specific DataFrame has been the result of a grouping operation. Contrary to the classic DataFrame, we cannot visualise the characteristics of this DataFrame simply running a cell with its name in it, but we can have a look at its statistical characteristics using a method we already introduced in the previous paragraphs: ```.describe()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f307e62-4c91-4f51-84ac-01c07e534b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194eb770-66fb-424a-8bfb-7c7dfec9fc30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649edebc-4101-4528-a2d9-178d2692da83",
   "metadata": {},
   "source": [
    "While on a classic DataFrame ```.describe()``` returns statistical information per column, on a DataFrameGroupBy it will return statistical information per group, in our case sex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efacfff7-8bce-411c-8550-cda251a4118e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TRY IT YOURSELF:</b> \n",
    "    <ul>\n",
    "        <li>Try to run the method <code>.mean()</code> on your DataFrameGroupBy. Can you tell what do you get?</li>\n",
    "        <li>What happens when you group by two columns (<code>.groupby(['plot_id', 'sex'])</code>) and you apply the method <code>.mean()</code> on your DataFrameGroupBy?</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65eecfd-1e77-4755-9efd-2cfacab49caf",
   "metadata": {},
   "source": [
    "Another very useful outcome of grouping is the possibility of performing selective counting. For example, let's see how to count the number of records per species. We just need to remember that each species has a unique ID and that records are identified by another ID stored in the column record ID. We will first group our data according to the species ID and then, for each group, we will count the number of records. Several consecutive operations that, once again, Pandas allows us to execute in a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac316506-a25d-4e48-93ed-309603ed7fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_species_counts = surveys_df.groupby('species_id')['record_id'].count()\n",
    "print(type(grouped_species_counts))\n",
    "grouped_species_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5d06a-c179-4661-9b07-f295022efc36",
   "metadata": {
    "tags": []
   },
   "source": [
    "Even if the result of our counting is a Series, we can still visualise it in a 2D plot. Indeed, a Pandas Series is a *labeled* 1D data structure, this means that to each data element of the Series it is assigned both a label and an index. Applying the ```.plot()``` method to our result will generate a plot with data labels (species ID) on the X-axis and counts on the Y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272b41d-e401-461b-a032-a612b0e2926a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_species_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e561f37-84b5-4081-906c-ec23c48292c1",
   "metadata": {},
   "source": [
    "## Indexing, Slicing, and Subsetting DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b3e8a-059a-4c50-a8a0-dcaf2e0c450e",
   "metadata": {},
   "source": [
    "We saw that using grouping we can conveniently subset our DataFrame according to different measurement characteristics. However, sometimes it is necessary to \"surgically\" extract small portions of DataFrame such us single rows and columns of data satisfying very specific filtering criteria. In this paragraph we will see how Pandas allows to perform all these operations with a quite intuitive syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e4a6ef-c56c-43d4-b12a-3de9177b63e2",
   "metadata": {},
   "source": [
    "### Selecting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700abef7-b29b-491c-84e8-c26e0b2cce66",
   "metadata": {},
   "source": [
    "Let's look again at our original DataFrame columns using a loop. This time we will add some extra conditional statements to highlight the column name corresponding to a specific index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a1d8a-d31f-4096-be5e-f514825d2b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sel_index = 5\n",
    "print('Index) Column name')\n",
    "for i,col in enumerate(surveys_df.columns):\n",
    "    if i == sel_index:\n",
    "        print('{}) {} <==='.format(i,col))\n",
    "    else:\n",
    "        print('{}) {}'.format(i,col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0431e-a995-4f65-b6ac-57bcb581251c",
   "metadata": {},
   "source": [
    "We already saw in one of the previous paragraph how to extract a specific DataFrame column, but we did not go too much into details. The next block of code shows how to retrieve the same column (specied_id corresponding to index 5) from our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7763b53-772e-456d-bffa-94945a9b8451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#By name\n",
    "# --------------------------------------\n",
    "#Method1\n",
    "plot_id_1 = surveys_df['species_id']\n",
    "\n",
    "#Method2\n",
    "plot_id_2 = surveys_df.species_id\n",
    "# --------------------------------------\n",
    "\n",
    "#By location\n",
    "# --------------------------------------\n",
    "#Method3\n",
    "plot_id_3 = surveys_df[surveys_df.columns[5]]\n",
    "\n",
    "#Method4\n",
    "plot_id_4 = surveys_df.iloc[:,5]\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e851be-e8d5-4d43-b9d9-25e3be30ed76",
   "metadata": {},
   "source": [
    "In the first two methods we extract the column specifying its name. The third method is substantially identical to the first one as the 6th (index 5) element of the Series ```surveys_df.columns``` is species_id. The fourth method uses the method ```iloc``` to select *all* the rows of the 6th column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477ca6f-9c69-4c56-bf2b-1496f5a83374",
   "metadata": {},
   "source": [
    "### Selecting data by type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0f370-e2d1-4093-9b74-e79a259a96ca",
   "metadata": {},
   "source": [
    "The attributes ```dtypes``` contains information about the data types contained in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54150e08-8379-4d68-80cf-e57968409c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e37d2e4-4525-4353-af3e-666a9748b80a",
   "metadata": {},
   "source": [
    "This information is not only important for our data analysis, but it also allows us to eventually select subset of data according to its type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f7299-f186-4b1a-8fdc-b93b9c552821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df_float_sel = surveys_df.select_dtypes(include = ['float64'])\n",
    "print(type(surveys_df_float_sel))\n",
    "surveys_df_float_sel.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627856c-5e04-4372-927b-89854a20694b",
   "metadata": {},
   "source": [
    "In the previous block of code we used a method, ```.select_dtypes()```, to select the DataFrame columns storing only float64 values (double-precision floating point numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a38a00-8b97-416d-838f-8a28dc60c695",
   "metadata": {},
   "source": [
    "### Selecting by string in name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9fca28-8eff-4d55-9cee-24f09df89ec0",
   "metadata": {},
   "source": [
    "Another very convenient option to select data is specifying a string that must be contained in the column names. DataFrame column names are indicative (or at least, they should be) of the characteristics relative to measurements. All the columns containing a unique identifier, for example, may contain the suffix \"id\" while all the measurements relative to a specific body part (another example) will most probably contain that body part name as well. In this context, the Pandas method ```.filter(like=<str>)``` will allow to extract only those columns containing a certain string in their names. \n",
    "For example, let's extract all columns containing some sort of ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d1d09-889e-44d7-aa35-3b1e6c14469e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(surveys_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2e1b3-15ac-4e59-b278-db0566ec95cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df_str_sel = surveys_df.filter(like='_id')\n",
    "print(type(surveys_df_str_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7315187-10d6-43bb-911a-41fca7c23521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df_str_sel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ec2f0-194b-4c29-8659-a01f73001aa8",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74d298-973c-4837-a093-7d2366a4f1fc",
   "metadata": {},
   "source": [
    "DataFrame slicing allows you to extract a portion of a DataFrame based on conditions or indices and create a new DataFrame containing only the subset of data that you are interested in. In Pandas slicing can be perfomed using the methods ```loc``` and '''iloc''' for slicing via names and indices, respectively. To remember the difference between the two, just notice that the \"i\" in ```iloc``` stands for \"index\".\n",
    "Let's start slicing our initial DataFrame into a 3x4 sub-DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de7cb1-904b-46c4-a558-af0a4e4f3ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df.iloc[0:3,0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854610d1-479e-4437-bc1f-7515f60406fd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WARNING:</b> In Python integer indexing starts with 0 and, when slicing using a continous range of indices, data corresponding to the last index is NOT included.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e81eb-21b6-403f-977e-ff97f43721f0",
   "metadata": {},
   "source": [
    "We can obtain the same result using ```loc```, but we need to specify a list with the first 4 column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c42807-22e8-4614-a180-0277af134872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df.loc[[0,1,2],['record_id','month','day','year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e46128-2ef5-4148-8ee2-4cd4a17b3cc3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TRY IT YOURSELF:</b> Can you tell what happens when you execute the following commands?\n",
    "</div>\n",
    "\n",
    "- ```surveys_df[0:1]```;\n",
    "- ```surveys_df[:4]```;\n",
    "- ```surveys_df[:-1]```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dad2a-73e0-45b7-a91d-523c791d7bd8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TRY IT YOURSELF:</b> What happens when you call the following commands? How are the two commands different?\n",
    "</div>\n",
    "\n",
    "- ```surveys_df.iloc[0:4, 1:4]```;\n",
    "- ```surveys_df.loc[0:4, 1:4]```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d296c-693b-4565-bea7-3e68235efd32",
   "metadata": {},
   "source": [
    "### Subsetting Data according to user-defined criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c8251-6e95-46d7-86d6-7c1bb9293bff",
   "metadata": {},
   "source": [
    "We can extract subsets of our DataFrame following the general syntax ```data_frame[<condition_on_data>]```. <condition_on_data> is a conditional statement on the DataFrame content itself. You may think at the conditional statement as a question or query you ask to your DataFrame. Here there are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56eae8-a974-4179-b954-b10069854b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the data collected in 2002?\n",
    "surveys_df[surveys_df.year == 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b573a-a94a-4b6d-86a5-fdec07ae6ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What are the data NOT collected in 2002?\n",
    "surveys_df[surveys_df.year != 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8e29d-03ae-430a-9e9e-4956a4f59dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What are the data NOT collected in 2002? (different syntax)\n",
    "surveys_df[~(surveys_df.year == 2002)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c8a66-3774-4add-9100-3f96762ad9a5",
   "metadata": {},
   "source": [
    "Our filtering conditions may be very specific, they can target different columns in the DataFrame, and they can be combined using the logical operator \"&\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbfed86-acd7-4c15-b4e2-d2b289802f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What are the data collected between 2000 and 2002 on female species?\n",
    "surveys_df[(surveys_df.year >= 2000) & (surveys_df.year <= 2002) & (surveys_df.sex == 'F')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60aa552-3dcb-4340-ba63-02fb8bcfa886",
   "metadata": {},
   "source": [
    "The method ```isin()``` allows to specify a range of \"permitted\" values for a certain column. Here it follows another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f055d-7737-45a9-83ff-12b66888a814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "surveys_df[(surveys_df.year == 2000) & (surveys_df.sex == 'F') & (surveys_df.month.isin([1,3,4]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a05df5-f24f-4a10-a392-854730f0cee4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TRY IT YOURSELF:</b> \n",
    "    <ol>\n",
    "    <li> Create a new DataFrame that only contains observations with sex values that are not female or male. Print the number of rows in this new DataFrame. Verify the result by comparing the number of rows in the new DataFrame with the number of rows in the surveys DataFrame where sex is null.</li>\n",
    "    <li>Create a new DataFrame that contains only observations that are of sex male or female and where weight values are greater than 0.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c72626b-e077-49f5-adf7-a84ce7ab952f",
   "metadata": {},
   "source": [
    "## DataFrame Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d008aac8-bce7-4fb6-9290-d564afc7f90b",
   "metadata": {},
   "source": [
    "A simple exploration of our DataFrame showed us that there are columns full of invalid values (NaN). One of the most important preliminary operations of data analysis is cleaning your data set, i.e. \"getting rid\" of non numerical values. Now that we mastered selecting, slicing, and subsetting, we can easily clean our DataFrame with few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87583bff-06c8-4414-9dc7-eb1be7121f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any invalid values in the weight column?\n",
    "n_tot = len(surveys_df)\n",
    "n_null_weight = len(surveys_df[pd.isnull(surveys_df.weight)])\n",
    "n_pos_weight  = len(surveys_df[surveys_df.weight > 0])\n",
    "\n",
    "print('Total number of rows:',n_tot)\n",
    "print('Number of null weight rows:',n_null_weight)\n",
    "print('Number of positive weight rows:',n_pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cc712-1406-4676-b3cd-e98481a0e701",
   "metadata": {},
   "source": [
    "As you can see, out of 35549 weight measurements, 3266 are not usable. The remaining 32283 values are positive, so usable, values. What happens if we compute the mean weight ignoring the fact that there are not numeric values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c436e91-4a89-47bb-9e21-5ad3852285e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ave_weight = surveys_df.weight.mean()\n",
    "print(ave_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd957cc-a21a-4c99-93af-5ccd26ad2260",
   "metadata": {},
   "source": [
    "A smooth run, without errors or warnings. As we said several times, Pandas is a library designed for data analysis and when performing data analysis it is very common to deal with not numeric values. In particular, the ```.mean()``` method has an argument called *skipna* that when set TRUE (default value, so we do not need to specify it) excludes NA/null values. This means that, in this case, Pandas simply ignores whatever it is not numeric and it performs computations only on numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe36d7-8b42-41a6-8169-c1ec5ee8d10e",
   "metadata": {},
   "source": [
    "If we are not happy with Pandas default behaviour, we can manually decide which value to assigni to NA/null values. One possible choice is setting them to zero. To do that, we just need to apply the method ```.fillna(<value>)```, where <value> is the number we want to substitute to the NA/null value (in our case, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2fd01c-df32-48d0-83d9-188c41d0f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_weight1 = surveys_df.weight.fillna(0)\n",
    "cleaned_weight_ave1 = cleaned_weight1.mean()\n",
    "print(cleaned_weight_ave1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189aad29-aa01-4e21-ba14-98067e3a2678",
   "metadata": {},
   "source": [
    "You probably noticed that compared to our previous mean computation, the result it's pretty different. This is because the mean is now computed on a sample with many more zeros compared to the previous one and, as a result, the value of the computed mean is smaller.\n",
    "Conscious of this problem, we may now choose a more appropriate value to \"fill\" our NA/null values. How about we use the \"clean\" mean of our first computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d10875-5528-4d78-86e4-86efaaa8f39c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_weight2 = surveys_df.weight.fillna(surveys_df.weight.mean())\n",
    "cleaned_weight_ave2 = cleaned_weight2.mean()\n",
    "print(cleaned_weight_ave2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469696a-c8c5-4147-a34d-b514500f245a",
   "metadata": {},
   "source": [
    "This time we obtain exactly the same result of our first computation, this is because we substituted the NA/null values with a mean computed excluding the NA/null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf2b6d-6469-457b-b15e-5f785973bc05",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>TRY IT YOURSELF:</b> Compute the average weight of data after having cleaned the weight and the sex column.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
