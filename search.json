[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python & Data",
    "section": "",
    "text": "Welcome!\nPython is a powerful programming language that is suitable for scientific computing and general-purpose programming. Python is used to write scripts to work efficiently and reproducibly with scientific data.\nIn this workshop, we will utilize the Jupyter Notebook interface and take you from the basics of Python syntax to using the pandas package to work with data frames. In doing so, we aim to give you the tools and confidence to start exploring Python and all it has to offer.\nOur workshop material is licensed under a Creative Commons Attribution 4.0 International License. You can view the license on our GitHub repository."
  },
  {
    "objectID": "acknowledgements.html#contributors",
    "href": "acknowledgements.html#contributors",
    "title": "Acknowledgements",
    "section": "Contributors",
    "text": "Contributors\nThe following indivduals have contributed to the development of this workshop:\n\nJelle Treep\nChristine Staiger\nRoel Brouwer\nNeha Moopen\nStefano Rapisarda"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Time\nActivity\n\n\n\n\n9:00\nWalk-in, tech support\n\n\n9:30\nIntroductions\n\n\n10:00\nPython Basics: Exercises 1-6\n\n\n11:25\nRecap & Questions\n\n\n11:30\nCoffee break\n\n\n11:45\nProgramming: Exercises 7-9\n\n\n12:40\nRecap & Questions\n\n\n12:45\nLunch break\n\n\n13:30\nIntroduction to pandas & Importing data: Exercises 1-4\n\n\n14:15\nSubsetting and mutating data: Exercises 5-8\n\n\n14:55\nRecap & Questions\n\n\n15:00\nCoffee break\n\n\n15:15\nTransformations & tidy data: Exercises 9-12\n\n\n16:00\nData visualization: Exercises 13-15\n\n\n16:55\nFinal recap and closing"
  },
  {
    "objectID": "installation-and-setup.html#overview",
    "href": "installation-and-setup.html#overview",
    "title": "1  Installation & Setup",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nThe course materials are designed to be run on a personal computer. All of the software and data used are freely available online, and instructions on how to obtain them are provided below.\nIf you have any questions, or are not comfortable with doing the installation yourself join the RDM walk-in hours to ask for help."
  },
  {
    "objectID": "installation-and-setup.html#install-python",
    "href": "installation-and-setup.html#install-python",
    "title": "1  Installation & Setup",
    "section": "1.2 Install Python",
    "text": "1.2 Install Python\nIn this course, we will be using Python 3 with some of its most popular scientific libraries. Python is a popular language for research computing, and great for general-purpose programming as well. Although one can install a plain-vanilla Python and all required libraries by hand, we recommend installing Anaconda, a Python distribution that comes with everything we need for the lesson. Detailed installation instructions can be found in the Anaconda documentation, or by following the instructions below.\nRegardless of how you choose to install it, please make sure you install Python version 3.x (e.g., 3.9 is fine).\nWe will teach Python using the Jupyter Notebook, a programming environment that runs in a web browser (Jupyter Notebook will be installed by Anaconda). For this to work you will need a reasonably up-to-date browser. The current versions of the Chrome, Safari and Firefox browsers are all supported (some older browsers, including Internet Explorer version 9 and below, are not).\n\nWindowsMacOSLinux\n\n\n\nOpen https://www.anaconda.com/products/individual#download-section with your web browser.\nDownload the Anaconda for Windows installer with Python 3. (If you are not sure which version to choose, you probably want the 64-bit Graphical Installer Anaconda3-...-Windows-x86_64.exe)\nInstall Python 3 by running the Anaconda Installer, using all of the defaults for installation except make sure to check Add Anaconda to my PATH environment variable.\n\n\n\n\nOpen https://www.anaconda.com/products/individual#download-section with your web browser.\nDownload the Anaconda Installer with Python 3 for macOS (you can either use the Graphical or the Command Line Installer).\nInstall Python 3 by running the Anaconda Installer using all of the defaults for installation.\n\n\n\n\nOpen https://www.anaconda.com/products/individual#download-section with your web browser.\nDownload the Anaconda Installer with Python 3 for Linux.  (The installation requires using the shell. If you aren’t comfortable doing the installation yourself stop here and come to the walk-in hours to ask for help.)\nOpen a terminal window and navigate to the directory where the executable is downloaded (e.g., cd ~/Downloads).\nType\nbash Anaconda3-\nand then press Tab to autocomplete the full file name. The name of file you just downloaded should appear.\nPress Enter (or Return depending on your keyboard). You will follow the text-only prompts. To move through the text, press Spacebar. Type yes and press enter to approve the license. Press Enter (or Return) to approve the default location for the files. Type yes and press Enter (or Return) to prepend Anaconda to your PATH (this makes the Anaconda distribution the default Python).\nClose the terminal window."
  },
  {
    "objectID": "installation-and-setup.html#obtain-lesson-materials",
    "href": "installation-and-setup.html#obtain-lesson-materials",
    "title": "1  Installation & Setup",
    "section": "1.3 Obtain lesson materials",
    "text": "1.3 Obtain lesson materials\n\nDownload the zip folder with all course materials (see Course Materials).\nCreate a folder called intro-python on your Desktop.\nMove downloaded files to intro-python.\nUnzip the files.\n\nYou should see two folders called data and code in the intro-python directory on your Desktop."
  },
  {
    "objectID": "installation-and-setup.html#launch-python-interface",
    "href": "installation-and-setup.html#launch-python-interface",
    "title": "1  Installation & Setup",
    "section": "1.4 Launch Python interface",
    "text": "1.4 Launch Python interface\nTo start working with Python, we need to launch a program that will interpret and execute our Python commands. Below we list several options. If you don’t have a preference, proceed with the top option in the list that is available on your machine. Otherwise, you may use any interface you like.\n\nOption A: Jupyter Notebook\nA Jupyter Notebook provides a browser-based interface for working with Python. If you installed Anaconda, you can launch a notebook in two ways:\n\nCommand line (Terminal)Anaconda Navigator\n\n\n\nNavigate to the data directory: Unix shell If you’re using a Unix shell application, such as Terminal app in macOS, Console or Terminal in Linux, or Git Bash on Windows, execute the following command:\ncd ~/Desktop/intro-python/data\nCommand Prompt (Windows) On Windows, you can use its native Command Prompt program. The easiest way to start it up is pressing Windows Logo Key+R, entering cmd, and hitting Return. In the Command Prompt, use the following command to navigate to the data folder:\ncd /D %userprofile%\\Desktop\\intro-python\\data\nStart Jupyter server: Unix shell\njupyter notebook\nCommand Prompt (Windows)\npython -m notebook\nLaunch the notebook by clicking on the “New” button on the right and selecting “Python 3” from the drop-down menu.\n\n\n\n\nLaunch Anaconda Navigator. It might ask you if you’d like to send anonymized usage information to Anaconda developers. Make your choice and click “Ok, and don’t show again” button.\nFind the “Notebook” tab and click on the “Launch” button. Anaconda will open a new browser window or tab with a Notebook Dashboard showing you the contents of your Home (or User) folder.\nNavigate to the data directory by clicking on the directory names leading to it. Desktop, intro-python, then data:\nLaunch the notebook by clicking on the “New” button and then selecting “Python 3”.\n\n\n\n\n\n\nOption B: IPython interpreter\nIPython is an alternative solution situated somewhere in between the plain-vanilla Python interpreter and Jupyter Notebook. It provides an interactive command-line based interpreter with various convenience features and commands. You should have IPython on your system if you installed Anaconda.\nTo start using IPython, execute:\nipython\n\n\nOption C: plain-vanilla Python interpreter\nTo launch a plain-vanilla Python interpreter, execute:\npython\nIf you are using Git Bash on Windows, you have to call Python via winpty:\nwinpty python"
  },
  {
    "objectID": "installation-and-setup.html#python-packages",
    "href": "installation-and-setup.html#python-packages",
    "title": "1  Installation & Setup",
    "section": "1.5 Python packages",
    "text": "1.5 Python packages\nThis workshop will make use of the following Python packages: - pandas - matplotlib\nAnaconda Navigator comes with these packages, so you are ready to go. If you are using another option to work with Python, you need to install these packages. If you need help with this, please ask the internet, your colleagues or us (see welcome email)."
  },
  {
    "objectID": "installation-and-setup.html#references",
    "href": "installation-and-setup.html#references",
    "title": "1  Installation & Setup",
    "section": "References",
    "text": "References\nThe instructions on this page were adapted from the setup instructions of the Software Carpentries “Programming with Python” course and their Workshop Template Python installation instructions, both released under the Creative Commons Attribution license. Changes to the material were made, and can be tracked in the Git repository associated with this course.\nzipfile-course-materials"
  },
  {
    "objectID": "course-materials.html#zipped-file",
    "href": "course-materials.html#zipped-file",
    "title": "Workshop Materials",
    "section": "Zipped File",
    "text": "Zipped File\nThe following zipped file contains the notebooks and data required for the workshop.\n\ncourse materials.zip\n\nDon’t forget to extract the contents of the zipped file after downloading!"
  },
  {
    "objectID": "course-materials.html#notebooks",
    "href": "course-materials.html#notebooks",
    "title": "Workshop Materials",
    "section": "Notebooks",
    "text": "Notebooks\nWe will be using the following Jupyter Notebooks to work on the exercises:\n\nmorning_exercises.ipynb\nafternoon_exercises.ipynb"
  },
  {
    "objectID": "course-materials.html#data",
    "href": "course-materials.html#data",
    "title": "Workshop Materials",
    "section": "Data",
    "text": "Data\nThe data for this workshop is from the Portal Teaching Database. We will be using the following datasets:\n\nsurveys.csv\nspecies.csv\nplots.csv"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "What is Python? Why do we like it so much more than R? Let’s get going!"
  },
  {
    "objectID": "Introduction_to_python_1.html#output-versus-printing",
    "href": "Introduction_to_python_1.html#output-versus-printing",
    "title": "2  Variables and printing output",
    "section": "2.1 Output versus printing",
    "text": "2.1 Output versus printing\nIn the above examples, most of the times output is printed directly below the cell, but not always the output is printed and not all operations are printed. The print command can be used to control what is printed when.\nNote, that text (strings) always has to be surrounded by \" or '.\n\nprint(\"Hello World\")\n\nHello World\n\n\nIn the example below we first print the value of the variable number using the print command, and then call the variable:\n\nprint(number)\nnumber\n\n42\n\n\n42\n\n\nNow we do it the other way around:\n\nnumber\nprint(number)\n\n42\n\n\nWhen not using the print command, only the output of the last operation in the input cell is printed. If the last operation is the assignment of a variable, nothing will be printed.\nIn general print is the only way to print output to the screen when you are not working in an interactive environment like Jupyter (as we are doing now).\nRule of thumb: use the normal output for quick checking the output of an operation while developing in your Jupyter notebook, use print for printing output that still needs to be there in the future while your scripts get more complicated.\n\nExercises\nNow go to the Jupyter Dashboard in your internet browser and navigate to the course materials and open the notebook morning_exercises.ipynb\nIf Jupyter Dashboard is not there, check Installation & Setup for instructions to start the Jupyter Dashboard.\nDo Exercise 0 and after that come back to this document to continue with the following chapter Operators and Built-in Functions"
  },
  {
    "objectID": "Introduction_to_python_2.html#mathematical-operations",
    "href": "Introduction_to_python_2.html#mathematical-operations",
    "title": "3  Operators and built-in functions",
    "section": "3.1 Mathematical operations",
    "text": "3.1 Mathematical operations\nIn Python you can do a wide variety of mathematical operations. A few examples:\n\nsumming = 2 + 2\nmultiply = 2 * 7\npower = 2 ** 16\nmodulo = 13 % 5\n\nprint(\"Sum: \", summing)\nprint(\"Multiply: \", multiply)\nprint(\"Power: \", power)\nprint(\"Modulo: \", modulo)\n\nSum:  4\nMultiply:  14\nPower:  65536\nModulo:  3\n\n\nOnce we have data stored in variables, we can use the variables to do calculations.\n\nnumber = 42\npi_value = 3.14159265358\n\noutput = number * pi_value\nprint(output)\n\n131.94689145036"
  },
  {
    "objectID": "Introduction_to_python_2.html#built-in-python-functions",
    "href": "Introduction_to_python_2.html#built-in-python-functions",
    "title": "3  Operators and built-in functions",
    "section": "3.2 Built-in Python functions",
    "text": "3.2 Built-in Python functions\nTo carry out common tasks with data and variables in Python, the language provides us with several built-in functions. Examples of built-in functions that we already used above are print and type.\nCalling a function When we want to make use of a function (referred to as calling the function), we type the name of the function followed by parentheses. Between the parentheses we can pass arguments.\nArguments We typically provide a function with ‘arguments’ to tell python which values or variables are used to perform the body of the function. In the example below type is the function name and pi_value is the argument.\n\ntype(pi_value)\n\nfloat\n\n\nOther useful built-in functions are abs(), max(), min(), range(). Find more built-in functions here.\n\nmax([1,2,3,2,1])\n\n3"
  },
  {
    "objectID": "Introduction_to_python_2.html#boolean-values-logical-expressions-and-operators",
    "href": "Introduction_to_python_2.html#boolean-values-logical-expressions-and-operators",
    "title": "3  Operators and built-in functions",
    "section": "3.3 Boolean values, Logical expressions and operators",
    "text": "3.3 Boolean values, Logical expressions and operators\nIn programming you often need to know if something is True or False. True and False are called Boolean values and have their own data type (bool so they are not of type str!!). True and False are the only two Boolean values.\n\na = True\na\n\nTrue\n\n\n\nb = False\nb\n\nFalse\n\n\n\ntype(a)\n\nbool\n\n\nComparison operators (e.g. &gt;, &lt;, ==) are used in an expression to compare two values. The result of this expression is either True or False. Why this is useful we will show later (see if-statements).\n\n3 &gt; 4\n\nFalse\n\n\n3 &gt; 4 is an example of a ‘logical expression’ (also known as condition), where &gt; is the comparison operator.\n\n4 &gt; 3\n\nTrue\n\n\n== is another comparison operator to check if two values or variables are the same. If this is the case it will return True\n\nfour = 4          # first we assign the integer 4 to a variable\nfour == 4         # then we check if it is equal to 4\n\nTrue\n\n\n!= is used to check if two values or variable are not the same. If this is the case it will return True\n\nprint(\"Four is not equal to 5: \", four != 5)\nprint(\"Four is not equal to 4: \", four != 4)\n\nFour is not equal to 5:  True\nFour is not equal to 4:  False\n\n\nand, or and not are ‘logical operators’, and are used to join two logical expressions (or revert a logical expression in the case of not) to create more complex conditions.\nand will return True if both expression on either side are True.\n\na = True\nb = True\na and b\n\nTrue\n\n\n\na = True\nb = False\na and b\n\nFalse\n\n\n\n4 &gt; 3 and 9 &gt; 3\n\nTrue\n\n\nor is used to check if at least one of two logical expressions are True. If this is the case it will return True.\n\n3 &gt; 4 or 9 &gt; 3\n\nTrue\n\n\n\n4 &gt; 3 or 9 &gt; 3\n\nTrue\n\n\nIn the last three examples you can see that multiple expressions can be combined in a single line of Python code. Python evaluates the expressions one by one. 4 &gt; 3 would return True, 9 &gt; 3 would return True, so 4 &gt; 3 or 9 &gt; 3 would translate to True or True.\nIt is also possible to assign the output of an expression to a variable:\n\ngreater = 3 &gt; 4\nprint(\"3 &gt; 4 : \", greater)\n\n3 &gt; 4 :  False\n\n\nThe not operator can be used to reverse the Boolean value. If you apply not to an expression that evaluates to True, then you get False as a result. If you apply not to an expression that evaluates to False, then you get True as a result:\n\nnot 4 &gt; 3\n\nFalse\n\n\nLogical operators bind variables with different strengths. The and is stronger than the or and gets evaluated first in a boolean expression. So a or b and c will be evaluated like a or (b and c), while in (a or b) and c first the value of the or is evaluated and then combined with and c. This leads to a different result.\n\na = True\nb = True\nc = False\nprint(\"This expression 'a or b and c' evalutes to \", a or b and c)\nprint(\"And this is the same as 'a or (b and c)')\", a or (b and c))\nprint(\"But this expression evaluates '(a or b) and c' first the 'or' and generates:\", (a or b) and c)\n\nThis expression 'a or b and c' evalutes to  True\nAnd this is the same as 'a or (b and c)') True\nBut this expression evaluates '(a or b) and c' first the 'or' and generates: False\n\n\n\nExercises\nNow go back to your browser to morning_exercises.ipynb and continue with exercises 1-3.\nWhen you finished the exercises, continue to chapter Data types, if-statements and for loops"
  },
  {
    "objectID": "Introduction_to_python_3.html#the-if-statement",
    "href": "Introduction_to_python_3.html#the-if-statement",
    "title": "4  Data types, if-statements and for-loops",
    "section": "4.1 The if-statement",
    "text": "4.1 The if-statement\nIf statements can be used to perform tasks only when a certain condition is met.\n\nnum = 101\n\nif num &gt; 100:\n    print('number is greater than 100')\n\nnumber is greater than 100\n\n\nAs you can see, the line print(...) starts with 4 spaces indentation. In Python indentation is very important. Python uses indentation to determine which lines of code belong to what part of the code. This is mostly important when defining e.g. if-statements, for loops or functions. After the if condition, all lines with indentation are only performed when the if-condition is met.\n\nnum = 99\nif num &gt; 100:\n    print('This line is only executed when num &gt; 100')\n    print('This line is only executed when num &gt; 100')\n    \n    print('This line is only executed when num &gt; 100')\n    \nprint('This line is always executed')\n\nThis line is always executed\n\n\nIt is also possible to specify a task that is performed when the condition is not met using else (note the use of indentation):\n\nnum = 37\n\nif num &gt; 100:\n    print('number is greater than 100')\nelse:\n    print('number is not greater than 100')\n\nprint('done')\n\nnumber is not greater than 100\ndone\n\n\nAn if ... else statement can be extended with (one or more) elif to specify more tasks that need to be performed on other conditions. These extended if ... else statements always start with if followed by (one or more) elif. When an else statement is included it is always the last statement.\nOrder matters: The statements (or conditions) are checked in order from top to bottom and only the task belonging to the first condition that is met is being performed.\n\nnum = -3\n\nif num &gt; 0:\n    print(num, 'is positive')\nelif num == 0:\n    print(num, 'is zero')\nelse:\n    print(num, 'is negative')\n\n-3 is negative\n\n\nAlong with the &gt; and == comparison operators that we have already used for comparing values in our logical expressions above, there are a few more options to know about:\n\n&gt;: greater than\n&lt;: less than\n==: equal to\n!=: does not equal\n&gt;=: greater than or equal to\n&lt;=: less than or equal to\n\nWe can combine logical statements using and and or in more complex conditions in if statements.\n\nif (1 &lt; 0) or (1 &gt;= 0):\n    print('at least one the above logical statements is true')\n\nat least one the above logical statements is true\n\n\nWhile and is only true if both parts are true\n\nif (1 &lt; 0) and (1 &gt;= 0):\n    print('both tests are true')\nelse:\n    print('at least one of the tests is not true')\n\nat least one of the tests is not true"
  },
  {
    "objectID": "Introduction_to_python_3.html#lists-and-tuples",
    "href": "Introduction_to_python_3.html#lists-and-tuples",
    "title": "4  Data types, if-statements and for-loops",
    "section": "4.2 Lists and Tuples",
    "text": "4.2 Lists and Tuples\nUntil now we have worked with values and variables that hold one value or string. Now we will go into other data types that can combine multiple values or strings.\nLists are common data structures to hold a sequence of elements. We can create a list by putting values inside square brackets and separating the values with commas.\n\nnumbers = [1, 2, 3]\nprint(numbers)\n\n[1, 2, 3]\n\n\nEach element can be accessed by an index. The index of the first element in a list in Python is 0 (in some other programming languages that would be 1).\n\nprint(\"The first element in the list numbers is: \", numbers[0])\n\nThe first element in the list numbers is:  1\n\n\n\ntype(numbers)\n\nlist\n\n\nA total number of items in a list is called the ‘length’ and can be calculated using the len() function.\n\nlen(numbers)\n\n3\n\n\nYou can do various things with lists. E.g. it is possible to sum the items in a list (when the items are all numbers)\n\nprint(\"The sum of the items in the list is:\", sum(numbers))\nprint(\"The mean of the items in the list is:\", sum(numbers)/len(numbers))\n\nThe sum of the items in the list is: 6\nThe mean of the items in the list is: 2.0\n\n\nWhat happens here:\n\nnumbers[3]\n\nIndexError: list index out of range\n\n\nThis error is expected. The list consists of three items, and the indices of those items are 0, 1 and 2.\n\nnumbers[-1]\n\n3\n\n\nYes, we can use negative numbers as indices in Python. When we do so, the index -1 gives us the last element in the list, -2 the second to last, and so on. Because of this, numbers[2] and numbers[-1] point to the same element.\n\nnumbers[2] == numbers[-1]\n\nTrue\n\n\nIt is also possible to combine strings in a list:\n\nwords = [\"cat\", \"dog\", \"horse\"]\nwords[1]\n\n'dog'\n\n\n\ntype(words)\n\nlist\n\n\n\nif type(words) == type(numbers):\n    print(\"these variables have the same type!\")\n\nthese variables have the same type!\n\n\nIt is also possible to combine values of different type (e.g. strings and integers) in a list\n\nnewlist = [\"cat\", 1, \"horse\"]\n\nThe type of the variable newlist is list. The elements of the list have their own data type:\n\ntype(newlist[0])\n\nstr\n\n\n\ntype(newlist[1])\n\nint\n\n\nIt is possible to add numbers to an existing list using list.append()\n\nnumbers.append(4)\nprint(numbers)\n\n[1, 2, 3, 4]\n\n\nUsing the index of an item, you can replace the item in a list:\n\nnumbers[2] = 333\nprint(numbers)\n\n[1, 2, 333, 4]\n\n\nNow what do you do if you do not know the index but you know the value of an item that you want to find in a list. How to find out at which position the value is listed?\n\nindex = newlist.index(\"cat\")\nprint(\"'cat' can be found at index\", index)\nprint(newlist[index])\n\n'cat' can be found at index 0\ncat\n\n\nA tuple is similar to a list in that it’s a sequence of elements. However, tuples can not be changed once created (they are “immutable”). Tuples are created by placing comma-separated values inside parentheses () (instead of square brackets []).\n\n# Tuples use parentheses\na_tuple = (1, 2, 3)\nanother_tuple = ('blue', 'green', 'red')\n\n# Note: lists use square brackets\na_list = [1, 2, 3]\n\n\na_list[1] = 5\nprint(a_list)\n\n[1, 5, 3]\n\n\n\na_tuple[1] = 5\nprint(a_tuple)\n\nTypeError: 'tuple' object does not support item assignment\n\n\nHere we see that once the tuple is created, we cannot replace any of the values inside of the tuple.\n\ntype(a_tuple)\n\ntuple"
  },
  {
    "objectID": "Introduction_to_python_3.html#dictionaries",
    "href": "Introduction_to_python_3.html#dictionaries",
    "title": "4  Data types, if-statements and for-loops",
    "section": "4.3 Dictionaries",
    "text": "4.3 Dictionaries\nA dictionary is another way to store multiple items into one object. In dictionaries, however, this is done with keys and values. This can be useful for several reasons, one example is to store model settings, parameters or variable values for multiple scenarios.\n\nmy_dict = {'one': 'first', 'two': 'second'}\nmy_dict\n\n{'one': 'first', 'two': 'second'}\n\n\nWe can access dictionary items by their key:\n\nmy_dict['one']\n\n'first'\n\n\nAnd we can add new key-value pairs like that:\n\nmy_dict['third'] = 'three'\nmy_dict\n\n{'one': 'first', 'two': 'second', 'third': 'three'}\n\n\nDictionary items are key-value pairs. The keys are changeable and unique. The values are changable, but not necessarily unique.\n\nmy_dict['two'] = 'three'\nmy_dict\n\n{'one': 'first', 'two': 'three', 'third': 'three'}\n\n\n\nprint(\"Dictionary keys: \", my_dict.keys())\nprint(\"Dictionary values: \", my_dict.values())\nprint(\"Dictionary items (key, value): \", my_dict.items())\n\nDictionary keys:  dict_keys(['one', 'two', 'third'])\nDictionary values:  dict_values(['first', 'three', 'three'])\nDictionary items (key, value):  dict_items([('one', 'first'), ('two', 'three'), ('third', 'three')])"
  },
  {
    "objectID": "Introduction_to_python_3.html#for-loops",
    "href": "Introduction_to_python_3.html#for-loops",
    "title": "4  Data types, if-statements and for-loops",
    "section": "4.4 For loops",
    "text": "4.4 For loops\nLet’s have a look at our list again. One way to print each number is to use three print statements:\n\nnumbers = [5, 6, 7]\nprint(numbers[0])\nprint(numbers[1])\nprint(numbers[2])\n\n5\n6\n7\n\n\nA more efficient (less typing) and reliable way to print each element of a list is to loop over the list using a for loop:\n\nfor item in numbers:\n    print(item)\n\n5\n6\n7\n\n\nThe improved version uses a for loop to repeat an operation — in this case, printing — once for each item in a sequence. Note that (similar to if statements) Python needs indentation (4 whitespaces) to determine which lines of code are part of the for loop.\nIf we want to also get the index, we can use the built-in function enumerate:\n\nwords = [\"cat\", \"dog\", \"horse\"]\n\nfor index, item in enumerate(words):\n    print(index)\n    print(item)\n\n0\ncat\n1\ndog\n2\nhorse\n\n\nFor loops can also be used with dictionaries. Let’s take our dictionary from the previous section and inspect the dictionary items\n\nfor item in my_dict.items():\n    print(item, \"is of type\", type(item))\n\n('one', 'first') is of type &lt;class 'tuple'&gt;\n('two', 'three') is of type &lt;class 'tuple'&gt;\n('third', 'three') is of type &lt;class 'tuple'&gt;\n\n\nWe can extract the keys and values from the items directly in the for statement:\n\nfor key, value in my_dict.items():\n    print(key, \"-&gt;\", value)\n\none -&gt; first\ntwo -&gt; three\nthird -&gt; three\n\n\n\nExercises\nNow go back to your browser to morning_exercises.ipynb and continue with exercises 4-7.\nWhen you finished the exercises, continue to chapter Write your own Python function"
  },
  {
    "objectID": "Introduction_to_python_4.html#functions",
    "href": "Introduction_to_python_4.html#functions",
    "title": "5  Write your own Python function",
    "section": "5.1 Functions",
    "text": "5.1 Functions\nWe have already seen some built-in functions: e.g. print, type, len. And we have seen special functions that belong to a variable (python object) like my_dict.items() and my_list.append(). There are more built-in functions e.g. for mathematical operations:\n\nnumbers = [5, 6, 7]\nsum(numbers)\n\n18\n\n\nPlease refer to https://docs.python.org/3/library/functions.html for more built-in functions.\n\n5.1.1 Writing own functions\nWe will now turn to writing own functions. When should you write your own function?\n1. If the functionality is not covered by an out-of-the-box function like the built-in functions or another python package\n2. When code is getting pretty long, you can split it up into logical and reusable units\n3. When code is often reused, e.g. you are reading in tens of spreadsheets and you need to clean them all in the same way. Instead of typing the line of code over and over again, it is more elegant and looks cleaner to create a function.\n4. When code may be reused outside your current project. Scripts and the functions in a script can be imported in other scripts to be able to reuse them.\nA big advantage of not having duplicate code inside your script or in multiple scripts is that when you want to make a slight modification to a function, you only have to do this modification in one place, instead of multiple lines that are doing more or less similar things.\nPython provides for this by letting us define things called ‘functions’. Let’s start by defining a function fahr_to_celsius that converts temperatures from Fahrenheit to Celsius:\n\ndef fahr_to_celsius(temp_fahrenheit):\n    temp_celsius = (temp_fahrenheit - 32) * (5/9)\n    return temp_celsius\n\nThe function definition opens with the keyword def followed by the name of the function fahr_to_celsius and a parenthesized list of variables (in this case only one temp_fahrenheit). The body of the function — the statements that are executed when it runs — is indented below the definition line. The body concludes with a return keyword followed by the return value.\nWhen we call the function, the values we pass to it as arguments are assigned to the variables in the function definition so that we can use them inside the function. Inside the function, we use a return statement to send a result back to whoever asked for it.\nLet’s try running our function.\n\nfahr_to_celsius(98)\n\n36.66666666666667\n\n\n\nprint('freezing point of water:', fahr_to_celsius(32), 'C')\nprint('boiling point of water:', fahr_to_celsius(212), 'C')\n\nfreezing point of water: 0.0 C\nboiling point of water: 100.0 C\n\n\nHere we directly passed a value to the function. We can also call the function with a variable:\n\na = 0\nprint(fahr_to_celsius(a))\n\n-17.77777777777778\n\n\nWhat happens if you pass a variable name that is not defined yet?\n\nprint(fahr_to_celsius(b))\n\nNameError: name 'b' is not defined\n\n\n\nExercises\nNow go back to your browser to morning_exercises.ipynb and continue with exercises 8 and 9.\nWhen you finished the exercises, continue to the afternoon session"
  },
  {
    "objectID": "data-science-with-pandas-1.html",
    "href": "data-science-with-pandas-1.html",
    "title": "6  Reading and Exploring data",
    "section": "",
    "text": "7 Reading and Exploring data\nIn the previous chapters we started with the fundamentals of Python. Now we will continue with more practical examples of doing data analysis with Python using the Pandas library."
  },
  {
    "objectID": "data-science-with-pandas-1.html#libraries",
    "href": "data-science-with-pandas-1.html#libraries",
    "title": "6  Reading and Exploring data",
    "section": "7.1 Libraries",
    "text": "7.1 Libraries\nThe power of Python (and many programming languages) is in the libraries.\nA library (aka package) is a collection of files (aka python scripts) that contains functions that can be used to perform specific tasks. A library may also contain data. The functions in a library are typically related and used for a specific purpose, e.g. there are libraries for plotting, handling audio data and machine learning and many many more. Some libraries are built into python, but most packages need to be installed before you can use it.\nImportant to add: libraries are developed and maintained by other Python users. A popular library like Pandas has a large user base and the maintainers are supported by several funders, which makes it a reliable library that is updated very frequently. But this is not always the case, on the other side of the spectrum, a library can also be published once and not maintained at all."
  },
  {
    "objectID": "data-science-with-pandas-1.html#pandas",
    "href": "data-science-with-pandas-1.html#pandas",
    "title": "6  Reading and Exploring data",
    "section": "7.2 Pandas",
    "text": "7.2 Pandas\nThe python library Pandas is a popular open-source data analysis and data manipulation library for Python which was developed in 2008. The library has some similarities with R, mainly related to the DataFrame data type that is used to handle table like datasets.\nPandas is widely used in data analyses and machine learning, as it provides powerful tools for data handling and manipulation. Furthermore, it integrates well with other Python libraries for data analysis, machine learning, and statistical analysis, such as NumPy, Scikit-Learn, and StatsModels.\nIn this first chapter we will explore the main features of Pandas related to reading and exploring a dataset. In the following chapters we will go into finding, selecting and grouping data, merging datasets and visualization.\nFor this purpose, we will be using data from the Portal Project Teaching Database: real world example of life-history, population, and ecological data and, occasionally, a small ad-hoc dataset to exaplain DataFrame operations."
  },
  {
    "objectID": "data-science-with-pandas-1.html#preliminaries",
    "href": "data-science-with-pandas-1.html#preliminaries",
    "title": "6  Reading and Exploring data",
    "section": "7.3 Preliminaries",
    "text": "7.3 Preliminaries\nBefore we start our journey into Pandas functionalities, there are some preliminary operations to run.\nThe Pandas library is not a built-in library of python, it needs to be installed and loaded. Installations instructions for Pandas are in the the setup instructions for this course. Assuming you already installed it, let’s start importing the Pandas library and checking our installed version.\n\nimport pandas as pd\nprint(pd.__version__)\n\n1.5.3\n\n\nIt is also possible to just type import pandas, but we chose to give the library an ‘alias’: pd. The main reason is that we can now use functions from the library by typing pd instead of pandas (see the following line print(pd.__version__))\nTo be able to read the data files that we will be using, we need to specify the location of the files (also referred to as ‘path’). It is best practice to specify the path relative to the main project folder, so, in order to properly read our dataset, it’s important to check that we are working in the main project folder. In order to do that, we will load another library called ‘os’, containing all sort of tools to interact with our operating system. The function os.getcwd(), returns the current working directory (cwd).\n\nimport os\ncwd = os.getcwd()\nprint(cwd)\n\n/home/runner/work/workshop-introduction-to-python/workshop-introduction-to-python/book\n\n\nIf the current working directory ends with &lt;...&gt;/workshop-introduction-to-python, where &lt;...&gt; is whatever directory you chose to download and unzip the course material, you are in the right place. If not, use os.chdir(&lt;...&gt;) to change the working directory, where &lt;...&gt; is the full path of the workshop-introductions-to-python directory.\nLet’s store the relative path of our data into a variable and let’s check if the data file actually exists using the function os.path.exists()\n\ndata_file = '../course_materials/data/surveys.csv'\nprint(os.path.exists(data_file))\n\nTrue\n\n\nIf the result is True, we are all set up to go!"
  },
  {
    "objectID": "data-science-with-pandas-1.html#reading-data",
    "href": "data-science-with-pandas-1.html#reading-data",
    "title": "6  Reading and Exploring data",
    "section": "7.4 Reading data",
    "text": "7.4 Reading data\nThe very first operation we will perform is loading our data into a Pandas DataFrame using pd.read_csv().\n\nsurveys_df = pd.read_csv(data_file)\n\nprint(type(data_file))\nprint(type(surveys_df))\n\n&lt;class 'str'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\nPandas can read quite a large variety of formats like the comma-separated values (CSV) and Excel file formats.\nSometimes values in CSV files are separated using “;” or tabs. The default separator Pandas expects is a comma, if it is different it is necessary to specify the separator in an argument, e.g.: pd.read_csv(data_file, sep=\";\"). The documentation of pandas provides a full overview of the arguments you may use for this function.\nIn Jupyter Notebook or Jupyter Lab you can visualise the DataFrame simply by writing its name in a code cell and running the cell (in the same way you would display the value of any variable). Let’s have a look at our just created DataFrame:\n\nsurveys_df\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n35549 rows × 9 columns\n\n\n\nBy looking at the DataFrame we can finally understand what a DataFrame actually is: a 2-dimensional data structure storing different types of variables in columns. All rows in the DataFrame have a row index (starting from 0). The columns have names. The row indices and column names can be used to do operations on values in the column (we will go into this later).\nAs you can see Jupyter only prints the first and last 5 rows separated by ... . In this way the notebook remains clear and tidy (printing the whole DataFrame may result in a large table and a lot of scrolling to get to the next code cell).\nIt is, however, enough for a quick exploration of how the dataset looks like in terms of columns names, values, and potential reading errors.\n\nExercise 1\nNow go to the Jupyter Dashboard in your internet browser and open the notebook afternoon_exercises.ipynb and do exercise 1.\n\n\n\n\n\n\nNote\n\n\n\nAs you can see in this exercise a DataFrame object comes with several methods that can be applied to the DataFrame. A method is similar to a function, but it can only be applied to the object it belongs to and has a different notation than a function.\nCompare the notation of the function len: len(surveys_df)\nwith the DataFrame specific method shape: surveys_df.shape"
  },
  {
    "objectID": "data-science-with-pandas-1.html#exploring-data",
    "href": "data-science-with-pandas-1.html#exploring-data",
    "title": "6  Reading and Exploring data",
    "section": "7.5 Exploring data",
    "text": "7.5 Exploring data\nNow we will take a closer look into the actual values in the DataFrame. In order to do this it is helpful to have the column names at hand (easy for copy-pasting):\n\nprint(surveys_df.columns)\n\nIndex(['record_id', 'month', 'day', 'year', 'plot_id', 'species_id', 'sex',\n       'hindfoot_length', 'weight'],\n      dtype='object')\n\n\nAs you can see this gives you more information than just the column names. It is also possible to just print the column names using a for loop (see chapter 4):\n\nfor column in surveys_df.columns:\n    print(column)\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\nLet’s select the column weight in our DataFrame and let’s run some statistics on it\n\nsurveys_df['weight']\n\n0         NaN\n1         NaN\n2         NaN\n3         NaN\n4         NaN\n         ... \n35544     NaN\n35545     NaN\n35546    14.0\n35547    51.0\n35548     NaN\nName: weight, Length: 35549, dtype: float64\n\n\nNow let’s plot the values in a histogram:\n\nsurveys_df['weight'].plot(kind='hist')\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nDid you notice how easy it was to obtain a summary plot of a column of our DataFrame? We can repeat the same for every column with a single line of code.\n\nsurveys_df['hindfoot_length'].plot(kind='hist')\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nWe can also make quick scatterplots to explore the relation between two columns:\n\n%matplotlib inline # what this does will be discussed in the last chapter about visualization\nax1 = surveys_df.plot(x='weight', y='hindfoot_length', kind='scatter')\n\nUsageError: unrecognized arguments: # what this does will be discussed in the last chapter about visualization\n\n\n\nExercise 2\nNow go to the Jupyter Dashboard in your internet browser and continue with exercise 2.\nInstead of running the methods above one by one, we can obtain a statistical summary using the method .describe(). Let’s get a statistical summary for the weight column.\n\nsurveys_df['weight'].describe()\n\ncount    32283.000000\nmean        42.672428\nstd         36.631259\nmin          4.000000\n25%         20.000000\n50%         37.000000\n75%         48.000000\nmax        280.000000\nName: weight, dtype: float64\n\n\nThere are many more methods that can be used. For a complete overview, check out the documentation of Pandas. Some useful ones are the unique method to display all unique values in a certain column:\n\nsurveys_df['species_id'].unique()\n\narray(['NL', 'DM', 'PF', 'PE', 'DS', 'PP', 'SH', 'OT', 'DO', 'OX', 'SS',\n       'OL', 'RM', nan, 'SA', 'PM', 'AH', 'DX', 'AB', 'CB', 'CM', 'CQ',\n       'RF', 'PC', 'PG', 'PH', 'PU', 'CV', 'UR', 'UP', 'ZL', 'UL', 'CS',\n       'SC', 'BA', 'SF', 'RO', 'AS', 'SO', 'PI', 'ST', 'CU', 'SU', 'RX',\n       'PB', 'PL', 'PX', 'CT', 'US'], dtype=object)\n\n\nOr .nunique() to return the number of unique elements in a column.\n\nprint(surveys_df['plot_id'].nunique())\n\n24\n\n\nPerhaps we want to get some insight into the values for certain species or plots, in the next chapter we will go into making groups and selections."
  },
  {
    "objectID": "data-science-with-pandas-2.html#recap-load-the-data",
    "href": "data-science-with-pandas-2.html#recap-load-the-data",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.1 Recap: Load the data",
    "text": "7.1 Recap: Load the data\n\nimport pandas as pd\nsurveys_df = pd.read_csv('../course_materials/data/surveys.csv')\nsurveys_df.describe\n\n&lt;bound method NDFrame.describe of        record_id  month  day  year  plot_id species_id  sex  hindfoot_length  \\\n0              1      7   16  1977        2         NL    M             32.0   \n1              2      7   16  1977        3         NL    M             33.0   \n2              3      7   16  1977        2         DM    F             37.0   \n3              4      7   16  1977        7         DM    M             36.0   \n4              5      7   16  1977        3         DM    M             35.0   \n...          ...    ...  ...   ...      ...        ...  ...              ...   \n35544      35545     12   31  2002       15         AH  NaN              NaN   \n35545      35546     12   31  2002       15         AH  NaN              NaN   \n35546      35547     12   31  2002       10         RM    F             15.0   \n35547      35548     12   31  2002        7         DO    M             36.0   \n35548      35549     12   31  2002        5        NaN  NaN              NaN   \n\n       weight  \n0         NaN  \n1         NaN  \n2         NaN  \n3         NaN  \n4         NaN  \n...       ...  \n35544     NaN  \n35545     NaN  \n35546    14.0  \n35547    51.0  \n35548     NaN  \n\n[35549 rows x 9 columns]&gt;\n\n\nIn addition to learning about characteristics of our dataset as a whole, we may be interested in analyzing parts (subsets) of our data. For exampe we want to know how heavy our samples are:\n\nsurveys_df['weight'].describe()\n\ncount    32283.000000\nmean        42.672428\nstd         36.631259\nmin          4.000000\n25%         20.000000\n50%         37.000000\n75%         48.000000\nmax        280.000000\nName: weight, dtype: float64\n\n\nWe can also extract one specific metric if we wish:\n\nsurveys_df['weight'].min()\nsurveys_df['weight'].max()\nsurveys_df['weight'].mean()\nsurveys_df['weight'].std()\nsurveys_df['weight'].count()\n\n32283"
  },
  {
    "objectID": "data-science-with-pandas-2.html#selecting-data-using-column-names",
    "href": "data-science-with-pandas-2.html#selecting-data-using-column-names",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.2 Selecting data using column names",
    "text": "7.2 Selecting data using column names\nIn the morning session we saw how to get specific values from dictionaries using keys. We can do the same with DataFrames, in fact we have already accessed the values in a column by the column name. In this section we will discover how to select values, slices of data and subsets of a DataFrame. There are two ways of selecting columns, we have already used the first:\n\nsurveys_df['species_id']\n\n0         NL\n1         NL\n2         DM\n3         DM\n4         DM\n        ... \n35544     AH\n35545     AH\n35546     RM\n35547     DO\n35548    NaN\nName: species_id, Length: 35549, dtype: object\n\n\n\nsurveys_df.species_id\n\n0         NL\n1         NL\n2         DM\n3         DM\n4         DM\n        ... \n35544     AH\n35545     AH\n35546     RM\n35547     DO\n35548    NaN\nName: species_id, Length: 35549, dtype: object\n\n\nHow can we now create a DataFrame that only consists of the two columns plot_id and species_id?\n\nsurveys_df[['plot_id', 'species_id']]\n\n\n\n\n\n\n\n\nplot_id\nspecies_id\n\n\n\n\n0\n2\nNL\n\n\n1\n3\nNL\n\n\n2\n2\nDM\n\n\n3\n7\nDM\n\n\n4\n3\nDM\n\n\n...\n...\n...\n\n\n35544\n15\nAH\n\n\n35545\n15\nAH\n\n\n35546\n10\nRM\n\n\n35547\n7\nDO\n\n\n35548\n5\nNaN\n\n\n\n\n35549 rows × 2 columns\n\n\n\nWhy the double [[..]]? What is the difference between surveys_df['plot_id'] and surveys_df[['plot_id']]? Let us have a closer look:\n\nprint(type(surveys_df['plot_id']))\nprint(type(surveys_df[['plot_id']]))\n\n&lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\nThe DataFrame is organised as a dictionary with the column names as keys and row numbers as keys for the values stored in a row. surveys_df['plot_id'] will give us the value behind the key plot_id, in our case the series of numbers. When we ask for the values behind plot_id and species_id we need to give the DataFrame a list of column names like we did with surveys_df[['plot_id', 'species_id']]. When we pass a list of column names to a DataFrame, Pandas will execute for us the following code so that we do not have to worry about that any longer:\n\ncol1 = surveys_df['plot_id']\ncol2 = surveys_df['species_id']\naggregatedData = pd.DataFrame(dict(col1 = col1, col2 = col2))\naggregatedData\n\n\n\n\n\n\n\n\ncol1\ncol2\n\n\n\n\n0\n2\nNL\n\n\n1\n3\nNL\n\n\n2\n2\nDM\n\n\n3\n7\nDM\n\n\n4\n3\nDM\n\n\n...\n...\n...\n\n\n35544\n15\nAH\n\n\n35545\n15\nAH\n\n\n35546\n10\nRM\n\n\n35547\n7\nDO\n\n\n35548\n5\nNaN\n\n\n\n\n35549 rows × 2 columns"
  },
  {
    "objectID": "data-science-with-pandas-2.html#slicing-subsets-of-rows",
    "href": "data-science-with-pandas-2.html#slicing-subsets-of-rows",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.3 Slicing subsets of rows",
    "text": "7.3 Slicing subsets of rows\nSlicing using the [] operator selects a set of rows and/or columns from a DataFrame. To slice out a set of rows, you use the following syntax: data[start:stop]. When slicing in pandas the start bound is included in the output. The stop bound is not included. The slicing stops before the stop bound. So if you want to select rows 0, 1 and 2 your code would look like this:\n\nsurveys_df[0:3]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n\n\n\n\n\nWe can select specific ranges of our data in both the row and column directions using either label or integer-based indexing. The respective functions for that are called loc (label-based indexing) and iloc (integer-based indexing).\nLet’s have a look at iloc first. where we use the index of a row and/or column to select it. In the example below we select the first three entries and the columns month, day and year (the second, third and fourth column, remember indexing starts at 0 on Python). The first range of numbers selects the rows, the second the columns:\n\n# iloc[row slicing, column slicing]\nsurveys_df.iloc[0:3, 1:4]\n\n\n\n\n\n\n\n\nmonth\nday\nyear\n\n\n\n\n0\n7\n16\n1977\n\n\n1\n7\n16\n1977\n\n\n2\n7\n16\n1977\n\n\n\n\n\n\n\nWe can achieve the same with the function loc, only instead of column indices, we use the column labels this time. So, we need to know the names of the columns:\n\nsurveys_df.loc[0:3, ['month', 'day', 'year']]\n\n\n\n\n\n\n\n\nmonth\nday\nyear\n\n\n\n\n0\n7\n16\n1977\n\n\n1\n7\n16\n1977\n\n\n2\n7\n16\n1977\n\n\n3\n7\n16\n1977\n\n\n\n\n\n\n\nAnd there is a third way: In a forst step we select the columns by their names surveys_df[['month', 'day', 'year']]. From the resulting DataFrame we then, in a second step, select the first three rows [0:3]. Putting the two steps together, the code looks like this:\n\nsurveys_df[['month', 'day', 'year']][0:3]\n\n\n\n\n\n\n\n\nmonth\nday\nyear\n\n\n\n\n0\n7\n16\n1977\n\n\n1\n7\n16\n1977\n\n\n2\n7\n16\n1977\n\n\n\n\n\n\n\n\n7.3.1 Interactive Part\nLet us further explore the loc and iloc functions as they are more powerful. Have a look at the examples below and predict their outcome before hitting enter.\n\n# Select all columns for rows of index values 0 and 10\nsurveys_df.loc[[0, 10], :]\n\n# What does this do?\nsurveys_df.loc[0, ['species_id', 'plot_id', 'weight']]\n\n# What happens when you type the code below?\nsurveys_df.loc[[0, 10, 35549], :]\n\nKeyError: '[35549] not in index'\n\n\nWe can also extract single values from our DataFrame:\n\n# data.iloc[row, column]\nsurveys_df.iloc[2, 6]\n\n'F'\n\n\n\n\n7.3.2 Summary: Selecting slices, rows and columns\nIn the first two methods we extract the column specifying its name. The third method is essentially identical to the first one as the 6th (index 5) element of the Series surveys_df.columns is species_id. The fourth method uses the method iloc to select all the rows of the 6th column.\n\n# By name\n# --------------------------------------\n# Method1\nplot_id_1 = surveys_df['species_id']\n\n# Method2\nplot_id_2 = surveys_df.species_id\n# --------------------------------------\n\n# By location\n# --------------------------------------\n# Method3\nplot_id_3 = surveys_df[surveys_df.columns[5]]\n\n# Method4\nplot_id_4 = surveys_df.iloc[:,5]\n# --------------------------------------\n\n\nExercise 3 to 5\nNow go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercises 3 to 5.\n\n\n7.3.3 Subsetting Data according to user-defined criteria\nWe can extract subsets of our DataFrame following the general syntax data_frame[&lt;condition_on_data&gt;]  is a conditional statement on the DataFrame content itself. You may think at the conditional statement as a question or query you ask to your DataFrame. Here there are some examples:\n\n# What are the data collected in the year 2002?\nsurveys_df[surveys_df.year == 2002]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n33320\n33321\n1\n12\n2002\n1\nDM\nM\n38.0\n44.0\n\n\n33321\n33322\n1\n12\n2002\n1\nDO\nM\n37.0\n58.0\n\n\n33322\n33323\n1\n12\n2002\n1\nPB\nM\n28.0\n45.0\n\n\n33323\n33324\n1\n12\n2002\n1\nAB\nNaN\nNaN\nNaN\n\n\n33324\n33325\n1\n12\n2002\n1\nDO\nM\n35.0\n29.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2229 rows × 9 columns\n\n\n\n\n# What are the data NOT collected in the year 2002?\nsurveys_df[surveys_df.year != 2002]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n33315\n33316\n12\n16\n2001\n11\nNaN\nNaN\nNaN\nNaN\n\n\n33316\n33317\n12\n16\n2001\n13\nNaN\nNaN\nNaN\nNaN\n\n\n33317\n33318\n12\n16\n2001\n14\nNaN\nNaN\nNaN\nNaN\n\n\n33318\n33319\n12\n16\n2001\n15\nNaN\nNaN\nNaN\nNaN\n\n\n33319\n33320\n12\n16\n2001\n16\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n33320 rows × 9 columns\n\n\n\n\n# What are the data NOT collected in the year 2002? (different syntax)\nsurveys_df[~(surveys_df.year == 2002)]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n33315\n33316\n12\n16\n2001\n11\nNaN\nNaN\nNaN\nNaN\n\n\n33316\n33317\n12\n16\n2001\n13\nNaN\nNaN\nNaN\nNaN\n\n\n33317\n33318\n12\n16\n2001\n14\nNaN\nNaN\nNaN\nNaN\n\n\n33318\n33319\n12\n16\n2001\n15\nNaN\nNaN\nNaN\nNaN\n\n\n33319\n33320\n12\n16\n2001\n16\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n33320 rows × 9 columns\n\n\n\nOur filtering conditions may be very specific, they can target different columns in the DataFrame, and they can be combined using the logical operator “&” which means and:\n\n# What are the data collected between 2000 and 2002 on female species?\nsurveys_df[(surveys_df.year &gt;= 2000) & (surveys_df.year &lt;= 2002) & (surveys_df.sex == 'F')]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n30158\n30159\n1\n8\n2000\n1\nPP\nF\n22.0\n17.0\n\n\n30160\n30161\n1\n8\n2000\n1\nPP\nF\n21.0\n17.0\n\n\n30164\n30165\n1\n8\n2000\n1\nPP\nF\n22.0\n15.0\n\n\n30168\n30169\n1\n8\n2000\n2\nPB\nF\n25.0\n24.0\n\n\n30171\n30172\n1\n8\n2000\n2\nNL\nF\n30.0\n137.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35539\n35540\n12\n31\n2002\n15\nPB\nF\n26.0\n23.0\n\n\n35540\n35541\n12\n31\n2002\n15\nPB\nF\n24.0\n31.0\n\n\n35541\n35542\n12\n31\n2002\n15\nPB\nF\n26.0\n29.0\n\n\n35542\n35543\n12\n31\n2002\n15\nPB\nF\n27.0\n34.0\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n\n\n2582 rows × 9 columns\n\n\n\nWe have also an operator for or. Below we filter for rows with collected data on female species in the year 2000 or 2002. “Give me all data where sex is Female and data is collected in 2000 or 2002”.\nThe method isin() allows to specify a range of “permitted” values for a certain column. Here it follows another example:\n\nsurveys_df[(surveys_df.year == 2000) & (surveys_df.sex == 'F') & (surveys_df.month.isin([1,3,4]))]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n30158\n30159\n1\n8\n2000\n1\nPP\nF\n22.0\n17.0\n\n\n30160\n30161\n1\n8\n2000\n1\nPP\nF\n21.0\n17.0\n\n\n30164\n30165\n1\n8\n2000\n1\nPP\nF\n22.0\n15.0\n\n\n30168\n30169\n1\n8\n2000\n2\nPB\nF\n25.0\n24.0\n\n\n30171\n30172\n1\n8\n2000\n2\nNL\nF\n30.0\n137.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n30637\n30638\n4\n30\n2000\n20\nPP\nF\n22.0\n20.0\n\n\n30640\n30641\n4\n30\n2000\n20\nNL\nF\n30.0\nNaN\n\n\n30645\n30646\n4\n30\n2000\n24\nPP\nF\n20.0\n17.0\n\n\n30647\n30648\n4\n30\n2000\n17\nDM\nF\n36.0\n46.0\n\n\n30648\n30649\n4\n30\n2000\n17\nDO\nF\n36.0\n59.0\n\n\n\n\n156 rows × 9 columns"
  },
  {
    "objectID": "data-science-with-pandas-2.html#dataframe-cleaning",
    "href": "data-science-with-pandas-2.html#dataframe-cleaning",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.4 DataFrame Cleaning",
    "text": "7.4 DataFrame Cleaning\nA simple exploration of our DataFrame showed us that there are columns full of invalid values (NaN). One of the most important preliminary operations of data analysis is cleaning your data set, i.e. “getting rid” of non-numerical or non-character values. we want to make sure that our data only contains meaningful values.\nNow that we mastered selecting, slicing, and subsetting, we can easily clean our DataFrame with few lines of code. Let us have a look at the function isnull. It is a Pandas function which we imported at the beginning with import pandas as pd. Now we can call the functin like this:\n\npd.isnull(4)\n\nFalse\n\n\n\npd.isnull([1, 2, 3, '', dict(), None])\n\narray([False, False, False, False, False,  True])\n\n\nWe can pass single values or array-like values to the function. The function will then check for us whether each value is NaN (Not a Number) or None and return a boolean array. Note, that values like the empty string (a strin without any characters in it) or an empty dictionary etc will not count as null value, they do have a type, they only do not contain any values but they are something. null values in python are only NaN and None. When you read in tabular data into a DataFrame empty cells will be shown as NaN. None stands for the type NoneType, which we will not dive into further in this workshop.\nWith all that kowledge we can now detect null values in the column weight and do something about it. Let us have a look how many null values we can find:\n\nprint(\"Length of the full dataframe: \", len(surveys_df))\npd.isnull(surveys_df.weight) # boolean array indicating where null values are found\nsurveys_df[pd.isnull(surveys_df.weight)] # all lines that have a null value in the column weight\nlen(surveys_df[pd.isnull(surveys_df.weight)]) # length\n\nLength of the full dataframe:  35549\n\n\n3266\n\n\nAs you can see, in our whole dataset 3266 weight values are not usable. We need to do something with those values.\nAnother thing that would not make sense are negative weights. Let’s check whether the remaining 32283 values in the weight column are positive:\n\nlen(surveys_df[surveys_df.weight &gt; 0])\n\n32283\n\n\nAs we see, we have 32283 non-negative weighht values. The remaining 3266 values in the weight column are not set, so they are null. How can we impute the values? Let us have a look at the average weight:\n\nsurveys_df.weight.mean()\n\n42.672428212991356\n\n\nA smooth run, without errors or warnings. As we said several times, Pandas is a library designed for data analysis and when performing data analysis it is very common to deal with not numeric values. In particular, the .mean() method has an argument called skipna that when set True (default value, so we do not need to specify it) excludes NaN values. This means that, in this case, Pandas simply ignores whatever it is not numeric and it performs computations only on numeric values.\nIf we are not happy with Pandas default behaviour, we can manually decide which value to assign to cells that contain null values. One possible choice is setting them to zero. To do that, we just need to apply the method .fillna(&lt;value&gt;), where &lt;value&gt; is the number we want to substitute to the null value with (in our case, 0).\n\ncleaned_weight1 = surveys_df.weight.fillna(0)\ncleaned_weight_ave1 = cleaned_weight1.mean()\nprint(cleaned_weight_ave1)\n\n38.751976145601844\n\n\nYou see that when filling the null values with 0, the average weight decreases. This is because the mean is now computed on data with many more zeros compared to the previous one. Conscious of this problem, we may now choose a more appropriate value to “fill” our null values. How about we use the “clean” mean of our first computation?\n\ncleaned_weight2 = surveys_df.weight.fillna(surveys_df.weight.mean())\ncleaned_weight_ave2 = cleaned_weight2.mean()\nprint(cleaned_weight_ave2)\n\n42.672428212991356\n\n\nThis time we obtain exactly the same result of our first computation, this is because we substituted the null values with a mean computed excluding the null values.\n\nExercise 6 and 7\nNow go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercises 6 and 7."
  },
  {
    "objectID": "data-science-with-pandas-2.html#grouping",
    "href": "data-science-with-pandas-2.html#grouping",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.5 Grouping",
    "text": "7.5 Grouping\nWe often want to calculate summary statistics grouped by subsets or attributes within fields of our data. For example, we might want to calculate the average weight of all individuals per site.\nAs we have seen above we can calculate basic statistics for all records in a single column using the syntax below:\n\nsurveys_df['weight'].describe()\n\ncount    32283.000000\nmean        42.672428\nstd         36.631259\nmin          4.000000\n25%         20.000000\n50%         37.000000\n75%         48.000000\nmax        280.000000\nName: weight, dtype: float64\n\n\nIf we want to summarize by one or more variables, for example sex, we can use Pandas’ .groupby() method. Once we’ve created a groupby DataFrame, we can quickly calculate summary statistics by a group of our choice.\n\ngrouped_data = surveys_df.groupby('sex')\ngrouped_data.describe()\n\n\n\n\n\n\n\n\nrecord_id\nmonth\n...\nhindfoot_length\nweight\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\n15690.0\n18036.412046\n10423.089000\n3.0\n8917.50\n18075.5\n27250.00\n35547.0\n15690.0\n6.587253\n...\n36.0\n64.0\n15303.0\n42.170555\n36.847958\n4.0\n20.0\n34.0\n46.0\n274.0\n\n\nM\n17348.0\n17754.835601\n10132.203323\n1.0\n8969.75\n17727.5\n26454.25\n35548.0\n17348.0\n6.396184\n...\n36.0\n58.0\n16879.0\n42.995379\n36.184981\n4.0\n20.0\n39.0\n49.0\n280.0\n\n\n\n\n2 rows × 56 columns\n\n\n\nThe output is a bit overwhelming. Let’s just have a look at one statistical value, the mean, to understand what is happening here:\n\ngrouped_data.mean()\n\n/tmp/ipykernel_2298/1133710423.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  grouped_data.mean()\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nhindfoot_length\nweight\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\nF\n18036.412046\n6.587253\n15.880943\n1990.644997\n11.440854\n28.836780\n42.170555\n\n\nM\n17754.835601\n6.396184\n16.078799\n1990.480401\n11.098282\n29.709578\n42.995379\n\n\n\n\n\n\n\nWe see that the data is divided into two groups, one group where the value in the column sex equals “F” and another group where the value in the column sex equals “M”. The statistics is then calculated for all samples in that specific group for each of the columns in the dataframe. Note that samples annotated with sex equals NaN and column values with NaN are left out.\n\ngrouped_data = surveys_df.groupby(...)\ngrouped_data[...].mean()\n\nTypeError: 'ellipsis' object is not callable"
  },
  {
    "objectID": "data-science-with-pandas-2.html#structure-of-a-groupby-object",
    "href": "data-science-with-pandas-2.html#structure-of-a-groupby-object",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.6 Structure of a groupby object",
    "text": "7.6 Structure of a groupby object\nWe can investigate which rows are assigned to which group as follows:\n\nprint(type(grouped_data.groups)) # dictionary\nprint(\"Plot ids: \", grouped_data.groups.keys()) # keys are the unique values of the column we grouped by\nprint(\"Rows belonging to plot id \", 1, \": \", grouped_data.groups[1]) # values are row indexes \n\n&lt;class 'pandas.io.formats.printing.PrettyDict'&gt;\nPlot ids:  dict_keys(['F', 'M'])\n\n\nKeyError: 1"
  },
  {
    "objectID": "data-science-with-pandas-2.html#grouping-by-multiple-columns",
    "href": "data-science-with-pandas-2.html#grouping-by-multiple-columns",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.7 Grouping by multiple columns",
    "text": "7.7 Grouping by multiple columns\nNow let’s have a look at a more complex grouping example. We want an overview statistics of the weight of all females and males by plot id. So in fact we want to group by sex and by plot_id at the same time.\nThis will give us exactly 48 groups for our survey data: - female, plot id = 1 - female, plot id = 2 - … - female, plot id = 24 - male, plot id = 1 - … - male, plot id = 24\nWhy 48 groups? We have 24 unique values for plot_id. Per plot we have two groups of samples, female and male. Hence, the grouping returns 48 groups.\n\ngrouped_data = surveys_df.groupby(['sex', 'plot_id'])\ngrouped_data[\"weight\"].describe()\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsex\nplot_id\n\n\n\n\n\n\n\n\n\n\n\n\nF\n1\n826.0\n46.311138\n33.240958\n5.0\n26.00\n40.0\n50.00\n196.0\n\n\n2\n954.0\n52.561845\n45.547697\n5.0\n25.00\n40.0\n51.00\n274.0\n\n\n3\n873.0\n31.215349\n30.687451\n4.0\n15.00\n23.0\n34.00\n199.0\n\n\n4\n850.0\n46.818824\n33.560664\n5.0\n28.00\n40.0\n47.00\n200.0\n\n\n5\n516.0\n40.974806\n36.396966\n5.0\n21.00\n35.0\n45.00\n248.0\n\n\n6\n721.0\n36.352288\n29.513333\n5.0\n19.00\n29.0\n41.00\n188.0\n\n\n7\n326.0\n20.006135\n17.895937\n6.0\n12.00\n17.0\n23.00\n170.0\n\n\n8\n817.0\n45.623011\n31.045426\n5.0\n25.00\n42.0\n50.00\n178.0\n\n\n9\n823.0\n53.618469\n35.572793\n6.0\n35.00\n43.0\n54.00\n177.0\n\n\n10\n138.0\n17.094203\n14.074820\n7.0\n10.00\n13.0\n20.00\n130.0\n\n\n11\n796.0\n43.515075\n29.627049\n5.0\n27.00\n40.0\n46.00\n208.0\n\n\n12\n1040.0\n49.831731\n43.790247\n6.0\n26.00\n41.0\n48.25\n264.0\n\n\n13\n610.0\n40.524590\n36.109806\n5.0\n21.00\n31.0\n42.00\n192.0\n\n\n14\n692.0\n47.355491\n29.563455\n5.0\n37.00\n43.0\n48.00\n211.0\n\n\n15\n467.0\n26.670236\n31.983137\n4.0\n12.50\n18.0\n26.00\n198.0\n\n\n16\n211.0\n25.810427\n20.902314\n4.0\n13.00\n21.0\n31.00\n158.0\n\n\n17\n874.0\n48.176201\n37.485528\n6.0\n27.00\n41.0\n49.00\n192.0\n\n\n18\n740.0\n36.963514\n35.184417\n5.0\n17.00\n28.5\n40.00\n212.0\n\n\n19\n514.0\n21.978599\n14.008822\n6.0\n12.00\n20.0\n29.00\n139.0\n\n\n20\n631.0\n52.624406\n55.257665\n5.0\n17.00\n30.0\n48.00\n220.0\n\n\n21\n596.0\n25.974832\n22.619863\n4.0\n11.00\n24.0\n31.00\n188.0\n\n\n22\n646.0\n53.647059\n38.588538\n5.0\n29.00\n39.0\n54.00\n161.0\n\n\n23\n163.0\n20.564417\n18.933945\n8.0\n12.00\n16.0\n23.00\n199.0\n\n\n24\n479.0\n47.914405\n49.112574\n6.0\n21.00\n33.0\n44.00\n251.0\n\n\nM\n1\n1072.0\n55.950560\n41.035686\n4.0\n37.00\n46.0\n54.00\n231.0\n\n\n2\n1114.0\n51.391382\n46.690887\n5.0\n24.00\n42.0\n50.00\n278.0\n\n\n3\n827.0\n34.163241\n40.260426\n5.0\n13.00\n23.0\n39.00\n250.0\n\n\n4\n1010.0\n48.888119\n32.254168\n4.0\n32.00\n44.5\n50.00\n187.0\n\n\n5\n573.0\n40.708551\n31.250967\n6.0\n21.00\n40.0\n49.00\n240.0\n\n\n6\n739.0\n36.867388\n30.867779\n6.0\n18.00\n31.0\n46.00\n241.0\n\n\n7\n303.0\n21.194719\n23.971252\n4.0\n11.00\n17.0\n23.00\n235.0\n\n\n8\n962.0\n49.641372\n34.820355\n5.0\n29.00\n45.0\n52.00\n173.0\n\n\n9\n984.0\n49.519309\n31.888023\n6.0\n37.00\n46.0\n50.00\n275.0\n\n\n10\n139.0\n19.971223\n25.061068\n4.0\n10.00\n12.0\n22.00\n237.0\n\n\n11\n994.0\n43.366197\n28.425105\n6.0\n25.00\n43.0\n49.00\n212.0\n\n\n12\n1174.0\n48.909710\n39.301038\n7.0\n25.25\n43.0\n50.00\n280.0\n\n\n13\n757.0\n40.097754\n31.753448\n6.0\n20.00\n34.0\n47.00\n241.0\n\n\n14\n1029.0\n45.159378\n25.272173\n5.0\n35.00\n44.0\n50.00\n222.0\n\n\n15\n401.0\n27.523691\n38.631271\n4.0\n10.00\n18.0\n25.00\n259.0\n\n\n16\n265.0\n23.811321\n14.663726\n5.0\n11.00\n20.0\n35.00\n61.0\n\n\n17\n1011.0\n47.558853\n34.082010\n4.0\n27.00\n45.0\n51.00\n216.0\n\n\n18\n607.0\n43.546952\n41.864279\n7.0\n18.00\n33.0\n48.00\n256.0\n\n\n19\n567.0\n20.306878\n12.553954\n4.0\n10.00\n19.0\n25.00\n100.0\n\n\n20\n588.0\n44.197279\n43.361503\n5.0\n17.00\n34.0\n47.00\n223.0\n\n\n21\n431.0\n22.772622\n18.984554\n4.0\n9.00\n19.0\n32.00\n190.0\n\n\n22\n648.0\n54.572531\n38.841066\n6.0\n31.00\n44.0\n53.00\n212.0\n\n\n23\n205.0\n18.941463\n17.979740\n4.0\n10.00\n12.0\n22.00\n131.0\n\n\n24\n479.0\n39.321503\n42.003947\n4.0\n17.00\n24.0\n45.00\n230.0"
  },
  {
    "objectID": "data-science-with-pandas-2.html#counting-and-plotting",
    "href": "data-science-with-pandas-2.html#counting-and-plotting",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.8 Counting and plotting",
    "text": "7.8 Counting and plotting\nAnother very useful outcome of grouping is the possibility of performing selective counting. For example, let’s see how to count the number of records per species. We just need to remember that each species has a unique ID and that records are identified by another ID stored in the column record ID. We will first group our data according to the species ID and then, for each group, we will count the number of records. Several consecutive operations that, once again, Pandas allows us to execute in a single line.\n\nspecies_counts = surveys_df.groupby('species_id')['record_id'].count()\nprint(type(grouped_species_counts))\nspecies_counts\n\nNameError: name 'grouped_species_counts' is not defined\n\n\nWe can also plot the information for better overview. We will learn more about plotting after the next chapter.\n\nspecies_counts.plot(kind='bar')\n\n&lt;AxesSubplot: xlabel='species_id'&gt;"
  },
  {
    "objectID": "data-science-with-pandas-2.html#summary-grouping",
    "href": "data-science-with-pandas-2.html#summary-grouping",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.9 Summary grouping",
    "text": "7.9 Summary grouping\nGrouping is one of the most common operation in data analysis. Data often consists of different measurements on the same samples. In many cases we are not only interested in one particular measurement but in the cross product of measurements. In the picture below we labeled samples with green lines, blue dots and red lines. We are now interested how these three different groups relate to each other given the all other measurements in the dataframe. Pandas’ groupby function gives us the means to compare these three groups with several built-in statistical methods.\n\n\n\nGrouping sketch\n\n\n\nExercise 8 to 10\nNow go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercises 8 to 10.\nAfter you finished the exercises please come back to this document and continue with the following chapter."
  },
  {
    "objectID": "data-science-with-pandas-3.html#combining-dataframes",
    "href": "data-science-with-pandas-3.html#combining-dataframes",
    "title": "8  Combining DataFrames",
    "section": "8.1 Combining DataFrames",
    "text": "8.1 Combining DataFrames\nPreviously, we have seen how to analyze and manipulate data in a single DataFrame. However, you will often find data saved into different files and, therefore, you may need to deal with several different pandas DataFrames. In this session we will explore different ways of combining DataFrames into a single DataFrame.\nLet’s start loading the pandas library, reading two data sets into DataFrames, and having a quick look at the tabular data: surveys.csv and species.csv\n\nimport pandas as pd\n\n\nsurveys_df = pd.read_csv(\"../course_materials/data/surveys.csv\", keep_default_na=False, na_values=[\"\"])\nspecies_df = pd.read_csv(\"../course_materials/data/species.csv\", keep_default_na=False, na_values=[\"\"])\n\n\nprint(surveys_df.info())\nsurveys_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 35549 entries, 0 to 35548\nData columns (total 9 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   record_id        35549 non-null  int64  \n 1   month            35549 non-null  int64  \n 2   day              35549 non-null  int64  \n 3   year             35549 non-null  int64  \n 4   plot_id          35549 non-null  int64  \n 5   species_id       34786 non-null  object \n 6   sex              33038 non-null  object \n 7   hindfoot_length  31438 non-null  float64\n 8   weight           32283 non-null  float64\ndtypes: float64(2), int64(5), object(2)\nmemory usage: 2.4+ MB\nNone\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n\n\n\n\n\n\nprint(species_df.info())\nspecies_df.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 54 entries, 0 to 53\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   species_id  54 non-null     object\n 1   genus       54 non-null     object\n 2   species     54 non-null     object\n 3   taxa        54 non-null     object\ndtypes: object(4)\nmemory usage: 1.8+ KB\nNone\n\n\n\n\n\n\n\n\n\nspecies_id\ngenus\nspecies\ntaxa\n\n\n\n\n0\nAB\nAmphispiza\nbilineata\nBird\n\n\n1\nAH\nAmmospermophilus\nharrisi\nRodent\n\n\n2\nAS\nAmmodramus\nsavannarum\nBird\n\n\n3\nBA\nBaiomys\ntaylori\nRodent\n\n\n4\nCB\nCampylorhynchus\nbrunneicapillus\nBird\n\n\n\n\n\n\n\nWe now have two DataFrames. The first, surveys_df, contains information on individuals of a species recorded in a survey, while the second, species_df, contains more detailed information on each species."
  },
  {
    "objectID": "data-science-with-pandas-3.html#concatenating-dataframes",
    "href": "data-science-with-pandas-3.html#concatenating-dataframes",
    "title": "8  Combining DataFrames",
    "section": "8.2 Concatenating DataFrames",
    "text": "8.2 Concatenating DataFrames\nThe first way we will combine DataFrames is concatenation, i.e. simply putting DataFrames one after the other either verically or horizontally.\nConcatenation can be used if the DataFrames are similar, meaning that they either have the same rows or columns. We will see examples of this later.\nTo concatenate two DataFrames you will use the function pd.concat(), specifying as arguments the DataFrames to concatenate and axis=0 or axis=1 for vertical or horizontal concatenation, respectively.\nLet us first obtain two small DataFrames from the larger surveys.csv dataset.\n\n# Subsetting DataFrames\nsurveys_df_sub_first10 = surveys_df.head(10)\nsurveys_df_sub_last10  = surveys_df.tail(10)\n\nWe now have two DataFrames, one with the first ten rows of the original dataset, and another with the last ten rows.\n\n8.2.1 Vertical concatenation\nLet’s start with vertical stacking. In this case the two DataFrames are simply stacked ‘on top of’ eachother (remember to specify axis=0).\n\n\n\nVertical stacking can be understood as combining two DataFrames that have different sets of the same type of data. In our example, it may be that one field researcher has registered the first ten entries, and another did the last ten, both using the same laboratory sheets. They both wrote down the same information (weight, species, and so on) of all different individuals. If we combine them, we have one list of twenty records, rather than two lists of ten.\n\n# Stack the DataFrames on top of each other\nvertical_stack = pd.concat([surveys_df_sub_first10, surveys_df_sub_last10], axis=0)\n\n\nprint(vertical_stack.info())\nvertical_stack\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 20 entries, 0 to 35548\nData columns (total 9 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   record_id        20 non-null     int64  \n 1   month            20 non-null     int64  \n 2   day              20 non-null     int64  \n 3   year             20 non-null     int64  \n 4   plot_id          20 non-null     int64  \n 5   species_id       19 non-null     object \n 6   sex              16 non-null     object \n 7   hindfoot_length  15 non-null     float64\n 8   weight           6 non-null      float64\ndtypes: float64(2), int64(5), object(2)\nmemory usage: 1.6+ KB\nNone\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n5\n6\n7\n16\n1977\n1\nPF\nM\n14.0\nNaN\n\n\n6\n7\n7\n16\n1977\n2\nPE\nF\nNaN\nNaN\n\n\n7\n8\n7\n16\n1977\n1\nDM\nM\n37.0\nNaN\n\n\n8\n9\n7\n16\n1977\n1\nDM\nF\n34.0\nNaN\n\n\n9\n10\n7\n16\n1977\n6\nPF\nF\n20.0\nNaN\n\n\n35539\n35540\n12\n31\n2002\n15\nPB\nF\n26.0\n23.0\n\n\n35540\n35541\n12\n31\n2002\n15\nPB\nF\n24.0\n31.0\n\n\n35541\n35542\n12\n31\n2002\n15\nPB\nF\n26.0\n29.0\n\n\n35542\n35543\n12\n31\n2002\n15\nPB\nF\n27.0\n34.0\n\n\n35543\n35544\n12\n31\n2002\n15\nUS\nNaN\nNaN\nNaN\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nThe resulting DataFrame (vertical_stack) consists, as expected, of 20 rows. These are the result of the first and last 10 rows of our original DataFrame surveys_df.\nYou may have noticed that the last ten rows have very high index, not consecutive with the first ten rows. This is because concatenation preserves the indices of the two original DataFrames. If you want a brand new set of indices for your concateneted DataFrame, simply reset the indices using the method .reset_index(). Notice that this adds a column index to your DataFrame, that maintains the original index. If you pass drop=True into the function, you will avoid the addition of this column.\n\nvertical_stack.reset_index()\n\n\n\n\n\n\n\n\nindex\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n5\n5\n6\n7\n16\n1977\n1\nPF\nM\n14.0\nNaN\n\n\n6\n6\n7\n7\n16\n1977\n2\nPE\nF\nNaN\nNaN\n\n\n7\n7\n8\n7\n16\n1977\n1\nDM\nM\n37.0\nNaN\n\n\n8\n8\n9\n7\n16\n1977\n1\nDM\nF\n34.0\nNaN\n\n\n9\n9\n10\n7\n16\n1977\n6\nPF\nF\n20.0\nNaN\n\n\n10\n35539\n35540\n12\n31\n2002\n15\nPB\nF\n26.0\n23.0\n\n\n11\n35540\n35541\n12\n31\n2002\n15\nPB\nF\n24.0\n31.0\n\n\n12\n35541\n35542\n12\n31\n2002\n15\nPB\nF\n26.0\n29.0\n\n\n13\n35542\n35543\n12\n31\n2002\n15\nPB\nF\n27.0\n34.0\n\n\n14\n35543\n35544\n12\n31\n2002\n15\nUS\nNaN\nNaN\nNaN\n\n\n15\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n16\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n17\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n18\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n19\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nExercise 11\nNow go to the Jupyter Dashboard in your internet browser and continue with exercise 11.\n\n\n8.2.2 Horizontal concatenation\nIt’s now time to try horizontal stacking. In this case the two DataFrames are simply put one after the other (remember to specify axis=1).\n\n\n\nHorizontal stacking can be understood as combining two DataFrames that have different measurements on the same observed objects. In our example, it may be that one field researcher has registered the weight and hindfoot length of an individual, and another wrote down their species and sex. They both registered different information of the same individuals. If we combine them, we have one list with all the information of the individual, rather than two lists with partial information.\nWe now go back to our DataFrames with 10 survey result each, and concatenate those. In this case, as a result, we would expect a DataFrame with the same number of rows of the original ones (10 row) and twice the number of columns (18 columns).\n\n# Place the DataFrames side by side\nhorizontal_stack = pd.concat([surveys_df_sub_first10, surveys_df_sub_last10], axis=1)\n\n\nprint(horizontal_stack.info())\nhorizontal_stack\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 20 entries, 0 to 35548\nData columns (total 18 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   record_id        10 non-null     float64\n 1   month            10 non-null     float64\n 2   day              10 non-null     float64\n 3   year             10 non-null     float64\n 4   plot_id          10 non-null     float64\n 5   species_id       10 non-null     object \n 6   sex              10 non-null     object \n 7   hindfoot_length  9 non-null      float64\n 8   weight           0 non-null      float64\n 9   record_id        10 non-null     float64\n 10  month            10 non-null     float64\n 11  day              10 non-null     float64\n 12  year             10 non-null     float64\n 13  plot_id          10 non-null     float64\n 14  species_id       9 non-null      object \n 15  sex              6 non-null      object \n 16  hindfoot_length  6 non-null      float64\n 17  weight           6 non-null      float64\ndtypes: float64(14), object(4)\nmemory usage: 3.0+ KB\nNone\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1.0\n7.0\n16.0\n1977.0\n2.0\nNL\nM\n32.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2.0\n7.0\n16.0\n1977.0\n3.0\nNL\nM\n33.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n3.0\n7.0\n16.0\n1977.0\n2.0\nDM\nF\n37.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n4.0\n7.0\n16.0\n1977.0\n7.0\nDM\nM\n36.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n5.0\n7.0\n16.0\n1977.0\n3.0\nDM\nM\n35.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\n6.0\n7.0\n16.0\n1977.0\n1.0\nPF\nM\n14.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6\n7.0\n7.0\n16.0\n1977.0\n2.0\nPE\nF\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\n8.0\n7.0\n16.0\n1977.0\n1.0\nDM\nM\n37.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8\n9.0\n7.0\n16.0\n1977.0\n1.0\nDM\nF\n34.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9\n10.0\n7.0\n16.0\n1977.0\n6.0\nPF\nF\n20.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n35539\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n35540.0\n12.0\n31.0\n2002.0\n15.0\nPB\nF\n26.0\n23.0\n\n\n35540\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n35541.0\n12.0\n31.0\n2002.0\n15.0\nPB\nF\n24.0\n31.0\n\n\n35541\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n35542.0\n12.0\n31.0\n2002.0\n15.0\nPB\nF\n26.0\n29.0\n\n\n35542\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n35543.0\n12.0\n31.0\n2002.0\n15.0\nPB\nF\n27.0\n34.0\n\n\n35543\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n35544.0\n12.0\n31.0\n2002.0\n15.0\nUS\nNaN\nNaN\nNaN\n\n\n35544\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n35545.0\n12.0\n31.0\n2002.0\n15.0\nAH\nNaN\nNaN\nNaN\n\n\n35545\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n35546.0\n12.0\n31.0\n2002.0\n15.0\nAH\nNaN\nNaN\nNaN\n\n\n35546\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n35547.0\n12.0\n31.0\n2002.0\n10.0\nRM\nF\n15.0\n14.0\n\n\n35547\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n35548.0\n12.0\n31.0\n2002.0\n7.0\nDO\nM\n36.0\n51.0\n\n\n35548\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n35549.0\n12.0\n31.0\n2002.0\n5.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nLooking at the result of our horizontal concatenation, we may realise that something went wrong. The total number of rows on the resulting DataFrame is 20, instead of 10.\nThis happens because horizontal stacking will only merge rows that actually “belong together”. Rows that relate to the same observed object are merged. To determine this, it compares the indices of the rows. In our two DataFrames, the rows have different indices (1-9 and 35539-35548 respectively). It will therefore not merge any of the rows together, as it does not find any two rows that relate to the same observation.\nIf we want to force the DataFrames into the form we had in mind, we need to reset the indices of the second DataFrame so that they will match the ones of the first DataFrame.\n\nsurveys_df_sub_last10 = surveys_df_sub_last10.reset_index(drop=True)\n\n\nsurveys_df_sub_last10\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n35540\n12\n31\n2002\n15\nPB\nF\n26.0\n23.0\n\n\n1\n35541\n12\n31\n2002\n15\nPB\nF\n24.0\n31.0\n\n\n2\n35542\n12\n31\n2002\n15\nPB\nF\n26.0\n29.0\n\n\n3\n35543\n12\n31\n2002\n15\nPB\nF\n27.0\n34.0\n\n\n4\n35544\n12\n31\n2002\n15\nUS\nNaN\nNaN\nNaN\n\n\n5\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n6\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n7\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n8\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n9\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nNow that the index has been reset, we can concatenate the two DataFrames.\n\nhorizontal_stack = pd.concat([surveys_df_sub_first10, surveys_df_sub_last10], axis=1)\n\n\nprint(horizontal_stack.info())\nhorizontal_stack\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10 entries, 0 to 9\nData columns (total 18 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   record_id        10 non-null     int64  \n 1   month            10 non-null     int64  \n 2   day              10 non-null     int64  \n 3   year             10 non-null     int64  \n 4   plot_id          10 non-null     int64  \n 5   species_id       10 non-null     object \n 6   sex              10 non-null     object \n 7   hindfoot_length  9 non-null      float64\n 8   weight           0 non-null      float64\n 9   record_id        10 non-null     int64  \n 10  month            10 non-null     int64  \n 11  day              10 non-null     int64  \n 12  year             10 non-null     int64  \n 13  plot_id          10 non-null     int64  \n 14  species_id       9 non-null      object \n 15  sex              6 non-null      object \n 16  hindfoot_length  6 non-null      float64\n 17  weight           6 non-null      float64\ndtypes: float64(4), int64(10), object(4)\nmemory usage: 1.5+ KB\nNone\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n35540\n12\n31\n2002\n15\nPB\nF\n26.0\n23.0\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n35541\n12\n31\n2002\n15\nPB\nF\n24.0\n31.0\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n35542\n12\n31\n2002\n15\nPB\nF\n26.0\n29.0\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n35543\n12\n31\n2002\n15\nPB\nF\n27.0\n34.0\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n35544\n12\n31\n2002\n15\nUS\nNaN\nNaN\nNaN\n\n\n5\n6\n7\n16\n1977\n1\nPF\nM\n14.0\nNaN\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n6\n7\n7\n16\n1977\n2\nPE\nF\nNaN\nNaN\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n7\n8\n7\n16\n1977\n1\nDM\nM\n37.0\nNaN\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n8\n9\n7\n16\n1977\n1\nDM\nF\n34.0\nNaN\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n9\n10\n7\n16\n1977\n6\nPF\nF\n20.0\nNaN\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nExercise 12\nNow go to the Jupyter Dashboard in your internet browser and continue with exercise 12."
  },
  {
    "objectID": "data-science-with-pandas-3.html#joining-dataframes",
    "href": "data-science-with-pandas-3.html#joining-dataframes",
    "title": "8  Combining DataFrames",
    "section": "8.3 Joining DataFrames",
    "text": "8.3 Joining DataFrames\nConcatenating DataFrames allows you to combine two entire DataFrames into a single one. In many cases, you want to combine only selected parts of two DataFrames.\nYou might, for example, want to merge rows of two DataFrames that have matching values in specific columns. The pandas function merge() performs an operation that you may know as a join if you worked with databases before. The join operation joins the content of two DataFrames in a particular way. There are different types of joins, but the workflow to perform a join operation is always the same:\n\nYou identify a left and a right DataFrame, among the two you want to join;\nYou identify in both your left and right DataFrame a column (or set of columns) to join on;\nYou choose the type of join;\nYou perform the join running the function pd.merge() with the specified inputs and options.\n\nWhat it means for a DataFrame to be ‘left’ or ‘right’ depends on the type of join, and will become clear in the examples below. For now, just remember that it matters which DataFrame you mention first when performing a join.\nLet’s see some join example considering two tiny (few rows) DataFrames. Our left DataFrame contains general data of European capitals, and our right DataFrame contains weather measurements for some Dutch towns. We first need to import these datasets:\n\nleft_df = pd.read_csv(\"../course_materials/data/EU_capitals_tiny.csv\", sep=\",\", header=0)\nright_df = pd.read_csv(\"../course_materials/data/Netherlands_town_weather_tiny.csv\", sep=\",\", header=0)\n\n\nleft_df\n\n\n\n\n\n\n\n\nCapital\nCountry\nPopulation\nTime_zone\nElevation\n\n\n\n\n0\nAmsterdam\nThe Netherland\n2480394\nUTC+1\n-2\n\n\n1\nRome\nItaly\n1459402\nUTC+1\n21\n\n\n2\nParis\nFrance\n10858852\nUTC+1\n131\n\n\n3\nMadrid\nSpain\n6791667\nUTC+1\n650\n\n\n4\nBerlin\nGermany\n4473101\nUTC+1\n34\n\n\n5\nLisbon\nPortugal\n2719000\nUTC+1\n2\n\n\n\n\n\n\n\n\nright_df\n\n\n\n\n\n\n\n\nTown\nElevation\nTemperature\nHumidity\nWind dir\nWind strengh\n\n\n\n\n0\nAmsterdam\n2\n12\n81\nSW\n21\n\n\n1\nArnhem\n13\n12\n82\nS\n6\n\n\n2\nUtrecht\n5\n13\n78\nS\n18\n\n\n3\nRotterdam\n0\n13\n84\nSW\n13\n\n\n4\nLeiden\n0\n12\n89\nSW\n19\n\n\n5\nDen Haag\n1\n12\n85\nSW\n19\n\n\n6\nRotterdam\n0\n13\n84\nSW\n16\n\n\n\n\n\n\n\nThe column we want to perform the join on is the one containing information about the town. In the left DataFrame this has name Capital while in the right one Town.\n\ninner_join = pd.merge(left_df,right_df,left_on='Capital',right_on='Town',how='inner')\ninner_join\n\n\n\n\n\n\n\n\nCapital\nCountry\nPopulation\nTime_zone\nElevation_x\nTown\nElevation_y\nTemperature\nHumidity\nWind dir\nWind strengh\n\n\n\n\n0\nAmsterdam\nThe Netherland\n2480394\nUTC+1\n-2\nAmsterdam\n2\n12\n81\nSW\n21\n\n\n\n\n\n\n\nAs you may notice, the resulting DataFrame has only one line, the only row that the columns Capital and Town have in common (Amsterdam). This is because an inner join selects only those row values that are the same in the two columns (mathematically, an intersection).\nThe columns of the two DataFrames are all preserved, even if they have the same name. In our case, both left and right DataFrames have a column with the same name (Elevation). After merging, the two columns are preserved, but with a suffix to distinguish them. If you are not happy with the default suffix, you may specify yours in the list of arguments of the pd.merge functions.\nLet’s now look at the other joins:\n\nleft_join = pd.merge(left_df,right_df,left_on='Capital',right_on='Town',how='left')\nleft_join\n\n\n\n\n\n\n\n\nCapital\nCountry\nPopulation\nTime_zone\nElevation_x\nTown\nElevation_y\nTemperature\nHumidity\nWind dir\nWind strengh\n\n\n\n\n0\nAmsterdam\nThe Netherland\n2480394\nUTC+1\n-2\nAmsterdam\n2.0\n12.0\n81.0\nSW\n21.0\n\n\n1\nRome\nItaly\n1459402\nUTC+1\n21\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nParis\nFrance\n10858852\nUTC+1\n131\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nMadrid\nSpain\n6791667\nUTC+1\n650\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nBerlin\nGermany\n4473101\nUTC+1\n34\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\nLisbon\nPortugal\n2719000\nUTC+1\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nright_join = pd.merge(left_df,right_df,left_on='Capital',right_on='Town',how='right')\nright_join\n\n\n\n\n\n\n\n\nCapital\nCountry\nPopulation\nTime_zone\nElevation_x\nTown\nElevation_y\nTemperature\nHumidity\nWind dir\nWind strengh\n\n\n\n\n0\nAmsterdam\nThe Netherland\n2480394.0\nUTC+1\n-2.0\nAmsterdam\n2\n12\n81\nSW\n21\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nArnhem\n13\n12\n82\nS\n6\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nUtrecht\n5\n13\n78\nS\n18\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nRotterdam\n0\n13\n84\nSW\n13\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nLeiden\n0\n12\n89\nSW\n19\n\n\n5\nNaN\nNaN\nNaN\nNaN\nNaN\nDen Haag\n1\n12\n85\nSW\n19\n\n\n6\nNaN\nNaN\nNaN\nNaN\nNaN\nRotterdam\n0\n13\n84\nSW\n16\n\n\n\n\n\n\n\n\nouter_join = pd.merge(left_df,right_df,left_on='Capital',right_on='Town',how='outer')\nouter_join\n\n\n\n\n\n\n\n\nCapital\nCountry\nPopulation\nTime_zone\nElevation_x\nTown\nElevation_y\nTemperature\nHumidity\nWind dir\nWind strengh\n\n\n\n\n0\nAmsterdam\nThe Netherland\n2480394.0\nUTC+1\n-2.0\nAmsterdam\n2.0\n12.0\n81.0\nSW\n21.0\n\n\n1\nRome\nItaly\n1459402.0\nUTC+1\n21.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nParis\nFrance\n10858852.0\nUTC+1\n131.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nMadrid\nSpain\n6791667.0\nUTC+1\n650.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nBerlin\nGermany\n4473101.0\nUTC+1\n34.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\nLisbon\nPortugal\n2719000.0\nUTC+1\n2.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6\nNaN\nNaN\nNaN\nNaN\nNaN\nArnhem\n13.0\n12.0\n82.0\nS\n6.0\n\n\n7\nNaN\nNaN\nNaN\nNaN\nNaN\nUtrecht\n5.0\n13.0\n78.0\nS\n18.0\n\n\n8\nNaN\nNaN\nNaN\nNaN\nNaN\nRotterdam\n0.0\n13.0\n84.0\nSW\n13.0\n\n\n9\nNaN\nNaN\nNaN\nNaN\nNaN\nRotterdam\n0.0\n13.0\n84.0\nSW\n16.0\n\n\n10\nNaN\nNaN\nNaN\nNaN\nNaN\nLeiden\n0.0\n12.0\n89.0\nSW\n19.0\n\n\n11\nNaN\nNaN\nNaN\nNaN\nNaN\nDen Haag\n1.0\n12.0\n85.0\nSW\n19.0\n\n\n\n\n\n\n\nTo resume: a join always merges rows that have matching values in the columns that you merge on. Which rows you find in the resulting DataFrame, depends on the type of join: - An inner join selects only the rows that result from the combination of matching rows in both the original left and right DataFrames (intersection); - A left join selects all rows that were in the original left DataFrame, some of which may have been joined with a matching entry from the right DataFrame; - A right join selects all rows that were in the original right DataFrame, some of which may have been joined with a matching entry from the left DataFrame; - An outer join merges the two DataFrames and keeps all resulting rows.\nTo better understand how join works, it may be useful to look at the diagrams below:\n\n\n\n\nDo you want to select only common information between the two DataFrames? Then you use an inner join;\nDo you want to add information to your left DataFrame? Then you use a left join;\nDo you want to add information to your right DataFrame? Then you use a right join;\nDo you want to get all the information from the two DataFrames? Then you use an outer join.\n\nWe will continue with Data visualization."
  },
  {
    "objectID": "data-science-with-pandas-4.html#data-visualization-with-python",
    "href": "data-science-with-pandas-4.html#data-visualization-with-python",
    "title": "9  Data Visualization",
    "section": "9.1 Data Visualization with python",
    "text": "9.1 Data Visualization with python\nMatplotlib is one of the most popular and widely-used data visualization libraries for Python. Matplotlib was inspired by the plotting functionalities of MATLAB (a non-open source programming language). It provides a comprehensive set of tools for creating a variety of plot types, such as line plots, scatter plots, bar plots, histograms, heatmaps, and many more.\nIn this session we will generate and plot some artificial data and data contained in our pandas DataFrames using pyplot. We will go through the main matplotlib concepts and we will generate several kinds of plots to illustrate matplotlib and pyplot potential."
  },
  {
    "objectID": "data-science-with-pandas-4.html#preliminaries",
    "href": "data-science-with-pandas-4.html#preliminaries",
    "title": "9  Data Visualization",
    "section": "9.2 Preliminaries",
    "text": "9.2 Preliminaries\nWe begin importing the pandas package in the same way we did in previous sessions:\n\nimport pandas as pd\n\nThe first thing to do to start visualizing data with python is importing the module pyplot from the matplotlib library. As with many of our previous imports, we import the module under an ‘alias’ (alternate shorter name) for convenience. Finally, we specify the command %matplotlib inline so that, when plotting, Jupyter Notebook will not display the plots into new windows, but in the notebook itself.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "objectID": "data-science-with-pandas-4.html#histograms",
    "href": "data-science-with-pandas-4.html#histograms",
    "title": "9  Data Visualization",
    "section": "9.3 Histograms",
    "text": "9.3 Histograms\nIn the first example we will generate some artificial data by generating 10000 normally distributed values. In order to obtain that, we will first import the package numpy, a package used for scientific computing and data analysis. In particular, the sub-package numpy.random contains very handy tools to work with random numbers and its function .normal() generates normally distributed random numbers.\n\nimport numpy as np\nsample_data = np.random.normal(0, 0.1, 10000)\n\nIn this case we are drawing 10000 points with 0 average and a standard deviation of 0.1. First we will use the pyplot function hist() to visualize a histogram of our data:\n\nplt.hist(sample_data)\n\n(array([  17.,   93.,  557., 1565., 2716., 2735., 1599.,  571.,  128.,\n          19.]),\n array([-0.37508064, -0.30023386, -0.22538708, -0.1505403 , -0.07569352,\n        -0.00084674,  0.07400004,  0.14884682,  0.2236936 ,  0.29854038,\n         0.37338716]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\nAs we expected, the histogram is centered around 0 and we can already see the bell shape arising among the blocks. The default values of histogram bins (blocks) is 10, so in our case 10000 points are subdivided into 10 bins, but we can change that by specifying the parameter bins:\n\nplt.hist(sample_data, bins=30)\n\n(array([  4.,   3.,  10.,  17.,  22.,  54.,  94., 187., 276., 367., 507.,\n        691., 829., 916., 971., 943., 941., 851., 683., 526., 390., 294.,\n        170., 107.,  72.,  35.,  21.,  12.,   5.,   2.]),\n array([-0.37508064, -0.35013171, -0.32518279, -0.30023386, -0.27528493,\n        -0.25033601, -0.22538708, -0.20043815, -0.17548922, -0.1505403 ,\n        -0.12559137, -0.10064244, -0.07569352, -0.05074459, -0.02579566,\n        -0.00084674,  0.02410219,  0.04905112,  0.07400004,  0.09894897,\n         0.1238979 ,  0.14884682,  0.17379575,  0.19874468,  0.2236936 ,\n         0.24864253,  0.27359146,  0.29854038,  0.32348931,  0.34843824,\n         0.37338716]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\n\n\n\nYou may have noticed that increasing the number of bins, the bell shape of the histogram is even more evident."
  },
  {
    "objectID": "data-science-with-pandas-4.html#customizing-plots",
    "href": "data-science-with-pandas-4.html#customizing-plots",
    "title": "9  Data Visualization",
    "section": "9.4 Customizing plots",
    "text": "9.4 Customizing plots\nIn the previous examples we generated very simple plots to have a quick look at the data, either from an existing pandas DataFrame or working with artificial data. However, with Matplotlib you can customize many more aspects of your plot: axes, x and y ticks and labels, titles, legends, and much more.\nTo get full control of the plots generated with Matplotlib.pyplot, it is important to be aware of jargon used to describe different parts of the figures that you create.\n\nAt the higher level we have Figures. A Figure is simply the total white space where you will organise your plots. You may think of it as the white page were you are going to draw your plots or also as a box containing all your plots. You can both have a single plot per Figure or multiple plots sharing the same Figure\nAt a lower level we have Axes. Axes are contained into Figures. Axes is the name for a single plot or graphs (which may be a bit confusing since it can be mistaken for the x axis and y axis of a plot). You can have a single Axes per Figure, so one plot per Figure (see Plot1 on the left of the figure below) or multiple Axes per Figure, like in Plot2 (on the right) where the same Figure contains three plots distributed in two rows: two on top and one on the bottom\nFinally, each Axes (aka each plot) contains two Axis, i.e. x and y axis.\n\nTo summarize, matplotlib organizes plots into Figures, Axes, and Axis. A Figure is a canvas that can contain one or more Axes. An Axes is where data is plotted and it is made of two Axis, x and y. Specifying parameters at these three different levels, you can customize your plots to the finest details.\n\n\n\nPlot Hierarchy\n\n\nCertain attributes like the Figure size and the number of plots inside the Figure belong to the Figure level. Ticks, labels, plot title, legend, etc belong to the Axes level. Data is plotted on Axes according to the specified x and y Axis. The main features of a “typical” plot generated with matplotlib are well summarized by the picture below from matplotlib documentation:\n\n&lt;img src=\"images/anatomy.jpeg\" alt=\"Plot Main Features\" width=\"70%\" /&gt;"
  },
  {
    "objectID": "data-science-with-pandas-4.html#customizing-titles-and-labels",
    "href": "data-science-with-pandas-4.html#customizing-titles-and-labels",
    "title": "9  Data Visualization",
    "section": "9.5 Customizing titles and labels",
    "text": "9.5 Customizing titles and labels\nTo add labels to the axis of a plot, we first create a figure containing 1 Axes (or one plot named ax). Then we use ax.set_xlabel('&lt;label&gt;') and ax.set_ylabel('&lt;label&gt;') to specify the axis labels. We can add a title to the figure using fig.suptitle('&lt;title&gt;')\n\nfig, ax = plt.subplots() # prepare a matplotlib figure\nax.hist(sample_data, bins=30)\n\n# add labels\n\nax.set_ylabel('density')\nax.set_xlabel('value')\nfig.suptitle('Histogram', fontsize=15)\n\nText(0.5, 0.98, 'Histogram')\n\n\n\n\n\nNote: To plot data on our Axes we used the same plotting methods used in the previous examples. We used hist() sampling the data in 30 bins, but this time we had to call the function from the Axes object, so ax.hist()."
  },
  {
    "objectID": "data-science-with-pandas-4.html#creating-subplots",
    "href": "data-science-with-pandas-4.html#creating-subplots",
    "title": "9  Data Visualization",
    "section": "9.6 Creating subplots",
    "text": "9.6 Creating subplots\nIf we want to create a figure that consists of multiple subplots we can use the plt.subplots() functions. The first two arguments indicate the number of vertical and horizontal plots we want to fit in our Figure. In this case, we will create two plots side to side, so our grid will have one row and two columns. As we want to be sure that there will be enough space for our two plots, we specify the size of the Figure to be 12 inches long and 6 inches high (inches is the default size unit, but you can specify different ones).\nIn this case our settings produced plots distributed in one row and two columns, so a total of 2 plots, therefore plt.subplots() will return 2 Axes objects in a tuple. We will store these two Axes into the variables ax1 and ax2.\n\n# prepare a matplotlib figure\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(12,6))\nax1.hist(sample_data, bins=30)\n# add labels\nax1.set_ylabel('density')\nax1.set_xlabel('value')\n\n# define and sample beta distribution\na = 5\nb = 10\nbeta_draws = np.random.beta(a, b, 1000)\n\n# plot beta distribution\nax2.hist(beta_draws, bins=30)\nax2.set_ylabel('density')\nax2.set_xlabel('value')\n\nText(0.5, 0, 'value')\n\n\n\n\n\nInstead of plotting side by side, it is also possible to add a plot inside (or actually overlaying another plot).\nIn order to do this we can initiate at a Figure and an Axes (the plot to put inside it), using plt.subplots(), without specifying any argument (so without specifying the number or rows and columns), the method will return a Figure and a single Axes object. We will assign these two python objects to the variables fig and ax.\n\nfig, ax = plt.subplots()  # initiate an empty figure and axis matplotlib object\nax.hist(sample_data, bins= 30)\n\n(array([  4.,   3.,  10.,  17.,  22.,  54.,  94., 187., 276., 367., 507.,\n        691., 829., 916., 971., 943., 941., 851., 683., 526., 390., 294.,\n        170., 107.,  72.,  35.,  21.,  12.,   5.,   2.]),\n array([-0.37508064, -0.35013171, -0.32518279, -0.30023386, -0.27528493,\n        -0.25033601, -0.22538708, -0.20043815, -0.17548922, -0.1505403 ,\n        -0.12559137, -0.10064244, -0.07569352, -0.05074459, -0.02579566,\n        -0.00084674,  0.02410219,  0.04905112,  0.07400004,  0.09894897,\n         0.1238979 ,  0.14884682,  0.17379575,  0.19874468,  0.2236936 ,\n         0.24864253,  0.27359146,  0.29854038,  0.32348931,  0.34843824,\n         0.37338716]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\n\n\n\nOnce we defined a Figure and an Axes, we can add other Axes to our Figure using fig.add_axes([left,bottom,length,height]) where the argument is a list containing the coordinates of our new Axes in the following format: [left edge, bottom edge, length, and height]. The left edge and bottom edgeare scaled from 0 to 1, so that 0.5 corresponds to the center of the Figure. For example, the list of coordinates [0.5,0.5,0.33,0.33] will locate the bottom-left corner of our additional Axis at the very center of the Figure. The new plot will be as wide as ~1/3 of the length of the Figure and as high as ~1/3 of the height of the Figure.\n\n# prepare a matplotlib figure\nfig, ax1 = plt.subplots()\nax1.hist(sample_data, 30)\n# add labels\nax1.set_ylabel('density')\nax1.set_xlabel('value')\n\n# define and sample beta distribution\na = 5\nb = 10\nbeta_draws = np.random.beta(a, b, 1000)\n\n# plot beta distribution\n# by adding additional axes to the figure\nax2 = fig.add_axes([0.65, 0.65, 0.2, 0.2])\n#ax2 = fig.add_axes([left, bottom, right, top])\nax2.hist(beta_draws, bins=30)\n\n(array([ 4.,  5., 16., 17., 44., 41., 63., 57., 48., 72., 61., 70., 65.,\n        65., 65., 51., 59., 39., 42., 36., 22., 22., 18.,  7.,  3.,  4.,\n         1.,  0.,  2.,  1.]),\n array([0.06167655, 0.08403887, 0.10640118, 0.12876349, 0.1511258 ,\n        0.17348812, 0.19585043, 0.21821274, 0.24057505, 0.26293736,\n        0.28529968, 0.30766199, 0.3300243 , 0.35238661, 0.37474893,\n        0.39711124, 0.41947355, 0.44183586, 0.46419818, 0.48656049,\n        0.5089228 , 0.53128511, 0.55364742, 0.57600974, 0.59837205,\n        0.62073436, 0.64309667, 0.66545899, 0.6878213 , 0.71018361,\n        0.73254592]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\n\n\n\nplt.subplots() parameters allow you to specify all sort of plot features: the size of the Figure in inches or cm, the number of plots to display in the Figure arranged in rows and columns, whether the subplots need to share the same axis, etc. The Matplotlib documentation provides all the information and examples."
  },
  {
    "objectID": "data-science-with-pandas-4.html#plotting-grouped-data",
    "href": "data-science-with-pandas-4.html#plotting-grouped-data",
    "title": "9  Data Visualization",
    "section": "9.7 Plotting grouped data",
    "text": "9.7 Plotting grouped data\nNow we will go back to the surveys dataset from the previous chapters:\n\nsurveys = pd.read_csv(('../course_materials/data/surveys.csv'))\n\n\nsurveys\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n35549 rows × 9 columns\n\n\n\nNow first, let’s create a scatterplot where we plot the weight as a function of hindfoot_length:\n\nfig, ax1 = plt.subplots() # prepare a matplotlib figure\n\nsurveys.plot(\"hindfoot_length\", \"weight\", kind=\"scatter\", ax=ax1)\n\n# Provide further adaptations with matplotlib:\nax1.set_xlabel(\"Hindfoot length\")\nax1.tick_params(labelsize=16, pad=8)\nfig.suptitle('Scatter plot of weight versus hindfoot length', fontsize=15)\n\nText(0.5, 0.98, 'Scatter plot of weight versus hindfoot length')\n\n\n\n\n\n\nExercise 13\nNow go to the Jupyter Dashboard in your internet browser and continue with exercise 13.\nIf we want to see if this distribution is different between males and females we can use a for loop and the groupby method to overlay two plots on top of each other in the same Axes object.\n\nfig, ax = plt.subplots()\nlabels = []\n\nfor i, group in list(surveys.groupby('sex')):\n    ax.scatter(group['hindfoot_length'], group['weight'], alpha=0.5)\n    labels.append(group['sex'].iloc[0])\n    \nax.legend(labels)\nax.set_xlabel(\"Hindfoot length\")\nax.set_ylabel(\"Weight\")\n\nText(0, 0.5, 'Weight')\n\n\n\n\n\nBy using ax.scatter 2 times inside the for loop, the two sets of points end up in the same Axes. Automatically the groups get different colors. With alpha=0.5 we can make the dots semi transparent so we can see them a bit better (however due to the large number of dots many blue dots are probably still hidden). We need to add the legend afterwards (after the for loop), but we do need the provide the labels in a list. We generated the list of labels in the for loop to make sure we don’t mix them up by accident.\nPerhaps we get a better view if we plot them in separate subplots, which is the next exercise:\n\nExercise 14\nNow go to the Jupyter Dashboard in your internet browser and continue with exercise 14.\n\n9.7.1 Saving your plot\nOnce you produced your plot you will probably need to share it in different media (website, papers, slide show, etc). To do that, we need to save our plot in a specific format. Once you have defined a Figure, you can do that with a single line of code:\n\nfig.savefig('MyFigure.png', dpi=200)\n\nThe Figure method savefig() will save your figure into a file. The first argument of the function is the name of the output file. Matplotlib will automatically recognize the format of the output file from its name. If you will specify only a name without extention, you can indicate the format explicitly with the parameter format. We also need to specify the dpi (dots per inch), i.e. the quality of the picture. The quality of the picture depends on the media we want to use. 200 dpi is good enough for visualization on a screen.\n\n\n9.7.2 What’s next?\nAs we mentioned in the introduction, matplotlib library is huge and you can customize every single little feature of your plot. With matplotlib you can also create animations and 3D plots. Now that you know the basics of plotting data, have a look at the matplotlib gallery to check the huge variety of plots you can generate with matplotlib.\nThis was the last chapter of this course! Go to What is next after this course, for tips on how to get started with Python in your own project!"
  },
  {
    "objectID": "what-next.html",
    "href": "what-next.html",
    "title": "What Next?",
    "section": "",
    "text": "See this website as reference: https://datacarpentry.org/python-ecology-lesson/00-before-we-start/index.html\nYou can continue on your programming journey using:\n\nhelp function\ndocumentation\nStackOverflow\nChatGPT haha\nWalk-In Hours\nProgramming Cafe"
  }
]