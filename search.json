[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python & Data",
    "section": "",
    "text": "Welcome!\nWelcome to the course book of the one-day introduction to Python course that is developed by Research Data Management support at Utrecht University. This course is taught every 2 months (agenda) and aims to introduce researchers into programming and data analysis with Python.\nIn this workshop, we will utilize the Jupyter Notebook interface and take you from the basics of Python syntax to using the pandas package to work with data frames. In doing so, we aim to give you the tools and confidence to start exploring Python and all it has to offer. After the course, you will be able to start using Python in your own project, this will be hard in the beginning (a one day course is unfortunately not enough to become a skilled programmer) but we are available during our weekly walk in hours to get you on track!\nOur workshop material is licensed under a Creative Commons Attribution 4.0 International License. You can view the license on our GitHub repository.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Contributors\nThe following indivduals have contributed to the development of this workshop:",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "acknowledgements.html#contributors",
    "href": "acknowledgements.html#contributors",
    "title": "Acknowledgements",
    "section": "",
    "text": "Jelle Treep\nChristine Staiger\nRoel Brouwer\nNeha Moopen\nStefano Rapisarda",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Time\nActivity\n\n\n\n\n9:00\nWalk-in, tech support\n\n\n9:30\nIntroductions\n\n\n10:00\nPython Fundamentals Chapters 1-3: Exercises 0-3\n\n\n10:55\nRecap & Questions\n\n\n11:00\nCoffee break\n\n\n11:15\nPython Fundamentals Chapters 4-5: Exercises 4-9\n\n\n12:40\nRecap & Questions\n\n\n12:45\nLunch break\n\n\n13:30\nData Science with Pandas Chapters 6-7: Exercises 0-4\n\n\n14:55\nRecap & Questions\n\n\n15:00\nCoffee break\n\n\n15:15\nData Science with Pandas Chapters 8-9: Exercises 5-11\n\n\n16:55\nFinal recap and closing",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "installation-and-setup.html",
    "href": "installation-and-setup.html",
    "title": "Installation & Setup",
    "section": "",
    "text": "Overview\nThe course materials are designed to be run on a personal computer. All of the software and data used are freely available online, and instructions on how to obtain them are provided below.\nIf you have any questions, or are not comfortable with doing the installation yourself join the RDM walk-in hours to ask for help.",
    "crumbs": [
      "Preparation",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "installation-and-setup.html#step-1-install-python",
    "href": "installation-and-setup.html#step-1-install-python",
    "title": "Installation & Setup",
    "section": "Step 1: Install Python",
    "text": "Step 1: Install Python\nIn this course, we will be using Python 3 with some of its most popular scientific libraries. Although one can install a plain-vanilla Python and all required libraries by hand, we recommend installing Anaconda, a Python distribution that comes with everything we need for the lesson. Detailed installation instructions can be found in the Anaconda documentation, or by following the instructions below.\nRegardless of how you choose to install it, please make sure you install Python version 3.x (e.g., 3.9 is fine).\nWe will teach Python using the Jupyter Notebook, a programming environment that runs in a web browser (Jupyter Notebook will be installed by Anaconda). For this to work you will need a reasonably up-to-date browser. The current versions of the Chrome, Safari and Firefox browsers are all supported (some older browsers, including Internet Explorer version 9 and below, are not).\n\nWindowsMacOSLinux\n\n\n\nDownload the Anaconda distribution from the Anaconda website. The website should automatically determine which installation is most suitable for your machine. Save the .exe file in a location that you can access.\nDouble click the Anaconda .executable file to start the installation set-up.\nFollow the steps in the set-up:\n\nWelcome to Anaconda: click next \nAccept the License Agreement: \nSelect the installation type: Just Me or All Users. The All Users option requires Admin access to your computer. If you do not have such rights, choose Just Me. Otherwise, we recommend choosing All Users. If you install Anaconda for all users, click Yes to the question “Do you want to allow this app to make changes to your device?” \nChoose the installation location: the proposed location differs depending on your choice in the previous step. Usually the default proposed location is fine. Click Next. \nAdvanced Installation Options: choose which advanced options you want. We recommend to at least check the “Register Anaconda as the system Python” option, because that allows other programs to detect the Python installation on your machine. \nWait for installation. This can take up a couple of minutes, depending on whether you are installing just for you or for all users. Click Next once the installation has finished \nClick Next in the following screen that promotes Anaconda cloud notebooks. \nFinally, indicate whether you want Anaconda to launch and whether you want to see the Getting Started with Anaconda Distribution, and click Finish. \n\n\n\n\n\nOpen https://www.anaconda.com/products/individual#download-section with your web browser.\nDownload the Anaconda Installer with Python 3 for macOS (you can either use the Graphical or the Command Line Installer).\nInstall Python 3 by running the Anaconda Installer using all of the defaults for installation.\n\n\n\n\nOpen https://www.anaconda.com/products/individual#download-section with your web browser.\nDownload the Anaconda Installer with Python 3 for Linux.  (The installation requires using the shell. If you aren’t comfortable doing the installation yourself stop here and come to the walk-in hours to ask for help.)\nOpen a terminal window and navigate to the directory where the executable is downloaded (e.g., cd ~/Downloads).\nType\nbash Anaconda3-\nand then press Tab to autocomplete the full file name. The name of file you just downloaded should appear.\nPress Enter (or Return depending on your keyboard). You will follow the text-only prompts. To move through the text, press Spacebar. Type yes and press enter to approve the license. Press Enter (or Return) to approve the default location for the files. Type yes and press Enter (or Return) to prepend Anaconda to your PATH (this makes the Anaconda distribution the default Python).\nClose the terminal window.",
    "crumbs": [
      "Preparation",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "installation-and-setup.html#step-2-obtain-lesson-materials",
    "href": "installation-and-setup.html#step-2-obtain-lesson-materials",
    "title": "Installation & Setup",
    "section": "Step 2: Obtain lesson materials",
    "text": "Step 2: Obtain lesson materials\n\nDownload the zip folder with all course materials (see Course Materials).\nCreate a folder called python-workshop on your Desktop (or any other place where you want to store the materials).\nMove the downloaded zip to the folder where you want to store these course materials.\nUnzip the zip file.\n\nIn your python-workshop you will see a folders called data and the following files:\npython-workshop\n├── data\n│   ├── species.csv\n│   ├── surveys.csv\n│   └── plots.csv\n├── morning_exercises.ipynb \n├── afternoon_exercises.ipynb \n└── preparation.ipynb",
    "crumbs": [
      "Preparation",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "installation-and-setup.html#step-3-start-jupyter-interface-and-complete-preparation.ipynb",
    "href": "installation-and-setup.html#step-3-start-jupyter-interface-and-complete-preparation.ipynb",
    "title": "Installation & Setup",
    "section": "Step 3: Start Jupyter interface and complete preparation.ipynb",
    "text": "Step 3: Start Jupyter interface and complete preparation.ipynb\nTo start working with Python, we need to launch a program that will interpret and execute our Python commands. Below we list several options. If you don’t have a preference, proceed with the top option in the list that is available on your machine. Otherwise, you may use any interface you like.\n\nOption A: Jupyter Notebook\nA Jupyter Notebook provides a browser-based interface for working with Python. If you installed Anaconda, you can launch a notebook in two ways:\n\nAnaconda NavigatorCommand line (Terminal)\n\n\n\nLaunch Anaconda Navigator. It might ask you if you’d like to send anonymized usage information to Anaconda developers. Make your choice and click “Ok, and don’t show again” button.\nFind the “Notebook” tab and click on the “Launch” button. Anaconda will open a new browser window or tab with a Notebook Dashboard showing you the contents of your Home (or User) folder.\nNavigate to the data directory by clicking on the directory names leading to it. Desktop, python-workshop, then data:\nLaunch the notebook called preparation.ipynb by clicking on it.\nFollow the instructions in the notebook to finalize your preparation for the workshop. If the output of the last cell displays 4 version numbers and the words “No errors! Ready to code!” instead of an error message, your installation is successful. If not, contact us at RDM walk in hours or reply to the welcome email.\n\n\n\n\nNavigate to the python-workshop directory: Unix shell If you’re using a Unix shell application, such as Terminal app in macOS, Console or Terminal in Linux, or Git Bash on Windows, execute the following command:\ncd ~/Desktop/python-workshop\nCommand Prompt (Windows) On Windows, you can use its native Command Prompt program. The easiest way to start it up is pressing Windows Logo Key+R, entering cmd, and hitting Return. In the Command Prompt, use the following command to navigate to the data folder:\ncd /D %userprofile%\\Desktop\\python-workshop\nStart Jupyter server: Unix shell\njupyter notebook\nCommand Prompt (Windows)\npython -m notebook\nLaunch the notebook called preparation.ipynb via the ‘File’ menu.\nFollow the instructions in the notebook to finalize your preparation for the workshop. If the output of the last cell displays 4 version numbers and the words “No errors! Ready to code!” instead of an error message, your installation is successful. If not, contact us at RDM walk in hours or reply to the welcome email.\n\n\n\n\n\n\nOption B: IPython interpreter\nIPython is an alternative solution situated somewhere in between the plain-vanilla Python interpreter and Jupyter Notebook. It provides an interactive command-line based interpreter with various convenience features and commands. You should have IPython on your system if you installed Anaconda.\nTo start using IPython, execute:\nipython\n\n\nOption C: plain-vanilla Python interpreter\nTo launch a plain-vanilla Python interpreter, execute:\npython\nIf you are using Git Bash on Windows, you have to call Python via winpty:\nwinpty python",
    "crumbs": [
      "Preparation",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "installation-and-setup.html#python-packages",
    "href": "installation-and-setup.html#python-packages",
    "title": "Installation & Setup",
    "section": "Python packages",
    "text": "Python packages\nThis workshop will make use of the following Python packages:\n\npandas\nmatplotlib\nnumpy\n\nAnaconda Navigator comes with these packages, so you are ready to go. If you are using another option to work with Python, you need to install these packages. We then assume that you know how to install packages, otherwise we recommend using Anaconda instead. If for some reason you cannot work with Anaconda and you need help with installing packages, we are happy to help during the RDM walk in hours, or email us by replying to the welcome email.",
    "crumbs": [
      "Preparation",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "installation-and-setup.html#references",
    "href": "installation-and-setup.html#references",
    "title": "Installation & Setup",
    "section": "References",
    "text": "References\nThe instructions on this page were adapted from the setup instructions of the Software Carpentries “Programming with Python” course and their Workshop Template Python installation instructions, both released under the Creative Commons Attribution license. Changes to the material were made, and can be tracked in the Git repository associated with this course.",
    "crumbs": [
      "Preparation",
      "Installation & Setup"
    ]
  },
  {
    "objectID": "course-materials.html",
    "href": "course-materials.html",
    "title": "Workshop Materials",
    "section": "",
    "text": "Zipped File\nThe following zipped file contains the notebooks and data required for the workshop.\nDon’t forget to extract the contents of the zipped file after downloading!",
    "crumbs": [
      "Preparation",
      "Workshop Materials"
    ]
  },
  {
    "objectID": "course-materials.html#zipped-file",
    "href": "course-materials.html#zipped-file",
    "title": "Workshop Materials",
    "section": "",
    "text": "course_materials.zip\n\n\nintroduction-python\n├── data\n│   ├── species.csv\n│   ├── surveys.csv\n│   └── plots.csv\n├── morning_exercises.ipynb \n└── afternoon_exercises.ipynb",
    "crumbs": [
      "Preparation",
      "Workshop Materials"
    ]
  },
  {
    "objectID": "course-materials.html#notebooks",
    "href": "course-materials.html#notebooks",
    "title": "Workshop Materials",
    "section": "Notebooks",
    "text": "Notebooks\nWe will be using the following Jupyter Notebooks to work on the exercises:\n\nmorning_exercises.ipynb\nafternoon_exercises.ipynb",
    "crumbs": [
      "Preparation",
      "Workshop Materials"
    ]
  },
  {
    "objectID": "course-materials.html#data",
    "href": "course-materials.html#data",
    "title": "Workshop Materials",
    "section": "Data",
    "text": "Data\nThe data for this workshop is from the Portal Teaching Database. We will be using the following datasets:\n\nsurveys.csv\nspecies.csv\nplots.csv",
    "crumbs": [
      "Preparation",
      "Workshop Materials"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Python?\nPython is a general-purpose programming language that is very popular in the scientific community. At Utrecht University it is the most popular programming language for open source research projects @ UU (see SWORDS-UU).\nPython is popular due to its readability, ease of use, its large community, and its large ecosystem of packages that can be used for a large variety of tasks, ranging from data analysis, modelling, simulation, machine learning to web development.\nPython is open source, which means that it is free to use and that the source code is available for anyone to inspect and modify. This makes it easy to install Python on any computer, and e.g. share code with others or scale up your computations to a high performance computing system.",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#what-is-python",
    "href": "introduction.html#what-is-python",
    "title": "1  Introduction",
    "section": "",
    "text": "Programming languages by user",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#scope-of-this-course",
    "href": "introduction.html#scope-of-this-course",
    "title": "1  Introduction",
    "section": "1.2 Scope of this course",
    "text": "1.2 Scope of this course\nProgramming is a skill that takes time and practice to learn. This course will not make you an expert programmer, but it will give you a small foundation to build on. The course will focus on the basics of the Python language, and simple data handling and visualization with the Pandas Python library. This will be enough to demonstrate the power of Python and hopefully inspire you to learn more. After the course you will have some basic knowledge to get started and we will be available during the RDM walk in hours and the Programming cafe to help you continue your learning journey and start using Python in your own research.",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#jupyter-notebooks",
    "href": "introduction.html#jupyter-notebooks",
    "title": "1  Introduction",
    "section": "1.3 Jupyter Notebooks",
    "text": "1.3 Jupyter Notebooks\nThere are many programs that can be used to edit and run Python code. This course will use Jupyter Notebooks. Jupyter Notebooks are a way to combine text, code, and output in a single document. Jupyter Notebooks are a great way to learn Python, as you can run code in small chunks and immediately see the results. But this doesn’t mean Jupyter Notebooks are only for beginners. Jupyter Notebooks are used by many professional programmers and scientists to share their work and even do computationally expensive analyses.\nSee below how a Jupyter Notebook looks like. The notebook is divided in text cells and code cells. The code cell contains Python code, which can be executed by pressing the ‘play’ button in the toolbar or by pressing Shift + Enter. The output of the code is shown below the code cell. The text cells contain text written in Markdown, which is a simple way to format text. You can edit the text by double clicking on the text cell. To render the text again, press Shift + Enter.\n Example of a Jupyter Notebook",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#course-setup",
    "href": "introduction.html#course-setup",
    "title": "1  Introduction",
    "section": "1.4 Course setup",
    "text": "1.4 Course setup\nDuring the course we will be introducing concepts in short lectures where we will demonstrate by means of ‘live coding’. This means that we will be writing code in a Jupyter Notebook and explain what we are doing. You can follow along by opening an empty notebook, typing along and running the code yourself. In between the lectures we will provide you with exercises to practice what you have learned.",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_1.html",
    "href": "Introduction_to_python_1.html",
    "title": "2  Variables and printing output",
    "section": "",
    "text": "2.1 Variables, values and their types\nThe cell below contains Python code that can be executed by the Python interpreter. One of the most basic things that we can do with Python is to use it as a calculator:\n2+2\n\n4\nGreat, but there are many calculators. It gets more interesting when we use variables to store information. This is done with the = operator. In Python, variable names:\nx = 3.0\nOnce assigned, variables can be used in new operations:\ny = 2.0\nx + y\n\n5.0\nPython knows various types of data. Three common ones are:\ntext = \"Data Carpentry\"\nnumber = 42\npi_value = 3.14159265358\nIn the example above, three variables are assigned. Variable number is an integer number with a value of 42 while pi_value is a floating point number and text is of type string.\nUsing the type command, it is possible to check the data type of a variable.\ntext\n\n'Data Carpentry'\ntype(text)\n\nstr\nnumber\n\n42\ntype(number)\n\nint\npi_value\n\n3.14159265358\ntype(pi_value)\n\nfloat",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Variables and printing output</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_1.html#variables-values-and-their-types",
    "href": "Introduction_to_python_1.html#variables-values-and-their-types",
    "title": "2  Variables and printing output",
    "section": "",
    "text": "can include letters, digits, and underscores\ncannot start with a digit\nare case sensitive.\n\n\n\n\n\n\ninteger numbers\nfloating point numbers\nstrings",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Variables and printing output</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_1.html#output-versus-printing",
    "href": "Introduction_to_python_1.html#output-versus-printing",
    "title": "2  Variables and printing output",
    "section": "2.2 Output versus printing",
    "text": "2.2 Output versus printing\nIn the above examples, most of the times output is printed directly below the cell, but not always the output is printed and not all operations are printed. The print command can be used to control what is printed when.\nNote, that text (strings) always has to be surrounded by \" or '.\n\nprint(\"Hello World\")\n\nHello World\n\n\nIn the example below we first print the value of the variable number using the print command, and then call the variable:\n\nprint(number)\nnumber\n\n42\n\n\n42\n\n\nNow we do it the other way around:\n\nnumber\nprint(number)\n\n42\n\n\nWhen not using the print command, only the output of the last operation in the input cell is printed. If the last operation is the assignment of a variable, nothing will be printed.\nIn general print is the only way to print output to the screen when you are not working in an interactive environment like Jupyter (as we are doing now).\nRule of thumb: use the normal output for quick checking the output of an operation while developing in your Jupyter notebook, use print for printing output that still needs to be there in the future while your scripts get more complicated.\n\nContinue with the following chapter Operators and Built-in Functions",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Variables and printing output</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_2.html",
    "href": "Introduction_to_python_2.html",
    "title": "3  Operators, built-in functions, if-statements",
    "section": "",
    "text": "3.1 Mathematical operations\nIn Python you can do a wide variety of mathematical operations. A few examples:\nsumming = 2 + 2\nmultiply = 2 * 7\npower = 2 ** 16\nmodulo = 13 % 5\n\nprint(\"Sum: \", summing)\nprint(\"Multiply: \", multiply)\nprint(\"Power: \", power)\nprint(\"Modulo: \", modulo)\n\nSum:  4\nMultiply:  14\nPower:  65536\nModulo:  3\nOnce we have data stored in variables, we can use the variables to do calculations.\nnumber = 42\npi_value = 3.14159265358\n\noutput = number * pi_value\nprint(output)\n\n131.94689145036",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Operators, built-in functions, if-statements</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_2.html#built-in-python-functions",
    "href": "Introduction_to_python_2.html#built-in-python-functions",
    "title": "3  Operators, built-in functions, if-statements",
    "section": "3.2 Built-in Python functions",
    "text": "3.2 Built-in Python functions\nTo carry out common tasks with data and variables in Python, the language provides us with several built-in functions. Examples of built-in functions that we already used above are print and type.\nCalling a function When we want to make use of a function (referred to as calling the function), we type the name of the function followed by parentheses. Between the parentheses we can pass arguments.\nArguments Arguments are used by a function to perform the body of the function with the value of this argument. In the example below type is the function name and pi_value is the argument.\n\ntype(pi_value)\n\nfloat\n\n\nOther useful built-in functions are abs(), max(), min(), range(). Find more built-in functions here.\n\nmax([1,2,3,2,1])\n\n3",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Operators, built-in functions, if-statements</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_2.html#if-statements-and-comparisons",
    "href": "Introduction_to_python_2.html#if-statements-and-comparisons",
    "title": "3  Operators, built-in functions, if-statements",
    "section": "3.3 If-statements and comparisons",
    "text": "3.3 If-statements and comparisons\nIn programming it is possible to perform certain tasks only when a certain condition is met: Assume we have a table where each row is a person and the table contains a column age. We want to create a new variable age_group which can have the values child and adult based on the value in the column age. We can use an if-statement to decide whether a person is younger than 18 and asign the right value to age_group.\n\nage = 17\n\nif age &lt; 18:\n    print('this person is a child')\n\nthis person is a child\n\n\nAs you can see, the line print(...) starts with 4 spaces indentation. In Python indentation is very important. Python uses indentation to determine which lines of code belong together. Lines with the same identation are called a code block. We use code blocks to define what should be done in an if-statement, for-loop or in a functions. E.g. after the if condition, all lines with indentation are only performed when the if-condition is met.\n\nnum = 99\nif num &gt; 100:\n    print('This line is only executed when num &gt; 100')\n    print('This line is only executed when num &gt; 100')\n    \n    print('This line is only executed when num &gt; 100')\n    \nprint('This line is always executed')\n\nThis line is always executed\n\n\nIt is also possible to specify a task that is performed when the condition is not met using else (note the use of indentation):\n\nage = 17\n\nif age &lt; 18:\n    print('this person is a child')\nelse:\n    print('this person is an adult')\n\nprint('done')\n\nthis person is a child\ndone\n\n\nWith an if .. else statement we can define one condition. If we need to check more conditions the if ... else statement can be extended with (one or more) elif to specify more tasks that need to be performed on other conditions. These extended if ... else statements always start with if followed by (one or more) elif. When an else statement is included it is always the last statement, it is the default which will be eceuted when all previous comparisons failed.\nOrder matters: The statements (or conditions) are checked in order from top to bottom and only the task belonging to the first condition that is met is being performed.\n\nnum = -3\n\nif num &gt; 0:\n    print(num, 'is positive')\nelif num == 0:\n    print(num, 'is zero')\nelse:\n    print(num, 'is negative')\n\n-3 is negative\n\n\nIf statements often use comparisons to check if a certain condition is met. Comparisons are done with comparison operators such as &gt;, ==.\nAlong with the &gt; and == comparison operators that we have already used for comparing values above, there are a few more options to know about:\n\n&gt;: greater than\n&lt;: less than\n==: equal to\n!=: does not equal\n&gt;=: greater than or equal to\n&lt;=: less than or equal to\n\nLet’s now play around with comparisons to see how they work in more detail.\n\n4 &gt; 3\n\nTrue\n\n\n\n4 &lt; 3\n\nFalse\n\n\n\na = 4 &gt; 3\ntype(a)\n\nbool\n\n\nAs you can see, comparisons return True or False. The data type of these values are bool which is short for boolean values.\nIf-statements work with boolean values. If the value is True, the task is performed, if the value is False, the task is not performed.\nBoolean values can also be assigned to variables:\n\na = True\nb = True\nc = False\ntype(a)\n\nbool\n\n\nand, or and not are ‘logical operators’, and are used to join two comparisons (or revert a logical expression in the case of not) to create more complex conditions.\nand will return True if both expression on either side are True.\n\na and b\n\nTrue\n\n\n\na and c\n\nFalse\n\n\n\n4 &gt; 3 and 9 &gt; 3\n\nTrue\n\n\nor is used to check if at least one of two logical expressions are True. If this is the case it will return True.\n\n3 &gt; 4 or 9 &gt; 3\n\nTrue\n\n\n\n4 &gt; 3 or 9 &gt; 3\n\nTrue\n\n\nIn the last three examples you can see that multiple expressions can be combined in a single line of Python code. Python evaluates the expressions one by one. 4 &gt; 3 would return True, 9 &gt; 3 would return True, so 4 &gt; 3 or 9 &gt; 3 would translate to True or True. Because at least one of the expressions on either side of the or operator is True, the whole expression results to True.\nThe not operator can be used to reverse the Boolean value. If you apply not to an expression that evaluates to True, then you get False as a result. If you apply not to an expression that evaluates to False, then you get True as a result:\n\nnot 4 &gt; 3\n\nFalse\n\n\nLogical operators bind variables with different strengths. The and is stronger than the or and gets evaluated first in a boolean expression. So a or b and c will be evaluated like a or (b and c), while in (a or b) and c first the value of the or is evaluated and then combined with and c. This leads to a different result.\n\na = True\nb = True\nc = False\nprint(\"This expression 'a or b and c' evalutes to \", a or b and c)\nprint(\"And this is the same as 'a or (b and c)')\", a or (b and c))\nprint(\"But this expression evaluates '(a or b) and c' first the 'or' and generates:\", (a or b) and c)\n\nThis expression 'a or b and c' evalutes to  True\nAnd this is the same as 'a or (b and c)') True\nBut this expression evaluates '(a or b) and c' first the 'or' and generates: False\n\n\nNow we know how to use comparisons and logical operators to create complex conditions. These complex conditions can be used in if-statements to perform tasks based on these conditions.\n\nif (1 &lt; 0) or (1 &gt;= 0):\n    print('at least one the above logical statements is true')\n\nat least one the above logical statements is true\n\n\nWhile and is only true if both parts are true\n\nif (1 &lt; 0) and (1 &gt;= 0):\n    print('both tests are true')\nelse:\n    print('at least one of the tests is not true')\n\nat least one of the tests is not true\n\n\n\nExercises\nNow open the morning_exercises.ipynb notebook from the place where you have stored it (see Installation and setup instruction) and do exercises 0-3.\nWhen you finished the exercises, continue to chapter Data types, if-statements and for loops",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Operators, built-in functions, if-statements</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_3.html",
    "href": "Introduction_to_python_3.html",
    "title": "4  Data types, lists and for-loops",
    "section": "",
    "text": "4.1 Lists\nUntil now we have worked with values and variables that hold one value or string. Now we will go into other data types that can combine multiple values or strings.\nLists are common data structures to hold a sequence of elements. We can create a list by putting values inside square brackets and separating the values with commas.\nnumbers = [1, 2, 3]\nprint(numbers)\n\n[1, 2, 3]\nEach element can be accessed by an index. The index of the first element in a list in Python is 0 (in some other programming languages that would be 1).\nprint(\"The first element in the list numbers is: \", numbers[0])\n\nThe first element in the list numbers is:  1\ntype(numbers)\n\nlist\nA total number of items in a list is called the ‘length’ and can be calculated using the len() function.\nlen(numbers)\n\n3\nYou can do various things with lists. E.g. it is possible to sum the items in a list (when the items are all numbers)\nprint(\"The sum of the items in the list is:\", sum(numbers))\nprint(\"The mean of the items in the list is:\", sum(numbers)/len(numbers))\n\nThe sum of the items in the list is: 6\nThe mean of the items in the list is: 2.0\nWhat happens here:\nnumbers[3]\n\nIndexError: list index out of range\nThis error is expected. The list consists of three items, and the indices of those items are 0, 1 and 2.\nnumbers[-1]\n\n3\nYes, we can use negative numbers as indices in Python. When we do so, the index -1 gives us the last element in the list, -2 the second to last, and so on. Because of this, numbers[2] and numbers[-1] point to the same element.\nnumbers[2] == numbers[-1]\n\nTrue\nIt is also possible to combine strings in a list:\nwords = [\"cat\", \"dog\", \"horse\"]\nwords[1]\n\n'dog'\ntype(words)\n\nlist\nif type(words) == type(numbers):\n    print(\"these variables have the same type!\")\n\nthese variables have the same type!\nIt is also possible to combine values of different type (e.g. strings and integers) in a list\nnewlist = [\"cat\", 1, \"horse\"]\nThe type of the variable newlist is list. The elements of the list have their own data type:\ntype(newlist[0])\n\nstr\ntype(newlist[1])\n\nint\nIt is possible to add numbers to an existing list using list.append()\nnumbers.append(4)\nprint(numbers)\n\n[1, 2, 3, 4]\nUsing the index of an item, you can replace the item in a list:\nnumbers[2] = 333\nprint(numbers)\n\n[1, 2, 333, 4]\nNow what do you do if you do not know the index but you know the value of an item that you want to find in a list. How to find out at which position the value is listed?\nindex = newlist.index(\"cat\")\nprint(\"'cat' can be found at index\", index)\nprint(newlist[index])\n\n'cat' can be found at index 0\ncat",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data types, lists and for-loops</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_3.html#tuples",
    "href": "Introduction_to_python_3.html#tuples",
    "title": "4  Data types, lists and for-loops",
    "section": "4.2 Tuples",
    "text": "4.2 Tuples\nA tuple is similar to a list in that it’s a sequence of elements. However, tuples can not be changed once created (they are “immutable”). Tuples are created by placing comma-separated values inside parentheses () (instead of square brackets []).\n\n# Tuples use parentheses\na_tuple = (1, 2, 3)\nanother_tuple = ('blue', 'green', 'red')\n\n# Note: lists use square brackets\na_list = [1, 2, 3]\n\n\na_list[1] = 5\nprint(a_list)\n\n[1, 5, 3]\n\n\n\na_tuple[1] = 5\nprint(a_tuple)\n\nTypeError: 'tuple' object does not support item assignment\n\n\nHere we see that once the tuple is created, we cannot replace any of the values inside of the tuple.\n\ntype(a_tuple)\n\ntuple",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data types, lists and for-loops</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_3.html#dictionaries",
    "href": "Introduction_to_python_3.html#dictionaries",
    "title": "4  Data types, lists and for-loops",
    "section": "4.3 Dictionaries",
    "text": "4.3 Dictionaries\nA dictionary is another way to store multiple items into one object. In dictionaries, however, this is done with keys and values. In dictionaries, keys are typically use to look up values. A good analogy may be the contact list in your phone where you use a name (key) to lookup a phone number (value). This can be useful for several reasons, one example is to store model settings, parameters or variable values for multiple scenarios.\n\nmy_dict = {'one': 1, 'two': 2}\nmy_dict\n\n{'one': 1, 'two': 2}\n\n\nWe can access dictionary items by their key:\n\nmy_dict['one']\n\n1\n\n\nAnd we can add new key-value pairs like that:\n\nmy_dict['three'] = 3\nmy_dict\n\n{'one': 1, 'two': 2, 'three': 3}\n\n\nDictionary items are key-value pairs. The keys are changeable and always have to be unique (within a dictionary object). The values within a dictionary are changable, and don’t have to be unique.\n\nmy_dict['two'] = 5\nmy_dict\n\n{'one': 1, 'two': 5, 'three': 3}\n\n\n\nprint(\"Dictionary keys: \", my_dict.keys())\nprint(\"Dictionary values: \", my_dict.values())\nprint(\"Dictionary items (key, value): \", my_dict.items())\n\nDictionary keys:  dict_keys(['one', 'two', 'three'])\nDictionary values:  dict_values([1, 5, 3])\nDictionary items (key, value):  dict_items([('one', 1), ('two', 5), ('three', 3)])",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data types, lists and for-loops</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_3.html#for-loops",
    "href": "Introduction_to_python_3.html#for-loops",
    "title": "4  Data types, lists and for-loops",
    "section": "4.4 For loops",
    "text": "4.4 For loops\nLet’s have a look at our list again. One way to print each number is to use three print statements:\n\nnumbers = [5, 6, 7]\nprint(numbers[0])\nprint(numbers[1])\nprint(numbers[2])\n\n5\n6\n7\n\n\nA more efficient (less typing) and reliable way to print each element of a list is to loop over the list using a for loop:\n\nfor item in numbers:\n    print(item)\n\n5\n6\n7\n\n\nThe improved version uses a for loop to repeat an operation — in this case, printing — once for each item in a sequence. Note that (similar to if statements) Python needs indentation (4 whitespaces) to determine which lines of code are part of the for loop.\nIf we want to also get the index, we can use the built-in function enumerate:\n\nwords = [\"cat\", \"dog\", \"horse\"]\n\nfor index, item in enumerate(words):\n    print(index)\n    print(item)\n\n0\ncat\n1\ndog\n2\nhorse\n\n\nFor loops can also be used with dictionaries. Let’s take our dictionary from the previous section and inspect the dictionary items\n\nfor item in my_dict.items():\n    print(item, \"is of type\", type(item))\n\n('one', 1) is of type &lt;class 'tuple'&gt;\n('two', 5) is of type &lt;class 'tuple'&gt;\n('three', 3) is of type &lt;class 'tuple'&gt;\n\n\nWe can extract the keys and values from the items directly in the for statement:\n\nfor key, value in my_dict.items():\n    print(key, \"-&gt;\", value)\n\none -&gt; 1\ntwo -&gt; 5\nthree -&gt; 3\n\n\n\nExercises\nNow go back to your browser to morning_exercises.ipynb and continue with exercises 4-7.\nWhen you finished the exercises, continue to chapter Write your own Python function",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data types, lists and for-loops</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_4.html",
    "href": "Introduction_to_python_4.html",
    "title": "5  Write your own Python function",
    "section": "",
    "text": "5.1 Functions\nWe have already seen some built-in functions: e.g. print, type, len. And we have seen special functions that belong to a variable (python object) like my_dict.items() and my_list.append(). There are more built-in functions e.g. for mathematical operations:\nnumbers = [5, 6, 7]\nsum(numbers)\n\n18\nThe Python Documentation at docs.python.org has more info about built-in functions.",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Write your own Python function</span>"
    ]
  },
  {
    "objectID": "Introduction_to_python_4.html#functions",
    "href": "Introduction_to_python_4.html#functions",
    "title": "5  Write your own Python function",
    "section": "",
    "text": "5.1.1 Writing own functions\nWe will now turn to writing own functions. When should you write your own function?\n\nIf the functionality is not covered by an out-of-the-box function like the built-in functions or another python package\n\nWhen code is getting pretty long, you can split it up into logical and reusable units\n\nWhen code is often reused, e.g. you are reading in tens of spreadsheets and you need to clean them all in the same way. Instead of typing the line of code over and over again, it is more elegant and looks cleaner to create a function.\n\nWhen code may be reused outside your current project. Scripts and the functions in a script can be imported in other scripts to be able to reuse them.\n\nA big advantage of not having duplicate code inside your script or in multiple scripts is that when you want to make a slight modification to a function, you only have to do this modification in one place, instead of multiple lines that are doing more or less similar things.\nPython provides for this by letting us define things called ‘functions’. Let’s start by defining a function fahr_to_celsius that converts temperatures from Fahrenheit to Celsius:\n\ndef fahr_to_celsius(temp_fahrenheit):\n    temp_celsius = (temp_fahrenheit - 32) * (5/9)\n    return temp_celsius\n\nThe function definition opens with the keyword def followed by the name of the function fahr_to_celsius and a parenthesized list of variables (in this case only one temp_fahrenheit). The body of the function — the statements that are executed when it runs — is indented below the definition line. The body concludes with a return keyword followed by the return value.\nWhen we call the function, the values we pass to it as arguments are assigned to the variables in the function definition so that we can use them inside the function. Inside the function, we use a return statement to send a result back to whoever asked for it.\nLet’s try running our function.\n\nfahr_to_celsius(98)\n\n36.66666666666667\n\n\n\nprint('freezing point of water:', fahr_to_celsius(32), 'C')\nprint('boiling point of water:', fahr_to_celsius(212), 'C')\n\nfreezing point of water: 0.0 C\nboiling point of water: 100.0 C\n\n\nHere we directly passed a value to the function. We can also call the function with a variable:\n\na = 0\nprint(fahr_to_celsius(a))\n\n-17.77777777777778\n\n\nWhat happens if you pass a variable name that is not defined yet?\n\nprint(fahr_to_celsius(b))\n\nNameError: name 'b' is not defined\n\n\n\nExercises\nNow go back to your browser to morning_exercises.ipynb and continue with exercises 8 and 9.\nWhen you finished the exercises, continue to the afternoon session",
    "crumbs": [
      "Python Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Write your own Python function</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-1.html",
    "href": "data-science-with-pandas-1.html",
    "title": "6  Reading and Exploring data",
    "section": "",
    "text": "6.1 Libraries\nThe power of Python (and many programming languages) is in the libraries.\nA library (aka package) is a collection of files (aka python scripts) that contains functions that can be used to perform specific tasks. A library may also contain data. The functions in a library are typically related and used for a specific purpose, e.g. there are libraries for plotting, handling audio data and machine learning and many many more. Some libraries are built into python, but most packages need to be installed before you can use it.\nLibraries are developed and maintained by other Python users. That is why there are so many packages and this is great: there is a huge variety of functions available that you can use instead of programming them yourself. But it is important to be aware that the quality can differ. A popular library like Pandas has a large user base and the maintainers are supported by several funders, which makes it a reliable library that is updated very frequently. But this is not always the case, on the other side of the spectrum, a library can also be published once, badly designed/documented and/or not maintained at all. To check the quality of a library, you can check e.g. the number of downloads, the number of contributors, the number of open issues, the date of the last update and the number of stars on GitHub.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reading and Exploring data</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-1.html#pandas",
    "href": "data-science-with-pandas-1.html#pandas",
    "title": "6  Reading and Exploring data",
    "section": "6.2 Pandas",
    "text": "6.2 Pandas\nThe python library Pandas is a popular open-source data analysis and data manipulation library for Python which was developed in 2008. The library has some similarities with R, mainly related to the DataFrame data type that is used to handle table like datasets.\nPandas is widely used in data analyses and machine learning, as it provides powerful tools for data handling and manipulation. Furthermore, it integrates well with other Python libraries for data analysis, machine learning, and statistical analysis, such as NumPy, Scikit-Learn, and StatsModels.\nIn this first chapter we will explore the main features of Pandas related to reading and exploring a dataset. In the following chapters we will go into finding, selecting and grouping data, merging datasets and visualization.\nFor this purpose, we will be using data from the Portal Project Teaching Database: real world example of life-history, population, and ecological data and, occasionally, a small ad-hoc dataset to exaplain DataFrame operations.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reading and Exploring data</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-1.html#preliminaries",
    "href": "data-science-with-pandas-1.html#preliminaries",
    "title": "6  Reading and Exploring data",
    "section": "6.3 Preliminaries",
    "text": "6.3 Preliminaries\nBefore we start our journey into Pandas functionalities, there are some preliminary operations to run.\nThe Pandas library is not a built-in library of python, it needs to be installed and loaded. If you are using Anaconda Navigator and/or followed the setup instructions for this course, Pandas is already installed on your system. Assuming you already installed it, let’s start importing the Pandas library and checking our installed version.\n\nimport pandas as pd\nprint(pd.__version__)\n\n1.5.3\n\n\nIt is also possible to just type import pandas, but we chose to give the library an ‘alias’: pd. The main reason is that we can now use functions from the library by typing pd instead of pandas (see the following line print(pd.__version__))\nTo be able to read the data files that we will be using, we need to specify the location of the files (also referred to as ‘path’). It is best practice to specify the path relative to the main project folder, so, in order to properly read our dataset, it’s important to check that we are working in the main project folder. In order to do that, we will load another library called ‘os’, containing all sort of tools to interact with our operating system. The function os.getcwd(), returns the current working directory (cwd).\n\nimport os\ncwd = os.getcwd()\nprint(cwd)\n\n/home/runner/work/workshop-introduction-to-python/workshop-introduction-to-python/book\n\n\nIf the current working directory ends with &lt;...&gt;/python-workshop(or any other name that you used to create a folder for storing the course materials during installation and setup), where &lt;...&gt; is whatever directory you chose to download and unzip the course material, you are in the right place. If not, use os.chdir(&lt;...&gt;) to change the working directory, where &lt;...&gt; is the full path of the python-workshop directory.\nLet’s store the relative path of our data into a variable and let’s check if the data file actually exists using the function os.path.exists()\n\ndata_file = '../course_materials/data/surveys.csv' # in your case the filepath should probably be ./data/surveys.csv\nprint(os.path.exists(data_file))\n\nTrue\n\n\nIf the result is True, we are all set up to go!",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reading and Exploring data</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-1.html#reading-data",
    "href": "data-science-with-pandas-1.html#reading-data",
    "title": "6  Reading and Exploring data",
    "section": "6.4 Reading data",
    "text": "6.4 Reading data\nThe very first operation we will perform is loading our data into a Pandas DataFrame using pd.read_csv().\n\nsurveys_df = pd.read_csv(data_file)\n\nprint(type(data_file))\nprint(type(surveys_df))\n\n&lt;class 'str'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\nPandas can read quite a large variety of formats like the comma-separated values (CSV) and Excel file formats.\nSometimes values in CSV files are separated using “;” or tabs. The default separator Pandas expects is a comma, if it is different it is necessary to specify the separator in an argument, e.g.: pd.read_csv(data_file, sep=\";\"). The documentation of pandas provides a full overview of the arguments you may use for this function.\nIn Jupyter Notebook or Jupyter Lab you can visualise the DataFrame simply by writing its name in a code cell and running the cell (in the same way you would display the value of any variable). Let’s have a look at our just created DataFrame:\n\nsurveys_df\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n35549 rows × 9 columns\n\n\n\n\nBy looking at the DataFrame we can finally understand what a DataFrame actually is: a 2-dimensional data structure storing different types of variables in columns. All rows in the DataFrame have a row index (starting from 0). The columns have names. The row indices and column names can be used to do operations on values in the column (we will go into this later).\nAs you can see Jupyter only prints the first and last 5 rows separated by ... . In this way the notebook remains clear and tidy (printing the whole DataFrame may result in a large table and a lot of scrolling to get to the next code cell).\nIt is, however, enough for a quick exploration of how the dataset looks like in terms of columns names, values, and potential reading errors.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reading and Exploring data</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-1.html#exploring-data",
    "href": "data-science-with-pandas-1.html#exploring-data",
    "title": "6  Reading and Exploring data",
    "section": "6.5 Exploring data",
    "text": "6.5 Exploring data\nNow we will take a closer look into the actual values in the DataFrame. In order to do this it is helpful to have the column names at hand (easy for copy-pasting):\n\nprint(surveys_df.columns)\n\nIndex(['record_id', 'month', 'day', 'year', 'plot_id', 'species_id', 'sex',\n       'hindfoot_length', 'weight'],\n      dtype='object')\n\n\nAs you can see this gives you more information than just the column names. It is also possible to just print the column names using a for loop (see chapter 4):\n\nfor column in surveys_df.columns:\n    print(column)\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\nLet’s select the column weight in our DataFrame and let’s run some statistics on it\n\nsurveys_df['weight']\n\n0         NaN\n1         NaN\n2         NaN\n3         NaN\n4         NaN\n         ... \n35544     NaN\n35545     NaN\n35546    14.0\n35547    51.0\n35548     NaN\nName: weight, Length: 35549, dtype: float64\n\n\nNow let’s plot the values in a histogram:\n\nsurveys_df['weight'].plot(kind='hist')\n\n\n\n\n\n\n\n\nDid you notice how easy it was to obtain a summary plot of a column of our DataFrame? We can repeat the same for every column with a single line of code.\n\nsurveys_df['hindfoot_length'].plot(kind='hist')\n\n\n\n\n\n\n\n\nWe can also make quick scatterplots to explore the relation between two columns:\n\n%matplotlib inline\n# what %matplotlib inline does will be discussed in the last chapter about visualization\nax1 = surveys_df.plot(x='weight', y='hindfoot_length', kind='scatter')\n\n\n\n\n\n\n\n\n\nExercises 0 and 1\nNow go to the Jupyter Dashboard in your internet browser, open the notebook afternoon_exercises.ipynb and continue with exercises 0 and 1.\n\n\n\n\n\n\n\nNote\n\n\n\nAs you can see in this exercise a DataFrame object comes with several methods that can be applied to the DataFrame. A method is similar to a function, but it can only be applied to the object it belongs to and has a different notation than a function.\nCompare the notation of the function len: len(surveys_df)\nwith the DataFrame specific method shape: surveys_df.shape\n\n\nInstead of running the methods above one by one, we can obtain a statistical summary using the method .describe(). Let’s get a statistical summary for the weight column.\n\nsurveys_df['weight'].describe()\n\ncount    32283.000000\nmean        42.672428\nstd         36.631259\nmin          4.000000\n25%         20.000000\n50%         37.000000\n75%         48.000000\nmax        280.000000\nName: weight, dtype: float64\n\n\nThere are many more methods that can be used. For a complete overview, check out the documentation of Pandas. Some useful ones are the unique method to display all unique values in a certain column:\n\nsurveys_df['species_id'].unique()\n\narray(['NL', 'DM', 'PF', 'PE', 'DS', 'PP', 'SH', 'OT', 'DO', 'OX', 'SS',\n       'OL', 'RM', nan, 'SA', 'PM', 'AH', 'DX', 'AB', 'CB', 'CM', 'CQ',\n       'RF', 'PC', 'PG', 'PH', 'PU', 'CV', 'UR', 'UP', 'ZL', 'UL', 'CS',\n       'SC', 'BA', 'SF', 'RO', 'AS', 'SO', 'PI', 'ST', 'CU', 'SU', 'RX',\n       'PB', 'PL', 'PX', 'CT', 'US'], dtype=object)\n\n\nOr .nunique() to return the number of unique elements in a column.\n\nprint(surveys_df['plot_id'].nunique())\n\n24\n\n\nPerhaps we want to get some insight into the values for certain species or plots, in the next chapter we will go into making groups and selections.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reading and Exploring data</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-2.html",
    "href": "data-science-with-pandas-2.html",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "",
    "text": "7.1 Recap: Load the data\nimport pandas as pd\nsurveys_df = pd.read_csv('../course_materials/data/surveys.csv')\nsurveys_df.describe()\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nhindfoot_length\nweight\n\n\n\n\ncount\n35549.000000\n35549.000000\n35549.000000\n35549.000000\n35549.000000\n31438.000000\n32283.000000\n\n\nmean\n17775.000000\n6.477847\n15.991195\n1990.475231\n11.397001\n29.287932\n42.672428\n\n\nstd\n10262.256696\n3.396925\n8.257366\n7.493355\n6.799406\n9.564759\n36.631259\n\n\nmin\n1.000000\n1.000000\n1.000000\n1977.000000\n1.000000\n2.000000\n4.000000\n\n\n25%\n8888.000000\n4.000000\n9.000000\n1984.000000\n5.000000\n21.000000\n20.000000\n\n\n50%\n17775.000000\n6.000000\n16.000000\n1990.000000\n11.000000\n32.000000\n37.000000\n\n\n75%\n26662.000000\n10.000000\n23.000000\n1997.000000\n17.000000\n36.000000\n48.000000\n\n\nmax\n35549.000000\n12.000000\n31.000000\n2002.000000\n24.000000\n70.000000\n280.000000\nIn addition to learning about characteristics of our dataset as a whole, we may be interested in analyzing parts (subsets) of our data. For exampe we want to know how heavy our samples are:\nsurveys_df['weight'].describe()\n\ncount    32283.000000\nmean        42.672428\nstd         36.631259\nmin          4.000000\n25%         20.000000\n50%         37.000000\n75%         48.000000\nmax        280.000000\nName: weight, dtype: float64\nWe can also extract one specific metric if we wish:\nsurveys_df['weight'].min()\nsurveys_df['weight'].max()\nsurveys_df['weight'].mean()\nsurveys_df['weight'].std()\nsurveys_df['weight'].count()\n\n32283",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Grouping, Indexing, Slicing, and Subsetting DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-2.html#selecting-data-using-column-names",
    "href": "data-science-with-pandas-2.html#selecting-data-using-column-names",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.2 Selecting data using column names",
    "text": "7.2 Selecting data using column names\nIn the morning session we saw how to get specific values from dictionaries using keys. We can do the same with DataFrames, in fact we have already accessed the values in a column by the column name. In this section we will show you two ways how to select values, slices of data and subsets of a DataFrame.\n\nsurveys_df[['species_id']]\n\n\n\n\n\n\n\n\n\nspecies_id\n\n\n\n\n0\nNL\n\n\n1\nNL\n\n\n2\nDM\n\n\n3\nDM\n\n\n4\nDM\n\n\n...\n...\n\n\n35544\nAH\n\n\n35545\nAH\n\n\n35546\nRM\n\n\n35547\nDO\n\n\n35548\nNaN\n\n\n\n\n35549 rows × 1 columns\n\n\n\n\nIn contrast to the examples we saw in the previous Section we are using double brackets [[...]] instead of single brackets. The single brackets retrieve a Pandas series of values and can only be applied to one column. The double brackets [[...]] tell Pandas to format the values of several columns as a Pandas DataFrame. With this syntax we can create a DataFrame that consists of the two columns plot_id and species_id!\n\nsurveys_df[['plot_id', 'species_id']]\n\n\n\n\n\n\n\n\n\nplot_id\nspecies_id\n\n\n\n\n0\n2\nNL\n\n\n1\n3\nNL\n\n\n2\n2\nDM\n\n\n3\n7\nDM\n\n\n4\n3\nDM\n\n\n...\n...\n...\n\n\n35544\n15\nAH\n\n\n35545\n15\nAH\n\n\n35546\n10\nRM\n\n\n35547\n7\nDO\n\n\n35548\n5\nNaN\n\n\n\n\n35549 rows × 2 columns",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Grouping, Indexing, Slicing, and Subsetting DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-2.html#slicing-subsets-of-rows",
    "href": "data-science-with-pandas-2.html#slicing-subsets-of-rows",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.3 Slicing subsets of rows",
    "text": "7.3 Slicing subsets of rows\nSlicing using the [] operator selects a set of rows and/or columns from a DataFrame. To slice out a set of rows, you use the following syntax: data[start:stop]. When slicing in pandas the start bound is included in the output. The stop bound is not included. The slicing stops before the stop bound. So if you want to select rows 0, 1 and 2 your code would look like this:\n\nsurveys_df[0:3]\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n\n\n\n\n\n\nWe can select specific ranges of our data in both the row and column directions using the row and column labels or using the row and column indices.\nLet’s have a look at iloc first. where we use the index of a row and/or column to select it. In the example below we select the first three entries and the columns month, day and year (the second, third and fourth column, remember indexing starts at 0 on Python). The first range of numbers selects the rows, the second the columns:\n\n# iloc[row slicing, column slicing]\nsurveys_df.iloc[0:3, 1:4]\n\n\n\n\n\n\n\n\n\nmonth\nday\nyear\n\n\n\n\n0\n7\n16\n1977\n\n\n1\n7\n16\n1977\n\n\n2\n7\n16\n1977\n\n\n\n\n\n\n\n\nWe can achieve the same result in a different way:\nIn a first step we select the columns by their names surveys_df[['month', 'day', 'year']]. From the resulting DataFrame we then, in a second step, select the first three rows [0:3]. Putting the two steps together, the code looks like this:\n\nsurveys_df[['month', 'day', 'year']][0:3]\n\n\n\n\n\n\n\n\n\nmonth\nday\nyear\n\n\n\n\n0\n7\n16\n1977\n\n\n1\n7\n16\n1977\n\n\n2\n7\n16\n1977\n\n\n\n\n\n\n\n\nWhat happens in this command?\n\n# data.iloc[row, column]\nsurveys_df.iloc[2, 6]\n\n'F'\n\n\n\n7.3.1 Summary: Selecting slices, rows and columns\nIn the first method we extract the column specifying its name. The second method is essentially identical to the first one as the 6th (index 5) element of the Series surveys_df.columns is species_id. The third method uses the method iloc to select all the rows of the 6th column. Additionally there is also a .loc method that can be used for this, but to avoid further confusing we leave it out for now. If your are interested, read this more detailed explanation.\n\n# By name\n# --------------------------------------\n# Method1\nplot_species_1 = surveys_df['species_id']\n\n# By location\n# --------------------------------------\n# Method2\nplot_id_2 = surveys_df[surveys_df.columns[5]]\n\n# Method3\nplot_id_3 = surveys_df.iloc[:,5]\n# --------------------------------------\n\n\nExercise 2 and 3\nNow go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercises 2 and 3.\n\n\n7.3.2 Subsetting Data according to user-defined criteria\nWe can extract subsets of our DataFrame following the general syntax data_frame[&lt;condition_on_data&gt;]  is a conditional statement on the DataFrame content itself. You may think at the conditional statement as a question or query you ask to your DataFrame. Here there are some examples:\n\n# What are the data collected in the year 2002?\nsurveys_df[surveys_df['year'] == 2002]\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n33320\n33321\n1\n12\n2002\n1\nDM\nM\n38.0\n44.0\n\n\n33321\n33322\n1\n12\n2002\n1\nDO\nM\n37.0\n58.0\n\n\n33322\n33323\n1\n12\n2002\n1\nPB\nM\n28.0\n45.0\n\n\n33323\n33324\n1\n12\n2002\n1\nAB\nNaN\nNaN\nNaN\n\n\n33324\n33325\n1\n12\n2002\n1\nDO\nM\n35.0\n29.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2229 rows × 9 columns\n\n\n\n\n\n# What are the data NOT collected in the year 2002?\nsurveys_df[surveys_df['year'] != 2002]\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n33315\n33316\n12\n16\n2001\n11\nNaN\nNaN\nNaN\nNaN\n\n\n33316\n33317\n12\n16\n2001\n13\nNaN\nNaN\nNaN\nNaN\n\n\n33317\n33318\n12\n16\n2001\n14\nNaN\nNaN\nNaN\nNaN\n\n\n33318\n33319\n12\n16\n2001\n15\nNaN\nNaN\nNaN\nNaN\n\n\n33319\n33320\n12\n16\n2001\n16\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n33320 rows × 9 columns\n\n\n\n\n\n# What are the data NOT collected in the year 2002? (different syntax)\nsurveys_df[~(surveys_df['year'] == 2002)]\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n33315\n33316\n12\n16\n2001\n11\nNaN\nNaN\nNaN\nNaN\n\n\n33316\n33317\n12\n16\n2001\n13\nNaN\nNaN\nNaN\nNaN\n\n\n33317\n33318\n12\n16\n2001\n14\nNaN\nNaN\nNaN\nNaN\n\n\n33318\n33319\n12\n16\n2001\n15\nNaN\nNaN\nNaN\nNaN\n\n\n33319\n33320\n12\n16\n2001\n16\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n33320 rows × 9 columns\n\n\n\n\nOur filtering conditions may be very specific, they can target different columns in the DataFrame, and they can be combined using the logical operator “&” which means and:\n\n# What are the data collected between 2000 and 2002 on female species?\nsurveys_df[(surveys_df['year'] &gt;= 2000) & (surveys_df['year'] &lt;= 2002) &\n           (surveys_df['sex'] == 'F')]\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n30158\n30159\n1\n8\n2000\n1\nPP\nF\n22.0\n17.0\n\n\n30160\n30161\n1\n8\n2000\n1\nPP\nF\n21.0\n17.0\n\n\n30164\n30165\n1\n8\n2000\n1\nPP\nF\n22.0\n15.0\n\n\n30168\n30169\n1\n8\n2000\n2\nPB\nF\n25.0\n24.0\n\n\n30171\n30172\n1\n8\n2000\n2\nNL\nF\n30.0\n137.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35539\n35540\n12\n31\n2002\n15\nPB\nF\n26.0\n23.0\n\n\n35540\n35541\n12\n31\n2002\n15\nPB\nF\n24.0\n31.0\n\n\n35541\n35542\n12\n31\n2002\n15\nPB\nF\n26.0\n29.0\n\n\n35542\n35543\n12\n31\n2002\n15\nPB\nF\n27.0\n34.0\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n\n\n2582 rows × 9 columns\n\n\n\n\nSpotlight on Syntax: We can introduce linebreaks in commands.\nBelow we filter for rows with collected data on female species in the year 2000 or 2002. “Give me all data where sex is Female and data is collected in 2000 or 2002”.\nThe method isin() allows to specify a range of “permitted” values for a certain column. Here it follows another example:\n\nsurveys_df[(surveys_df['year'] == 2000) & (surveys_df['sex'] == 'F') & \n           (surveys_df['month'].isin([1,3,4]))]\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n30158\n30159\n1\n8\n2000\n1\nPP\nF\n22.0\n17.0\n\n\n30160\n30161\n1\n8\n2000\n1\nPP\nF\n21.0\n17.0\n\n\n30164\n30165\n1\n8\n2000\n1\nPP\nF\n22.0\n15.0\n\n\n30168\n30169\n1\n8\n2000\n2\nPB\nF\n25.0\n24.0\n\n\n30171\n30172\n1\n8\n2000\n2\nNL\nF\n30.0\n137.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n30637\n30638\n4\n30\n2000\n20\nPP\nF\n22.0\n20.0\n\n\n30640\n30641\n4\n30\n2000\n20\nNL\nF\n30.0\nNaN\n\n\n30645\n30646\n4\n30\n2000\n24\nPP\nF\n20.0\n17.0\n\n\n30647\n30648\n4\n30\n2000\n17\nDM\nF\n36.0\n46.0\n\n\n30648\n30649\n4\n30\n2000\n17\nDO\nF\n36.0\n59.0\n\n\n\n\n156 rows × 9 columns\n\n\n\n\nWe have also an operator for or. For the sake of showing the syntax, below we fetch all entries from the year 2000 or from the gender female:\n\nprint(surveys_df[(surveys_df['year'] == 2000) | (surveys_df['sex'] == 'F')])\n\n       record_id  month  day  year  plot_id species_id sex  hindfoot_length  \\\n2              3      7   16  1977        2         DM   F             37.0   \n6              7      7   16  1977        2         PE   F              NaN   \n8              9      7   16  1977        1         DM   F             34.0   \n9             10      7   16  1977        6         PF   F             20.0   \n10            11      7   16  1977        5         DS   F             53.0   \n...          ...    ...  ...   ...      ...        ...  ..              ...   \n35539      35540     12   31  2002       15         PB   F             26.0   \n35540      35541     12   31  2002       15         PB   F             24.0   \n35541      35542     12   31  2002       15         PB   F             26.0   \n35542      35543     12   31  2002       15         PB   F             27.0   \n35546      35547     12   31  2002       10         RM   F             15.0   \n\n       weight  \n2         NaN  \n6         NaN  \n8         NaN  \n9         NaN  \n10        NaN  \n...       ...  \n35539    23.0  \n35540    31.0  \n35541    29.0  \n35542    34.0  \n35546    14.0  \n\n[16552 rows x 9 columns]\n\n\nSummary on logical operators in Pandas\n\n\n\noperator\npython\nPandas\n\n\n\n\nand\nand\n&\n\n\nor\nor\n|\n\n\nnot\n!\n~\n\n\n\n\nExercise 4\nNow go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercise 4.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Grouping, Indexing, Slicing, and Subsetting DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-2.html#optional-dataframe-cleaning",
    "href": "data-science-with-pandas-2.html#optional-dataframe-cleaning",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.4 Optional: DataFrame Cleaning",
    "text": "7.4 Optional: DataFrame Cleaning\nA simple exploration of our DataFrame showed us that there are columns full of invalid values (NaN). One of the most important preliminary operations of data analysis is cleaning your data set, i.e. “getting rid” of non-numerical or non-character values. we want to make sure that our data only contains meaningful values.\nNow that we mastered selecting, slicing, and subsetting, we can easily clean our DataFrame with few lines of code. Let us have a look at the function isnull. It is a Pandas function which we imported at the beginning with import pandas as pd. Now we can call the functin like this:\n\npd.isnull(4)\n\nFalse\n\n\n\npd.isnull([1, 2, 3, '', dict(), None])\n\narray([False, False, False, False, False,  True])\n\n\nWe can pass single values or array-like values to the function. The function will then check for us whether each value is NaN (Not a Number) or None and return a boolean array. Note, that values like the empty string (a strin without any characters in it) or an empty dictionary etc will not count as null value, they do have a type, they only do not contain any values but they are something. null values in python are only NaN and None. When you read in tabular data into a DataFrame empty cells will be shown as NaN. None stands for the type NoneType, which we will not dive into further in this workshop.\nWith all that kowledge we can now detect null values in the column weight and do something about it. Let us have a look how many null values we can find:\n\npd.isnull(surveys_df['weight']) # boolean array indicating where null values are found\n\n0         True\n1         True\n2         True\n3         True\n4         True\n         ...  \n35544     True\n35545     True\n35546    False\n35547    False\n35548     True\nName: weight, Length: 35549, dtype: bool\n\n\nWe can use the Series with the boolean values as a mask on the DataFrame. Here we only extract the rows of surveys_df where the weight is not defined:\n\nsurveys_df[pd.isnull(surveys_df['weight'])] # all lines that have a null value in the column weight\nlen(surveys_df[pd.isnull(surveys_df['weight'])]) # length\n\n3266\n\n\nAs you can see, in our whole dataset 3266 weight values are not usable. We need to do something with those values.\nAnother thing that would not make sense are negative weights. Let’s check whether the remaining 32283 values in the weight column are positive:\n\nlen(surveys_df[surveys_df['weight'] &gt; 0])\n\n32283\n\n\nAs we see, we have 32283 non-negative weight values. The remaining 3266 values in the weight column are not set, so they are null. How can we impute the values? Let us have a look at the average weight:\n\nsurveys_df['weight'].mean()\n\n42.672428212991356\n\n\nA smooth run, without errors or warnings. As we said several times, Pandas is a library designed for data analysis and when performing data analysis it is very common to deal with not numeric values. In particular, the .mean() method has an argument called skipna that when set True (default value, so we do not need to specify it) excludes NaN values. This means that, in this case, Pandas simply ignores whatever it is not numeric and it performs computations only on numeric values.\nIf we are not happy with Pandas default behaviour, we can manually decide which value to assign to cells that contain null values. One possible choice is setting them to zero. To do that, we just need to apply the method .fillna(&lt;value&gt;), where &lt;value&gt; is the number we want to substitute to the null value with (in our case, 0).\n\ncleaned_weight1 = surveys_df['weight'].fillna(0)\ncleaned_weight_ave1 = cleaned_weight1.mean()\nprint(cleaned_weight_ave1)\n\n38.751976145601844\n\n\nYou see that when filling the null values with 0, the average weight decreases. This is because the mean is now computed on data with many more zeros compared to the previous one. Conscious of this problem, we may now choose a more appropriate value to “fill” our null values. How about we use the “clean” mean of our first computation?\n\ncleaned_weight2 = surveys_df['weight'].fillna(surveys_df['weight'].mean())\ncleaned_weight_ave2 = cleaned_weight2.mean()\nprint(cleaned_weight_ave2)\n\n42.672428212991356\n\n\nThis time we obtain exactly the same result of our first computation, this is because we substituted the null values with a mean computed excluding the null values.\n\n(Optional) Exercise 5\nTo deepen the knowledge you can continue with Exercise 5.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Grouping, Indexing, Slicing, and Subsetting DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-2.html#optional-grouping",
    "href": "data-science-with-pandas-2.html#optional-grouping",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.5 Optional: Grouping",
    "text": "7.5 Optional: Grouping\nWe often want to calculate summary statistics grouped by subsets or attributes within fields of our data. For example, we might want to calculate the average weight of all individuals per site.\nAs we have seen above we can calculate basic statistics for all records in a single column using the syntax below:\n\nsurveys_df['weight'].describe()\n\ncount    32283.000000\nmean        42.672428\nstd         36.631259\nmin          4.000000\n25%         20.000000\n50%         37.000000\n75%         48.000000\nmax        280.000000\nName: weight, dtype: float64\n\n\nIf we want to summarize by one or more variables, for example sex, we can use Pandas’ .groupby() method. Once we’ve created a groupby DataFrame, we can quickly calculate summary statistics by a group of our choice.\n\ngrouped_data = surveys_df.groupby('sex')\ngrouped_data.describe()\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\n...\nhindfoot_length\nweight\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\n15690.0\n18036.412046\n10423.089000\n3.0\n8917.50\n18075.5\n27250.00\n35547.0\n15690.0\n6.587253\n...\n36.0\n64.0\n15303.0\n42.170555\n36.847958\n4.0\n20.0\n34.0\n46.0\n274.0\n\n\nM\n17348.0\n17754.835601\n10132.203323\n1.0\n8969.75\n17727.5\n26454.25\n35548.0\n17348.0\n6.396184\n...\n36.0\n58.0\n16879.0\n42.995379\n36.184981\n4.0\n20.0\n39.0\n49.0\n280.0\n\n\n\n\n2 rows × 56 columns\n\n\n\n\nThe output is a bit overwhelming. Let’s just have a look at one statistical value, the mean, to understand what is happening here:\n\ngrouped_data.mean()\n\n/tmp/ipykernel_2415/1133710423.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  grouped_data.mean()\n\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nhindfoot_length\nweight\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\nF\n18036.412046\n6.587253\n15.880943\n1990.644997\n11.440854\n28.836780\n42.170555\n\n\nM\n17754.835601\n6.396184\n16.078799\n1990.480401\n11.098282\n29.709578\n42.995379\n\n\n\n\n\n\n\n\nWe see that the data is divided into two groups, one group where the value in the column sex equals “F” and another group where the value in the column sex equals “M”. The statistics is then calculated for all samples in that specific group for each of the columns in the dataframe. Note that samples annotated with sex equals NaN and column values with NaN are left out.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Grouping, Indexing, Slicing, and Subsetting DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-2.html#optional-structure-of-a-groupby-object",
    "href": "data-science-with-pandas-2.html#optional-structure-of-a-groupby-object",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.6 Optional: Structure of a groupby object",
    "text": "7.6 Optional: Structure of a groupby object\nWe can investigate which rows are assigned to which group as follows:\n\nprint(type(grouped_data.groups)) # dictionary\nprint(\"Sexes: \", grouped_data.groups.keys()) # keys are the unique values of the column we grouped by\nprint(\"Rows belonging to sex 'F': \", grouped_data.groups['F']) # values are row indexes \n\n&lt;class 'pandas.io.formats.printing.PrettyDict'&gt;\nSexes:  dict_keys(['F', 'M'])\nRows belonging to sex 'F':  Int64Index([    2,     6,     8,     9,    10,    14,    15,    16,    19,\n               20,\n            ...\n            35531, 35532, 35535, 35536, 35537, 35539, 35540, 35541, 35542,\n            35546],\n           dtype='int64', length=15690)",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Grouping, Indexing, Slicing, and Subsetting DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-2.html#optional-grouping-by-multiple-columns",
    "href": "data-science-with-pandas-2.html#optional-grouping-by-multiple-columns",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.7 Optional: Grouping by multiple columns",
    "text": "7.7 Optional: Grouping by multiple columns\nNow let’s have a look at a more complex grouping example. We want an overview statistics of the weight of all females and males by plot id. So in fact we want to group by sex and by plot_id at the same time.\nThis will give us exactly 48 groups for our survey data:\n\nfemale, plot id = 1\nfemale, plot id = 2\n…\nfemale, plot id = 24\nmale, plot id = 1\n…\nmale, plot id = 24\n\nWhy 48 groups? We have 24 unique values for plot_id. Per plot we have two groups of samples, female and male. Hence, the grouping returns 48 groups.\n\ngrouped_data = surveys_df.groupby(['plot_id', 'sex'])\ngrouped_data['weight'].describe()\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nplot_id\nsex\n\n\n\n\n\n\n\n\n\n\n\n\n1\nF\n826.0\n46.311138\n33.240958\n5.0\n26.00\n40.0\n50.00\n196.0\n\n\nM\n1072.0\n55.950560\n41.035686\n4.0\n37.00\n46.0\n54.00\n231.0\n\n\n2\nF\n954.0\n52.561845\n45.547697\n5.0\n25.00\n40.0\n51.00\n274.0\n\n\nM\n1114.0\n51.391382\n46.690887\n5.0\n24.00\n42.0\n50.00\n278.0\n\n\n3\nF\n873.0\n31.215349\n30.687451\n4.0\n15.00\n23.0\n34.00\n199.0\n\n\nM\n827.0\n34.163241\n40.260426\n5.0\n13.00\n23.0\n39.00\n250.0\n\n\n4\nF\n850.0\n46.818824\n33.560664\n5.0\n28.00\n40.0\n47.00\n200.0\n\n\nM\n1010.0\n48.888119\n32.254168\n4.0\n32.00\n44.5\n50.00\n187.0\n\n\n5\nF\n516.0\n40.974806\n36.396966\n5.0\n21.00\n35.0\n45.00\n248.0\n\n\nM\n573.0\n40.708551\n31.250967\n6.0\n21.00\n40.0\n49.00\n240.0\n\n\n6\nF\n721.0\n36.352288\n29.513333\n5.0\n19.00\n29.0\n41.00\n188.0\n\n\nM\n739.0\n36.867388\n30.867779\n6.0\n18.00\n31.0\n46.00\n241.0\n\n\n7\nF\n326.0\n20.006135\n17.895937\n6.0\n12.00\n17.0\n23.00\n170.0\n\n\nM\n303.0\n21.194719\n23.971252\n4.0\n11.00\n17.0\n23.00\n235.0\n\n\n8\nF\n817.0\n45.623011\n31.045426\n5.0\n25.00\n42.0\n50.00\n178.0\n\n\nM\n962.0\n49.641372\n34.820355\n5.0\n29.00\n45.0\n52.00\n173.0\n\n\n9\nF\n823.0\n53.618469\n35.572793\n6.0\n35.00\n43.0\n54.00\n177.0\n\n\nM\n984.0\n49.519309\n31.888023\n6.0\n37.00\n46.0\n50.00\n275.0\n\n\n10\nF\n138.0\n17.094203\n14.074820\n7.0\n10.00\n13.0\n20.00\n130.0\n\n\nM\n139.0\n19.971223\n25.061068\n4.0\n10.00\n12.0\n22.00\n237.0\n\n\n11\nF\n796.0\n43.515075\n29.627049\n5.0\n27.00\n40.0\n46.00\n208.0\n\n\nM\n994.0\n43.366197\n28.425105\n6.0\n25.00\n43.0\n49.00\n212.0\n\n\n12\nF\n1040.0\n49.831731\n43.790247\n6.0\n26.00\n41.0\n48.25\n264.0\n\n\nM\n1174.0\n48.909710\n39.301038\n7.0\n25.25\n43.0\n50.00\n280.0\n\n\n13\nF\n610.0\n40.524590\n36.109806\n5.0\n21.00\n31.0\n42.00\n192.0\n\n\nM\n757.0\n40.097754\n31.753448\n6.0\n20.00\n34.0\n47.00\n241.0\n\n\n14\nF\n692.0\n47.355491\n29.563455\n5.0\n37.00\n43.0\n48.00\n211.0\n\n\nM\n1029.0\n45.159378\n25.272173\n5.0\n35.00\n44.0\n50.00\n222.0\n\n\n15\nF\n467.0\n26.670236\n31.983137\n4.0\n12.50\n18.0\n26.00\n198.0\n\n\nM\n401.0\n27.523691\n38.631271\n4.0\n10.00\n18.0\n25.00\n259.0\n\n\n16\nF\n211.0\n25.810427\n20.902314\n4.0\n13.00\n21.0\n31.00\n158.0\n\n\nM\n265.0\n23.811321\n14.663726\n5.0\n11.00\n20.0\n35.00\n61.0\n\n\n17\nF\n874.0\n48.176201\n37.485528\n6.0\n27.00\n41.0\n49.00\n192.0\n\n\nM\n1011.0\n47.558853\n34.082010\n4.0\n27.00\n45.0\n51.00\n216.0\n\n\n18\nF\n740.0\n36.963514\n35.184417\n5.0\n17.00\n28.5\n40.00\n212.0\n\n\nM\n607.0\n43.546952\n41.864279\n7.0\n18.00\n33.0\n48.00\n256.0\n\n\n19\nF\n514.0\n21.978599\n14.008822\n6.0\n12.00\n20.0\n29.00\n139.0\n\n\nM\n567.0\n20.306878\n12.553954\n4.0\n10.00\n19.0\n25.00\n100.0\n\n\n20\nF\n631.0\n52.624406\n55.257665\n5.0\n17.00\n30.0\n48.00\n220.0\n\n\nM\n588.0\n44.197279\n43.361503\n5.0\n17.00\n34.0\n47.00\n223.0\n\n\n21\nF\n596.0\n25.974832\n22.619863\n4.0\n11.00\n24.0\n31.00\n188.0\n\n\nM\n431.0\n22.772622\n18.984554\n4.0\n9.00\n19.0\n32.00\n190.0\n\n\n22\nF\n646.0\n53.647059\n38.588538\n5.0\n29.00\n39.0\n54.00\n161.0\n\n\nM\n648.0\n54.572531\n38.841066\n6.0\n31.00\n44.0\n53.00\n212.0\n\n\n23\nF\n163.0\n20.564417\n18.933945\n8.0\n12.00\n16.0\n23.00\n199.0\n\n\nM\n205.0\n18.941463\n17.979740\n4.0\n10.00\n12.0\n22.00\n131.0\n\n\n24\nF\n479.0\n47.914405\n49.112574\n6.0\n21.00\n33.0\n44.00\n251.0\n\n\nM\n479.0\n39.321503\n42.003947\n4.0\n17.00\n24.0\n45.00\n230.0\n\n\n\n\n\n\n\n\nWith the same grouped data we can compare the average weight between the males and females per plot and visualise that as stacked bar plot:\n\ngrouped_data = surveys_df.groupby(['plot_id', 'sex'])\ngrouped_data['weight'].mean().plot(kind='bar')\n\n\n\n\n\n\n\n\nFor each key, there is a bar in the plot. We can get a more insightful plot when we unstack the values for each key and only put the numbers on the x-axis:\n\ngrouped_data['weight'].mean().unstack().plot(kind = 'bar')",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Grouping, Indexing, Slicing, and Subsetting DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-2.html#optional-summary-grouping",
    "href": "data-science-with-pandas-2.html#optional-summary-grouping",
    "title": "7  Grouping, Indexing, Slicing, and Subsetting DataFrames",
    "section": "7.8 Optional: Summary grouping",
    "text": "7.8 Optional: Summary grouping\nGrouping is one of the most common operation in data analysis. Data often consists of different measurements on the same samples. In many cases we are not only interested in one particular measurement but in the cross product of measurements. In the picture below we labeled samples with green lines, blue dots and red lines. We are now interested how these three different groups relate to each other given the all other measurements in the dataframe. Pandas’ groupby function gives us the means to compare these three groups with several built-in statistical methods.\n\n\n\nGrouping sketch\n\n\n\nOptional Exercise 6 to 8\nTo deepen the knowledge you can do Exercises 6 to 8.\nAfter you finished the exercises please come back to this document and continue with the following chapter.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Grouping, Indexing, Slicing, and Subsetting DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-3.html",
    "href": "data-science-with-pandas-3.html",
    "title": "8  Combining DataFrames",
    "section": "",
    "text": "8.1 Concatenating DataFrames\nThe first way we will combine DataFrames is concatenation, i.e. simply putting DataFrames one after the other either vertically or horizontally.\nConcatenation can be used if the DataFrames are similar, meaning that they either have the same rows or columns. We will see examples of this later.\nTo concatenate two DataFrames you will use the function pd.concat(), specifying as arguments the DataFrames to concatenate and axis=0 or axis=1 for vertical or horizontal concatenation, respectively.\nWe will only be looking at vertical concatenation, but it is good to be aware that there is more to be discovered beyond what we describe here.\nLet us first obtain two small DataFrames from the larger surveys_df dataset.\n# Subsetting DataFrames\nsurveys_df_sub_first10 = surveys_df.head(10)\nsurveys_df_sub_last10  = surveys_df.tail(10)\nWe now have two DataFrames, one with the first ten rows of the original dataset, and another with the last ten rows.\nVertical stacking can be understood as combining two DataFrames that have different sets of the same type of data. In our example, it may be that one field researcher has registered the first ten entries, and another did the last ten, both using the same laboratory sheets. They both wrote down the same information (weight, species, and so on) of all different individuals. If we combine them, we have one list of twenty records, rather than two lists of ten.\n# Stack the DataFrames on top of each other\nvertical_stack = pd.concat([surveys_df_sub_first10, surveys_df_sub_last10], axis=0)\nprint(vertical_stack.info())\nvertical_stack\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 20 entries, 0 to 35548\nData columns (total 9 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   record_id        20 non-null     int64  \n 1   month            20 non-null     int64  \n 2   day              20 non-null     int64  \n 3   year             20 non-null     int64  \n 4   plot_id          20 non-null     int64  \n 5   species_id       19 non-null     object \n 6   sex              16 non-null     object \n 7   hindfoot_length  15 non-null     float64\n 8   weight           6 non-null      float64\ndtypes: float64(2), int64(5), object(2)\nmemory usage: 1.6+ KB\nNone\n\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n5\n6\n7\n16\n1977\n1\nPF\nM\n14.0\nNaN\n\n\n6\n7\n7\n16\n1977\n2\nPE\nF\nNaN\nNaN\n\n\n7\n8\n7\n16\n1977\n1\nDM\nM\n37.0\nNaN\n\n\n8\n9\n7\n16\n1977\n1\nDM\nF\n34.0\nNaN\n\n\n9\n10\n7\n16\n1977\n6\nPF\nF\n20.0\nNaN\n\n\n35539\n35540\n12\n31\n2002\n15\nPB\nF\n26.0\n23.0\n\n\n35540\n35541\n12\n31\n2002\n15\nPB\nF\n24.0\n31.0\n\n\n35541\n35542\n12\n31\n2002\n15\nPB\nF\n26.0\n29.0\n\n\n35542\n35543\n12\n31\n2002\n15\nPB\nF\n27.0\n34.0\n\n\n35543\n35544\n12\n31\n2002\n15\nUS\nNaN\nNaN\nNaN\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\nThe resulting DataFrame (vertical_stack) consists, as expected, of 20 rows. These are the result of the first and last 10 rows of our original DataFrame surveys_df.\nYou may have noticed that the last ten rows have very high index, not consecutive with the first ten rows. This is because concatenation preserves the indices of the two original DataFrames. If you want a brand new set of indices for your concateneted DataFrame, simply reset the indices using the method .reset_index(). Notice that this adds a column index to your DataFrame, that maintains the original index. If you pass drop=True into the function, you will avoid the addition of this column.\nvertical_stack.reset_index()\n\n\n\n\n\n\n\n\n\nindex\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n5\n5\n6\n7\n16\n1977\n1\nPF\nM\n14.0\nNaN\n\n\n6\n6\n7\n7\n16\n1977\n2\nPE\nF\nNaN\nNaN\n\n\n7\n7\n8\n7\n16\n1977\n1\nDM\nM\n37.0\nNaN\n\n\n8\n8\n9\n7\n16\n1977\n1\nDM\nF\n34.0\nNaN\n\n\n9\n9\n10\n7\n16\n1977\n6\nPF\nF\n20.0\nNaN\n\n\n10\n35539\n35540\n12\n31\n2002\n15\nPB\nF\n26.0\n23.0\n\n\n11\n35540\n35541\n12\n31\n2002\n15\nPB\nF\n24.0\n31.0\n\n\n12\n35541\n35542\n12\n31\n2002\n15\nPB\nF\n26.0\n29.0\n\n\n13\n35542\n35543\n12\n31\n2002\n15\nPB\nF\n27.0\n34.0\n\n\n14\n35543\n35544\n12\n31\n2002\n15\nUS\nNaN\nNaN\nNaN\n\n\n15\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n16\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n17\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n18\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n19\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Combining DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-3.html#concatenating-dataframes",
    "href": "data-science-with-pandas-3.html#concatenating-dataframes",
    "title": "8  Combining DataFrames",
    "section": "",
    "text": "Now, let us do some vertical concatenation or stacking. In this case the two DataFrames are simply stacked ‘on top of’ each other (remember to specify axis=0).",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Combining DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-3.html#joining-dataframes",
    "href": "data-science-with-pandas-3.html#joining-dataframes",
    "title": "8  Combining DataFrames",
    "section": "8.2 Joining DataFrames",
    "text": "8.2 Joining DataFrames\nConcatenating DataFrames allows you to combine two entire DataFrames into a single one. In many cases, you want to combine only selected parts of two DataFrames.\nYou might, for example, want to merge rows of two DataFrames that have matching values in specific columns. The pandas function merge() performs an operation that you may know as a join if you worked with databases before. The join operation joins the content of two DataFrames in a particular way. There are different types of joins, but the workflow to perform a join operation is always the same:\n\nYou identify a left and a right DataFrame, among the two you want to join;\nYou identify in both your left and right DataFrame a column (or set of columns) to join on;\nYou choose the type of join;\nYou perform the join running the function pd.merge() with the specified inputs and options.\n\nWhat it means for a DataFrame to be ‘left’ or ‘right’ depends on the type of join. The main thing to remember is that it matters which DataFrame you mention first when performing a join.\nWe will only be looking at what is know as an inner join. Investigating the other types of joins is left as an exercise.\nLet’s see some join example using the first ten rows of our surveys_df DataFrame (surveys_df_sub_first10). This will be our left DataFrame, containing data on individual animals. Our right DataFrame will be the first 20 rows of species_df, containing some information on each species. Let us rename them for the purpose of the join:\n\nleft_df = surveys_df_sub_first10\nright_df = species_df.head(20)\n\n\nleft_df\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n5\n6\n7\n16\n1977\n1\nPF\nM\n14.0\nNaN\n\n\n6\n7\n7\n16\n1977\n2\nPE\nF\nNaN\nNaN\n\n\n7\n8\n7\n16\n1977\n1\nDM\nM\n37.0\nNaN\n\n\n8\n9\n7\n16\n1977\n1\nDM\nF\n34.0\nNaN\n\n\n9\n10\n7\n16\n1977\n6\nPF\nF\n20.0\nNaN\n\n\n\n\n\n\n\n\n\nright_df\n\n\n\n\n\n\n\n\n\nspecies_id\ngenus\nspecies\ntaxa\n\n\n\n\n0\nAB\nAmphispiza\nbilineata\nBird\n\n\n1\nAH\nAmmospermophilus\nharrisi\nRodent\n\n\n2\nAS\nAmmodramus\nsavannarum\nBird\n\n\n3\nBA\nBaiomys\ntaylori\nRodent\n\n\n4\nCB\nCampylorhynchus\nbrunneicapillus\nBird\n\n\n5\nCM\nCalamospiza\nmelanocorys\nBird\n\n\n6\nCQ\nCallipepla\nsquamata\nBird\n\n\n7\nCS\nCrotalus\nscutalatus\nReptile\n\n\n8\nCT\nCnemidophorus\ntigris\nReptile\n\n\n9\nCU\nCnemidophorus\nuniparens\nReptile\n\n\n10\nCV\nCrotalus\nviridis\nReptile\n\n\n11\nDM\nDipodomys\nmerriami\nRodent\n\n\n12\nDO\nDipodomys\nordii\nRodent\n\n\n13\nDS\nDipodomys\nspectabilis\nRodent\n\n\n14\nDX\nDipodomys\nsp.\nRodent\n\n\n15\nEO\nEumeces\nobsoletus\nReptile\n\n\n16\nGS\nGambelia\nsilus\nReptile\n\n\n17\nNL\nNeotoma\nalbigula\nRodent\n\n\n18\nNX\nNeotoma\nsp.\nRodent\n\n\n19\nOL\nOnychomys\nleucogaster\nRodent\n\n\n\n\n\n\n\n\nThe column we want to perform the join on is the one containing information about the species_id. Conveniently, this column has the same label in both DataFrames. Note that this is not always the case.\n\ninner_join = pd.merge(left_df, right_df, left_on='species_id', right_on='species_id', how='inner')\ninner_join\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\ngenus\nspecies\ntaxa\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\nNeotoma\nalbigula\nRodent\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\nNeotoma\nalbigula\nRodent\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n5\n8\n7\n16\n1977\n1\nDM\nM\n37.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n6\n9\n7\n16\n1977\n1\nDM\nF\n34.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n\n\n\n\n\n\nAs you may notice, the resulting DataFrame has only seven rows, while our original DataFrames had 10 and 20 rows, respectively. This is because an inner join selects only those rows where the value of the joined column occurs in both DataFrames (mathematically, an intersection).\nAside from the inner join, there are three more types of join that you can do using the merge() function: - An inner join selects only the rows that result from the combination of matching rows in both the original left and right DataFrames (intersection); - A left join selects all rows that were in the original left DataFrame, some of which may have been joined with a matching entry from the right DataFrame; - A right join selects all rows that were in the original right DataFrame, some of which may have been joined with a matching entry from the left DataFrame; - An outer join merges the two DataFrames and keeps all resulting rows.\nTo better understand how join works, it may be useful to look at the diagrams below:\n\n\n\n\nDo you want to select only common information between the two DataFrames? Then you use an inner join;\nDo you want to add information to your left DataFrame? Then you use a left join;\nDo you want to add information to your right DataFrame? Then you use a right join;\nDo you want to get all the information from the two DataFrames? Then you use an outer join.\n\n\nExercises 9 and 10\nNow go to the Jupyter Dashboard in your internet browser and continue with exercise 9 and 10.\nWe will continue with Data visualization.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Combining DataFrames</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-4.html",
    "href": "data-science-with-pandas-4.html",
    "title": "9  Data Visualization",
    "section": "",
    "text": "9.1 Data Visualization with python\nMatplotlib is one of the most popular and widely-used data visualization libraries for Python. Matplotlib was inspired by the plotting functionalities of MATLAB (a non-open source programming language). It provides a comprehensive set of tools for creating a broad variety of plot types, such as line plots, scatter plots, bar plots, histograms, heatmaps, and many more.\nIn this session we will go through the main matplotlib concepts and we will generate several plots to illustrate the potential of matplotlib.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-4.html#preliminaries",
    "href": "data-science-with-pandas-4.html#preliminaries",
    "title": "9  Data Visualization",
    "section": "9.2 Preliminaries",
    "text": "9.2 Preliminaries\nWe begin importing the pandas package in the same way we did in previous sessions:\n\nimport pandas as pd\n\nUsually the first thing to do to start visualizing data with python is importing your visualization library, in our case the module pyplot from the matplotlib library. As with many of our previous imports, we import the module under an ‘alias’ (alternate shorter name) for convenience.\n\nimport matplotlib.pyplot as plt\n\nA slightly different, but equivalent way to import pyplot is the following:\n\nfrom matplotlib import pyplot as plt\n\nWe also specify the command %matplotlib inline so that, when plotting, Jupyter Notebook will not display the plots into new windows, but in the notebook itself (you are free to NOT run %matplotlib inline, make a plot, and see what happens!).\n\n%matplotlib inline\n\nFinally, as in previous sessions, we load the survey data into a pandas DataFrame and print the column names, just to see what data we can display in our plots:\n\nsurveys = pd.read_csv(('../course_materials/data/surveys.csv'))\n#in your case the path should probably be  'data/surveys.csv'\nsurveys.columns\n\nIndex(['record_id', 'month', 'day', 'year', 'plot_id', 'species_id', 'sex',\n       'hindfoot_length', 'weight'],\n      dtype='object')",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-4.html#simple-data-visualization",
    "href": "data-science-with-pandas-4.html#simple-data-visualization",
    "title": "9  Data Visualization",
    "section": "9.3 Simple data visualization",
    "text": "9.3 Simple data visualization\nAs you already found in session 6.5, data stored in pandas DataFrames can be visualized using a ‘method’ called (surprisingly!) plot:\n\nsurveys.plot(x='hindfoot_length',y='weight',kind='scatter',grid=True)\n\n\n\n\n\n\n\n\nThis is convenient since it allows for quick plotting with a limited amount of code. Check the documentation of Pandas to learn more about this.\nFrom now on we will create figures and plots using the matplotlib library. The plot method of pandas DataFrames is actually a wrapper around the plotting functionality of matplotlib. We will now try to create the same plot using the scatter function from matplotlib’s pyplot module:\n\nplt.scatter(x = surveys['hindfoot_length'],y = surveys['weight'], s=16)\nplt.grid()\nplt.ylabel('Weight [g]')\nplt.xlabel('Hindfoot length [mm]')\n\nText(0.5, 0, 'Hindfoot length [mm]')\n\n\n\n\n\n\n\n\n\nAs you can see the resulting plot is the same as above. When using plt.scatter we need a few more lines of code to get to this result, but plt provides more options to further customize the plot if we would want to. \n\n\n\n\n\n\nExpand to learn about Matplotlib plot structure\n\n\n\n\n\nIn the previous examples we generated very simple plots to have a quick look at the data stored in our pandas DataFrame. However, with Matplotlib you can customize many more aspects of your plot: axes, x and y ticks and labels, titles, legends, and much more. To get full control of the plots generated with Matplotlib.pyplot, it is important to be aware of the jargon used to describe the different layers of the figures that you create. Knowing the technical matplotlib language will be also of aid when asking AI tools to generate a plotting recipe for us (and to understand it!).\n\nAt the higher level we have Figures. A Figure is simply the total white space where you will organise your plots. You may think of it as the white page were you are going to draw your plots or also as a box containing all your plots. You can both have a single plot per Figure or multiple plots sharing the same Figure;\nAt a lower level we have Axes. Axes are contained into Figures. Axes is the name of a single plot or graph. This is probably the most confusing convention of matplotlib, as the word Axes, in common language, may indicate the x axis and y axis of a plot, but in matplotlib axes refers to a single entire plot. Be aware of this difference! You can have a single Axes per Figure, so one plot per Figure (see Plot1 on the left of the figure below) or multiple Axes per Figure, like in Plot2 (on the right) where the same Figure contains three plots distributed in two rows: two on top and one on the bottom;\nFinally, each Axes (aka each plot) contains two Axis, i.e. x and y axis, the guidelines to populate your plots with data.\n\n\n\n\nPlot Hierarchy\n\n\nTo summarize, matplotlib organizes plots into Figures, Axes, and Axis. A Figure is a canvas that can contain one or more Axes. An Axes is where data is plotted along two Axis, x and y. Specifying parameters at these three different levels, you can customize your plots to the finest details.\nCertain attributes like the Figure size and the number of plots inside the Figure belong to the Figure level. Ticks, labels, plot title (also the entire figure can have a title), legend, etc belong to the Axes level. Data is plotted on Axes according to the specified x and y Axis. The main features of a “typical” plot generated with matplotlib are well summarized by the picture below from matplotlib documentation:\n\n&lt;img src=\"images/anatomy.jpeg\" alt=\"Plot Main Features\" width=\"70%\"&gt;",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-4.html#more-plotting-options",
    "href": "data-science-with-pandas-4.html#more-plotting-options",
    "title": "9  Data Visualization",
    "section": "9.4 More plotting options",
    "text": "9.4 More plotting options\nLooking at our previous visualization, it seems that x and y label are too small, data points often overlap each other, and a title for the plot is missing. Furthermore, for publication purposes, we want our plot to occupy a space of 6x6 inches. Let’s apply these specifications to our visualization:\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.scatter(x = surveys['hindfoot_length'],y = surveys['weight'], s=12)\nax.grid(True)\nax.set_title('Scatter plot of weight vs. hindfoot length', fontsize=16)\nax.set_ylabel('Weight [g]', fontsize=14)\nax.set_xlabel('Hindfoot length [mm]', fontsize=14)\n\nText(0.5, 0, 'Hindfoot length [mm]')\n\n\n\n\n\n\n\n\n\nThis time we first created a Figure (fig) and an Axes (ax) object. On the Axes object we add the plot and specify some customizations:\n\ns regulates the size of the data points\nfigsize the (x,y) dimention in inches\nax.set_xlabel and ax.set_title are used to add the titles and labels to our plot\n\nTo modify the character sizes of the x and y labels, we can use the fontsize parameter of the set_xlabel and set_ylabel methods.\nIt is also possible to create other plot types, e.g. histograms, bar plots, box plots, etc. Let’s have a look at a histogram of the hindfoot length of our penguins for females and males:\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Filter the data for the two groups\nfemale_data = surveys[surveys[\"sex\"] == \"F\"]\nmale_data = surveys[surveys[\"sex\"] == \"M\"]\n\n# Plot the histograms\nax.hist([female_data[\"hindfoot_length\"].dropna(), male_data[\"hindfoot_length\"].dropna()], bins=20, rwidth=0.8, color=['green', 'blue'], label=['Female', 'Male'])\n\nax.set_xlabel('Hindfoot length')\nax.set_ylabel('Frequency')\nax.legend(loc='upper right')\n\nplt.show()",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-4.html#creating-subplots",
    "href": "data-science-with-pandas-4.html#creating-subplots",
    "title": "9  Data Visualization",
    "section": "9.5 Creating subplots",
    "text": "9.5 Creating subplots\nIn the histogram example, two subplots were created automatically. This is not possible for all plotting functions and methods. If we want to create a figure that consists of multiple plots, and have full control over what we put into the subplots, we use the plt.subplots() function. The first two arguments of the function (nrows and ncols) indicate the number of vertical and horizontal plots we want to fit in our figure (when not specified, the default value of these attributes is 1, one column and one row, i.e. a single plot). In this case, we will create two plots side to side, so our grid will have one row and two columns. As we want to be sure that there will be enough space for our two plots, we also specify the size of the Figure to be 12 inches long and 6 inches high (inches is the default size unit, but you can specify different ones).\nThe code below will produce 2 plots (one row and two columns). This means plt.subplots() will return one Figure object (the canvas containing all our plots) and 2 Axes objects (the 2 plots). We will store these two Axes into the variables ax1 and ax2.\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(12,6))\n\nax1.scatter(x = female_data['hindfoot_length'],y = female_data['weight'], s=12)\nax1.set_xlabel('Hindfoot length [mm]')\nax1.set_ylabel('Weight [g]')\nax1.set_title('Females')\n\nax2.scatter(x = male_data['hindfoot_length'],y = male_data['weight'], s=12, color='orange')\nax2.set_xlabel('Hindfoot length [mm]')\nax2.set_ylabel('Weight [g]')\nax2.set_title('Males')\n\nText(0.5, 1.0, 'Males')\n\n\n\n\n\n\n\n\n\nIn this case, we plotted two identical plots side by side using two different colours (default blue and orange). If you want to change any visual characteristic, for example, of the left plot, you would call method on ax1. Note that plots can be arranged also vertically just changing the values of ncols and nrows.\nWhen the number of plots becomes larger, it is convenient to store the Axes into a single variable and access them by row and column indices. Here an example:\n\nfig, axes = plt.subplots(nrows=3,ncols=3,figsize=(12,12))\n\nax1 = axes[1][1] # row 1, column 1 (note that rows and columns start at 0)\n\nax1.scatter(x = female_data['hindfoot_length'],y = female_data['weight'], s=12)\nax1.set_xlabel('Hindfoot length [mm]')\nax1.set_ylabel('Weight [g]')\nax1.set_title('Females')\n\nax2 = axes[2][0] # row 2, column 0\nax2.scatter(x = male_data['hindfoot_length'],y = male_data['weight'], s=12, color='orange')\nax2.set_xlabel('Hindfoot length [mm]')\nax2.set_ylabel('Weight [g]')\nax2.set_title('Males')\n\nText(0.5, 1.0, 'Males')\n\n\n\n\n\n\n\n\n\nLet’s see now another way to have multiple plots in the same figure. Once we defined a Figure and an Axes, we can add other Axes to our Figure using fig.add_axes([left,bottom,length,height]). The position and size of the new Axes is passed in a list: [left edge, bottom edge, length, and height]. The left edge and bottom edge are scaled from 0 to 1, so that 0.5 corresponds to the center of the Figure. For example, the list of coordinates [0.5,0.5,0.33,0.33] will locate the bottom-left corner of our additional Axis at the very center of the figure. The new plot will be as wide as ~1/3 of the length of the figure and as high as ~1/3 of the height of the figure.\n\n# prepare a matplotlib figure\nfig, ax1 = plt.subplots(figsize=(6,6))\nax1.scatter(x = female_data['hindfoot_length'],y = female_data['weight'], s=12)\nax1.set_xlabel('Hindfoot length [mm]')\nax1.set_ylabel('Weight [g]')\nax1.set_title('Females')\n\nax2 = fig.add_axes([0.65, 0.65, 0.25, 0.25])\nax2.scatter(x = male_data['hindfoot_length'],y = male_data['weight'], s=12, color='orange')\nax2.set_title('Males')\n\nText(0.5, 1.0, 'Males')\n\n\n\n\n\n\n\n\n\nSummarizing, we have seen that with plt.subplots() you can customize all sort of plot features:\n\nthe size of the figure in inches or cm\nthe number of plots to display in the figure\nhow to arrange them in rows and columns\nwhether the subplots need to share the same axis, etc.\n\nTo know more about subplots and take full control or your visualization, check out the Matplotlib documentation.",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-science-with-pandas-4.html#plotting-grouped-data",
    "href": "data-science-with-pandas-4.html#plotting-grouped-data",
    "title": "9  Data Visualization",
    "section": "9.6 Plotting grouped data",
    "text": "9.6 Plotting grouped data\nYou have already seen how to group data stored in pandas DataFrames in chapter 7. If we want to check if the pattern we observed in previously plotted data is the same in males and females without creating separate data objects (female_data and male_data), we can use a for loop and the groupby method, e.g. to overlay two plots on top of each other in the same Axes object.\n\nfig, ax = plt.subplots()\n\nfor i, group in list(surveys.groupby('sex')):\n    ax.scatter(group['hindfoot_length'], group['weight'], alpha=0.5, label = group['sex'].iloc[0])\n    \nax.legend(title='sex')\nax.grid()\nax.set_xlabel(\"Hindfoot length\", fontsize=14)\nax.set_ylabel(\"Weight\", fontsize=14)\n\nText(0, 0.5, 'Weight')\n\n\n\n\n\n\n\n\n\nBy using ax.scatter inside the for loop, the two sets of points end up in the same Axes. The points plotted in the second iteration (in this case the orange ones) are plotted on top of the previous iteration. Colors are assigned automatically, but you can assign this yourself (e.g. by creating a list of colors for the iterations of the for loop). alpha=0.5 is used to make the plotted dots semi-transparent (alpha can be set to any value between 0 and 1 to make the points less or more transparent). We add a legend explaining colors by calling the legend() method on the Axes object after the data is plotted.\n\nExercise 11\nNow go to the Jupyter Dashboard in your internet browser and continue with exercise 11.\n\n\n9.6.1 Saving your plot\nOnce you produced your plot you will probably need to share it in different media (website, papers, slide show, etc). To do that, we need to save our plot in a specific format. Once you have defined a Figure, you can do that with a single line of code:\n\nfig.savefig('MyFigure.png', dpi=300)\n\nThe Figure method savefig() will save your figure into a file. The first argument of the function is the name of the output file. Matplotlib will automatically recognize the format of the output file from its name. If you will specify only a name without extention, you can indicate the format explicitly with the parameter format. We also need to specify the dpi (dots per inch), i.e. the quality of the picture. The quality of the picture depends on the media we want to use. 200 dpi is good enough for visualization on a screen.\n\n\n9.6.2 What’s next?\nAs we mentioned in the introduction, matplotlib library is huge and you can customize every single little feature of your plot. With matplotlib you can also create animations and 3D plots. Now that you know the basics of plotting data, have a look at the matplotlib gallery to check out the huge variety of plots you can generate with matplotlib and try to reproduce those plots yourself.\nOk, Matplotlib is great, but is there anything else out there for data visualization? Yes, there is, seaborn! Seaborn is a Python data visualization library based on matplotlib and inspired by ggplot (R users will know this). It provides a high-level interface for drawing attractive and informative statistical graphics. Seaborn syntax is almost identical to Matplotlib, checkout seaborn gallery here.\n\nimport seaborn as sns\nsns.scatterplot(x=surveys['hindfoot_length'],y=surveys['weight'])\n\n\n\n\n\n\n\n\nWell, if you are reading this it means you managed to get to the end of this Introduction to Python course, congratulations! Go to What is next after this course for tips on how to get started with Python in your own project! Success!",
    "crumbs": [
      "Working With Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "solutions/morning_exercises_solutions.html",
    "href": "solutions/morning_exercises_solutions.html",
    "title": "10  Morning Exercises: Python fundamentals",
    "section": "",
    "text": "10.0.1 Exercise 0\n\nTry to run the code below. Why is there no output?\n\n\nx = 6\napple = \"apple\"\n\nAnswer: Only the output of the last command is printed and the last command is a variable assignment which does not produce any printed output.\n\nChange the code cell in such a way that the variable names and values are printed as output. You can type the variable names in the print statements, but do not type the value.\n\n\nx = 6\napple = \"apple\"\nprint(\"the value of x is\", x)\nprint(\"the value of apple is\", apple)\n\nthe value of x is 6\nthe value of apple is apple\n\n\n\n\n10.0.2 Exercise 1\n\nCalculate: One plus five and divide the sum by nine amd assign the result of the calculation to a variable.\nTest if the result is larger than one. (tip: output of the cell should be True or False)\nRound off the result to one decimal. Use the function round. (tip: use ? round or you internet search engine for information about how to use the function)\n\n\ns = 1+5/9\n\n\ns &gt; 1\n\nTrue\n\n\n\nround(s, 1)\n\n1.6\n\n\n\n\n10.0.3 Exercise 2\nPredict the results of each of the following comparisons first, then run the cell below: (double click this cell to edit)\n5 == 5 answer: True, 5 is equal to 5\nnot 3 &gt; 2 answer: False, not inverts True to False\nTrue == 'True' answer: False, The first True is of type bool and does not equal the second ‘True’ of type string\n\nprint(5 == 5)\nprint(not 3 &gt; 2)\nprint(True == 'True')\n\nTrue\nFalse\nFalse\n\n\n\n\n10.0.4 Exercise 3: variables and operators\nEvaluate the statements below. What do you think will be the output of python? Do you agree with python?\n\n1 + 1 answer: 2, 1 and 1 are added and the result is printed as output.\n1 + True answer: 2, in Python True`` is equal to1`\n1 + \"one\" answer: error, an integer cannot be added to a string\n\n\nprint(1 + 1)\n\n2\n\n\n\nprint(1 + True)\n\n2\n\n\n\nprint(1 + \"one\")\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\nNote, that the boolean variables True and False are turned into numbers when used in calculations. True becomes 1 and False becomes 0.\n\n\n10.0.5 Exercise 4\nMeet Ann, Bob, Chloe, and Dan. 1. Create a list with their names. Save the list as “name”.\n\nHow old are Ann, Bob, Chloe, and Dan? You decide! Design a numeric list with their respective ages. Save it as “age”.\nWhat is their average age? (Use the function sum to sum up their cumulative ages, you can use len(age) to get the number of elements in a list)\n\n\nname = ['Ann', 'Bob', 'Chloe', 'Dan']\nage = [25, 28, 36, 30]\nprint('average age is', sum(age)/len(age))\n\naverage age is 29.75\n\n\n\n\n10.0.6 Exercise 5\n\nReturn only the first number in the list age.\nReturn the 2nd and 4th name in your list name.\nReturn only ages under 30 from your list age.\nReturn the age of “Chloe” from the list age.\n\n\nage[0]\n\n25\n\n\n\n[name[1], name[3]]\n\n['Bob', 'Dan']\n\n\n\nnew_age=[]\nfor a in age:\n    if a &lt; 30:\n        new_age.append(a)\n\nnew_age\n\n# without the bonus:\n# for a in age: \n#     if a &lt; 30: \n#         print(a)\n\n[25, 28]\n\n\n\nage[name.index(\"Chloe\")]\n\n36\n\n\nNote, we can only fetch Claire’s age from age since the two lists are sorted accordingly. Dictionaries, see next Section, will give us a better way of mapping from one value to another.\n\n\n10.0.7 Exercise 6\n\nCreate a new dictionary that contains the keys: ‘name’, ‘age’ and ‘country’ and their respective values (you can make up the values yourself)..\nPrint the values of the dictionary to the screen\nReassign the value of key ‘age’ to 24\nPrint the values of the dictionary to the screen again to see if the value has changed.\n\n\nmy_dict = {'name': 'Jill', 'age': 39, 'country': 'Netherlands'}\n\n\nmy_dict.values()\n\ndict_values(['Jill', 39, 'Netherlands'])\n\n\n\nmy_dict['age'] = 24\n\n\nmy_dict.values()\n\ndict_values(['Jill', 24, 'Netherlands'])\n\n\n\n\n10.0.8 Exercise 7\nCreate an if statement that tests whether a number is even or odd, and saves the classification in a variable called number_class.\nHint, you can use the % operator (aka modulo operator). If necessary, you can try the operator to see what it does in the cell below or search the web.\n\nnumber = 5\n\nif number % 2 == 0:\n    number_class = \"even\"\nelse:\n    number_class = \"odd\"\n    \nprint(number_class)\n\nodd\n\n\n\n\n10.0.9 Exercise 8\nTurn the if statement from the last exercise into a function. Let the user provide the value for number, and return the number_class.\n\ndef even_or_odd(number):\n    if number % 2 == 0:\n        number_class = \"even\"\n    else:\n        number_class = \"odd\"\n    return number_class\n\n\n# Run this code to test your function above\nnumber = 5\nprint(number, \"is\", even_or_odd(number))\n\n5 is odd\n\n\n\n\n10.0.10 Exercise 9\nUse the function above to determine whether the numbers between 1 and 10 are even or odd.\nHint: the range function might be helpful.\n\nfor number in range(1,10,1):\n    print(number, \"is\", even_or_odd(number))\n\n1 is odd\n2 is even\n3 is odd\n4 is even\n5 is odd\n6 is even\n7 is odd\n8 is even\n9 is odd",
    "crumbs": [
      "Exercises solutions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Morning Exercises: Python fundamentals</span>"
    ]
  },
  {
    "objectID": "solutions/afternoon_exercises_solutions.html",
    "href": "solutions/afternoon_exercises_solutions.html",
    "title": "11  Afternoon Exercises: Working with data",
    "section": "",
    "text": "import pandas as pd\nsurveys_df = pd.read_csv('../../course_materials/data/surveys.csv') # in your notebook the path should be 'data/surveys.csv'\n\n\n11.0.1 Exercise 0\nType the following commands and check the outputs. Can you tell what each command does? What is the difference between commands with and without parenthesis?\nsurveys_df.shape # Answer: the dimensions of the dataframe\nsurveys_df.columns # Answer: the column names of the dataframe\nsurveys_df.index # Answer: the index (row labels) of the dataframe\nsurveys_df.dtypes # Answer: the data types of each column\nsurveys_df.head(&lt;try_various_integers_here&gt;) # Answer: the first n rows of the dataframe\nsurveys_df.tail(&lt;try_various_integers_here&gt;) # Answer: the last n rows of the dataframe\n\n\n11.0.2 Exercise 1\nPerform some basic statistics on the weight column. For practical reasons, it can be useful to first create a variable weight that contains the just the weight column. It will make the code look a bit cleaner. Can you tell what each method listed below does? Look at our explorative plot, do the statistics make sense?\nweight =surveys_df['weight'] # Answer: creates a new variable that contains the weight column\nweight.min() # Answer: the minimum value of the weight column\nweight.max() # Answer: the maximum value of the weight column\nweight.mean() # Answer: the mean value of the weight column\nweight.std() # Answer: the standard deviation of the weight column\nweight.count() # Answer: the number of non-NaN values in the weight column\n\n\n11.0.3 Exercise 2\n\nSwap the order of column names in surveys_df[['plot_id', 'species_id']]\nRepeat one of the column names like surveys_df[['plot_id', 'plot_id', 'species_id']]. What do the results look like and why?\n\n\nAnswer: the column names are repeated and the data is displayed twice. Column names do not have to be unique.\n\n\nWhich error occurs in surveys_df['plot_id', 'species_id'] and why?\n\n\nAnswer: KeyError: (‘plot_id’, ‘species_id’). The column names are not in a list. We need double square brackets to select multiple columns.\n\n\nWhich error occurs in surveys_df['speciess']?\n\n\nAnswer: KeyError: ‘speciess’. The column name does not exist. Typo.\n\n\nprint(surveys_df[['species_id', 'plot_id']])\n\n      species_id  plot_id\n0             NL        2\n1             NL        3\n2             DM        2\n3             DM        7\n4             DM        3\n...          ...      ...\n35544         AH       15\n35545         AH       15\n35546         RM       10\n35547         DO        7\n35548        NaN        5\n\n[35549 rows x 2 columns]\n\n\n\nsurveys_df[['plot_id', 'plot_id', 'species_id']]\n\n\n\n\n\n\n\n\n\nplot_id\nplot_id\nspecies_id\n\n\n\n\n0\n2\n2\nNL\n\n\n1\n3\n3\nNL\n\n\n2\n2\n2\nDM\n\n\n3\n7\n7\nDM\n\n\n4\n3\n3\nDM\n\n\n...\n...\n...\n...\n\n\n35544\n15\n15\nAH\n\n\n35545\n15\n15\nAH\n\n\n35546\n10\n10\nRM\n\n\n35547\n7\n7\nDO\n\n\n35548\n5\n5\nNaN\n\n\n\n\n35549 rows × 3 columns\n\n\n\n\n\nsurveys_df['plot_id', 'species_id'] \n\nKeyError: ('plot_id', 'species_id')\n\n\n\nsurveys_df['speciess']\n\nKeyError: 'speciess'\n\n\n\n\n11.0.4 Exercise 3\nWhat happens when you call:\n\nsurveys_df[0:1] Answer: shows the first row of the dataframe\nsurveys_df[:4] Answer: shows the first 4 rows of the dataframe from index 0 to index 3\nsurveys_df[:-1] Answer: shows all rows of the dataframe except the last row\n\n\nsurveys_df[0:1]\nsurveys_df[:4] \nsurveys_df[:-1] \n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35543\n35544\n12\n31\n2002\n15\nUS\nNaN\nNaN\nNaN\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n\n\n35548 rows × 9 columns\n\n\n\n\n\n\n11.0.5 Exercise 4\n\nFind all entries in the column sex which do not contain an M or a F.\nCreate a new DataFrame that contains only observations that are of sex male or female and where weight values are greater than 0.\n\n\ndf = surveys_df[(surveys_df['sex'] != 'M') & (surveys_df['sex'] != 'F')]\nprint(\"Number of rows not female or male:\", len(df))\n\nNumber of rows not female or male: 2511\n\n\n\ndf = surveys_df[((surveys_df['sex'] == 'M') | (surveys_df['sex'] == 'F')) & surveys_df['weight'] &gt; 0]\n\n\n\n11.0.6 Exercise 5: Putting it all together\n\nClean the column sex (leave out samples of which we do not know whether they are male or female) and save the result as a new dataframe clean_df.\nFill undefined weight values with the mean of all valid weights in surveys_df.\nCalculate the average weight of that new DataFrame clean_df\n\n\n# Step 1\n# sex is 'F' or 'M'. The `|` means or.\nclean_df = surveys_df[(surveys_df['sex']=='F') | (surveys_df['sex']=='M')]\n# Alternative solution: select columns where 'not' sex is null. The `~` means not.\nclean_df = surveys_df[~(surveys_df['sex'].isnull())]\n\n# Step 2\nclean_df.weight.fillna(surveys_df['weight'].mean())\n\n# Step 3\nprint(\"Average weight of surveys_df:\", surveys_df['weight'].mean())\nprint(\"Average weight of clean_df:\", clean_df['weight'].mean())\n\nAverage weight of surveys_df: 42.672428212991356\nAverage weight of clean_df: 42.60316325896464\n\n\n\n\n11.0.7 Exercise 6\nLet’s see in which plots animals get more food. Calculate the average weight per plot! Complete the code below.\n\ngrouped_data = surveys_df.groupby(\"plot_id\")\ngrouped_data['weight'].mean()\n\nplot_id\n1     51.822911\n2     52.251688\n3     32.654386\n4     47.928189\n5     40.947802\n6     36.738893\n7     20.663009\n8     47.758001\n9     51.432358\n10    18.541219\n11    43.451757\n12    49.496169\n13    40.445660\n14    46.277199\n15    27.042578\n16    24.585417\n17    47.889593\n18    40.005922\n19    21.105166\n20    48.665303\n21    24.627794\n22    54.146379\n23    19.634146\n24    43.679167\nName: weight, dtype: float64\n\n\n\n\n11.0.8 Exercise 7\nSee below a more complex grouping example. Investigate the group keys and row indexes for this more complex grouping example. Why are there more than 48 groups? Answer: nan values are not ignored when grouping. Calculate the average weight per group. What happened to the third group and why does it not turn up in our statistics? Answer: the third group contains only nan values and is therefore not included in the statistics.\n\ngrouped_data = surveys_df.groupby(['sex', 'plot_id'])\nprint(len(grouped_data.groups))\ngrouped_data.groups.keys()\n\n72\n\n\ndict_keys([('F', 1), ('F', 2), ('F', 3), ('F', 4), ('F', 5), ('F', 6), ('F', 7), ('F', 8), ('F', 9), ('F', 10), ('F', 11), ('F', 12), ('F', 13), ('F', 14), ('F', 15), ('F', 16), ('F', 17), ('F', 18), ('F', 19), ('F', 20), ('F', 21), ('F', 22), ('F', 23), ('F', 24), ('M', 1), ('M', 2), ('M', 3), ('M', 4), ('M', 5), ('M', 6), ('M', 7), ('M', 8), ('M', 9), ('M', 10), ('M', 11), ('M', 12), ('M', 13), ('M', 14), ('M', 15), ('M', 16), ('M', 17), ('M', 18), ('M', 19), ('M', 20), ('M', 21), ('M', 22), ('M', 23), ('M', 24), (nan, 1), (nan, 2), (nan, 3), (nan, 4), (nan, 5), (nan, 6), (nan, 7), (nan, 8), (nan, 9), (nan, 10), (nan, 11), (nan, 12), (nan, 13), (nan, 14), (nan, 15), (nan, 16), (nan, 17), (nan, 18), (nan, 19), (nan, 20), (nan, 21), (nan, 22), (nan, 23), (nan, 24)])\n\n\n\ngrouped_data['weight'].mean()\n\nsex  plot_id\nF    1          46.311138\n     2          52.561845\n     3          31.215349\n     4          46.818824\n     5          40.974806\n     6          36.352288\n     7          20.006135\n     8          45.623011\n     9          53.618469\n     10         17.094203\n     11         43.515075\n     12         49.831731\n     13         40.524590\n     14         47.355491\n     15         26.670236\n     16         25.810427\n     17         48.176201\n     18         36.963514\n     19         21.978599\n     20         52.624406\n     21         25.974832\n     22         53.647059\n     23         20.564417\n     24         47.914405\nM    1          55.950560\n     2          51.391382\n     3          34.163241\n     4          48.888119\n     5          40.708551\n     6          36.867388\n     7          21.194719\n     8          49.641372\n     9          49.519309\n     10         19.971223\n     11         43.366197\n     12         48.909710\n     13         40.097754\n     14         45.159378\n     15         27.523691\n     16         23.811321\n     17         47.558853\n     18         43.546952\n     19         20.306878\n     20         44.197279\n     21         22.772622\n     22         54.572531\n     23         18.941463\n     24         39.321503\nName: weight, dtype: float64\n\n\n\n\n11.0.9 Exercise 8\nWould it make sense to group our data frame by the column weight? Why or why not?\n\n# In real life nearly every sample has a unique value. So nearly every sample would \n# be placed in an own group.\n# In our training data you can see that there are quite some values for weight. So\n# usually it is not a good idea to categorise (group) data on such values.\nprint(\"Number of rows:\", len(surveys_df))\nprint(len(surveys_df['weight'].unique())) #includes nan\nprint(len(surveys_df.groupby(['weight']).groups)) #does not include nan\n\nNumber of rows: 35549\n256\n255\n\n\n\n\n11.0.10 Exercise 9\nIn the given example of vertical concatenation, you concatenated two DataFrames with the same columns. What would happen if the two DataFrames to concatenate have different column number and names?\n\nCreate a new DataFrame using the last 10 rows of the species DataFrame (species_df);\nConcatenate vertically surveys_df_sub_first10 and your just created DataFrame;\nPrint the concatenated DataFrame info on the screen. How may rows does it have? What happened to the columns? Explain why you get this result.\n\n\nspecies_df = pd.read_csv(\"../../course_materials/data/species.csv\")\nspecies_df_sub_last10 = species_df.tail(10)\n\nsurveys_df_sub_first10 = surveys_df.head(10)\nvert_concat = pd.concat([surveys_df_sub_first10, species_df_sub_last10], axis=0)\n\nvert_concat\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\ngenus\nspecies\ntaxa\n\n\n\n\n0\n1.0\n7.0\n16.0\n1977.0\n2.0\nNL\nM\n32.0\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2.0\n7.0\n16.0\n1977.0\n3.0\nNL\nM\n33.0\nNaN\nNaN\nNaN\nNaN\n\n\n2\n3.0\n7.0\n16.0\n1977.0\n2.0\nDM\nF\n37.0\nNaN\nNaN\nNaN\nNaN\n\n\n3\n4.0\n7.0\n16.0\n1977.0\n7.0\nDM\nM\n36.0\nNaN\nNaN\nNaN\nNaN\n\n\n4\n5.0\n7.0\n16.0\n1977.0\n3.0\nDM\nM\n35.0\nNaN\nNaN\nNaN\nNaN\n\n\n5\n6.0\n7.0\n16.0\n1977.0\n1.0\nPF\nM\n14.0\nNaN\nNaN\nNaN\nNaN\n\n\n6\n7.0\n7.0\n16.0\n1977.0\n2.0\nPE\nF\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\n8.0\n7.0\n16.0\n1977.0\n1.0\nDM\nM\n37.0\nNaN\nNaN\nNaN\nNaN\n\n\n8\n9.0\n7.0\n16.0\n1977.0\n1.0\nDM\nF\n34.0\nNaN\nNaN\nNaN\nNaN\n\n\n9\n10.0\n7.0\n16.0\n1977.0\n6.0\nPF\nF\n20.0\nNaN\nNaN\nNaN\nNaN\n\n\n44\nNaN\nNaN\nNaN\nNaN\nNaN\nSS\nNaN\nNaN\nNaN\nSpermophilus\nspilosoma\nRodent\n\n\n45\nNaN\nNaN\nNaN\nNaN\nNaN\nST\nNaN\nNaN\nNaN\nSpermophilus\ntereticaudus\nRodent\n\n\n46\nNaN\nNaN\nNaN\nNaN\nNaN\nSU\nNaN\nNaN\nNaN\nSceloporus\nundulatus\nReptile\n\n\n47\nNaN\nNaN\nNaN\nNaN\nNaN\nSX\nNaN\nNaN\nNaN\nSigmodon\nsp.\nRodent\n\n\n48\nNaN\nNaN\nNaN\nNaN\nNaN\nUL\nNaN\nNaN\nNaN\nLizard\nsp.\nReptile\n\n\n49\nNaN\nNaN\nNaN\nNaN\nNaN\nUP\nNaN\nNaN\nNaN\nPipilo\nsp.\nBird\n\n\n50\nNaN\nNaN\nNaN\nNaN\nNaN\nUR\nNaN\nNaN\nNaN\nRodent\nsp.\nRodent\n\n\n51\nNaN\nNaN\nNaN\nNaN\nNaN\nUS\nNaN\nNaN\nNaN\nSparrow\nsp.\nBird\n\n\n52\nNaN\nNaN\nNaN\nNaN\nNaN\nZL\nNaN\nNaN\nNaN\nZonotrichia\nleucophrys\nBird\n\n\n53\nNaN\nNaN\nNaN\nNaN\nNaN\nZM\nNaN\nNaN\nNaN\nZenaida\nmacroura\nBird\n\n\n\n\n\n\n\n\nWe get a total of 20 rows and 12 columns. The original dataframes together had a total of 13 columns. As they both have a column species_id, this one is collapsed. All other columns are padded with NaN values. We expect 20 rows, as we are putting two DataFrames of 10 rows one after the other. The padding of the columns happens because these two DataFrames do not have the same column names. To keep all the information that was in the original DataFrames, the padding of columns that occur in only one of the two is necessary.\n\n\n11.0.11 Exercise 10\n\nLooking at the inner_join example, can you explain how much of each of the two DataFrames is missing from the result?\n\nNow consider the other types of joins, for each one, can you predict the number of rows and the contents of the resulting DataFrame, based on the diagrams in the picture?\n\nFor the outer join;\nFor the left join;\nFor the right join.\nFrom the left DataFrame, three rows are not included in the inner_join DataFrame. This is because they have a value in their species_id column that is not present in the right DataFrame. From the right DataFrame, the information of 18 rows is missing from the result. This is because their species_id column has a value that does not occur in the left DataFrame. Note that the information from the two rows that are represented in the result is duplicated a number of times, as their species_id value occurs multiple times in the left DataFrame.\nThe result has a total of 28 rows. You may notice that the first seven of those rows are the same as the result of the inner join, followed by the three rows from the left DataFrame that are not represented in the inner join, and finally, the 18 rows from the right DataFrame that are not represented in the inner join. This makes for a total of 7 + 3 + 18 = 28 rows. The outer join preserves all the information from both the left and right DataFrames.\n\n\n# 2.\nleft_df = surveys_df.head(10)\nright_df = species_df.head(20)\nouter_join = pd.merge(left_df, right_df, left_on='species_id', right_on='species_id', \n                      how='outer')\nouter_join\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\ngenus\nspecies\ntaxa\n\n\n\n\n0\n1.0\n7.0\n16.0\n1977.0\n2.0\nNL\nM\n32.0\nNaN\nNeotoma\nalbigula\nRodent\n\n\n1\n2.0\n7.0\n16.0\n1977.0\n3.0\nNL\nM\n33.0\nNaN\nNeotoma\nalbigula\nRodent\n\n\n2\n3.0\n7.0\n16.0\n1977.0\n2.0\nDM\nF\n37.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n3\n4.0\n7.0\n16.0\n1977.0\n7.0\nDM\nM\n36.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n4\n5.0\n7.0\n16.0\n1977.0\n3.0\nDM\nM\n35.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n5\n8.0\n7.0\n16.0\n1977.0\n1.0\nDM\nM\n37.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n6\n9.0\n7.0\n16.0\n1977.0\n1.0\nDM\nF\n34.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n7\n6.0\n7.0\n16.0\n1977.0\n1.0\nPF\nM\n14.0\nNaN\nNaN\nNaN\nNaN\n\n\n8\n10.0\n7.0\n16.0\n1977.0\n6.0\nPF\nF\n20.0\nNaN\nNaN\nNaN\nNaN\n\n\n9\n7.0\n7.0\n16.0\n1977.0\n2.0\nPE\nF\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10\nNaN\nNaN\nNaN\nNaN\nNaN\nAB\nNaN\nNaN\nNaN\nAmphispiza\nbilineata\nBird\n\n\n11\nNaN\nNaN\nNaN\nNaN\nNaN\nAH\nNaN\nNaN\nNaN\nAmmospermophilus\nharrisi\nRodent\n\n\n12\nNaN\nNaN\nNaN\nNaN\nNaN\nAS\nNaN\nNaN\nNaN\nAmmodramus\nsavannarum\nBird\n\n\n13\nNaN\nNaN\nNaN\nNaN\nNaN\nBA\nNaN\nNaN\nNaN\nBaiomys\ntaylori\nRodent\n\n\n14\nNaN\nNaN\nNaN\nNaN\nNaN\nCB\nNaN\nNaN\nNaN\nCampylorhynchus\nbrunneicapillus\nBird\n\n\n15\nNaN\nNaN\nNaN\nNaN\nNaN\nCM\nNaN\nNaN\nNaN\nCalamospiza\nmelanocorys\nBird\n\n\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nCQ\nNaN\nNaN\nNaN\nCallipepla\nsquamata\nBird\n\n\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nCS\nNaN\nNaN\nNaN\nCrotalus\nscutalatus\nReptile\n\n\n18\nNaN\nNaN\nNaN\nNaN\nNaN\nCT\nNaN\nNaN\nNaN\nCnemidophorus\ntigris\nReptile\n\n\n19\nNaN\nNaN\nNaN\nNaN\nNaN\nCU\nNaN\nNaN\nNaN\nCnemidophorus\nuniparens\nReptile\n\n\n20\nNaN\nNaN\nNaN\nNaN\nNaN\nCV\nNaN\nNaN\nNaN\nCrotalus\nviridis\nReptile\n\n\n21\nNaN\nNaN\nNaN\nNaN\nNaN\nDO\nNaN\nNaN\nNaN\nDipodomys\nordii\nRodent\n\n\n22\nNaN\nNaN\nNaN\nNaN\nNaN\nDS\nNaN\nNaN\nNaN\nDipodomys\nspectabilis\nRodent\n\n\n23\nNaN\nNaN\nNaN\nNaN\nNaN\nDX\nNaN\nNaN\nNaN\nDipodomys\nsp.\nRodent\n\n\n24\nNaN\nNaN\nNaN\nNaN\nNaN\nEO\nNaN\nNaN\nNaN\nEumeces\nobsoletus\nReptile\n\n\n25\nNaN\nNaN\nNaN\nNaN\nNaN\nGS\nNaN\nNaN\nNaN\nGambelia\nsilus\nReptile\n\n\n26\nNaN\nNaN\nNaN\nNaN\nNaN\nNX\nNaN\nNaN\nNaN\nNeotoma\nsp.\nRodent\n\n\n27\nNaN\nNaN\nNaN\nNaN\nNaN\nOL\nNaN\nNaN\nNaN\nOnychomys\nleucogaster\nRodent\n\n\n\n\n\n\n\n\n\nTen rows. The resulting DataFrame closely resembles the original left DataFrame, but with information from the right DataFrame added to it, where applicable.\n\n\n# 3.\nleft_join = pd.merge(left_df, right_df, left_on='species_id', right_on='species_id', \n                     how='left')\nleft_join\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\ngenus\nspecies\ntaxa\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\nNeotoma\nalbigula\nRodent\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\nNeotoma\nalbigula\nRodent\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n5\n6\n7\n16\n1977\n1\nPF\nM\n14.0\nNaN\nNaN\nNaN\nNaN\n\n\n6\n7\n7\n16\n1977\n2\nPE\nF\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\n8\n7\n16\n1977\n1\nDM\nM\n37.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n8\n9\n7\n16\n1977\n1\nDM\nF\n34.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n9\n10\n7\n16\n1977\n6\nPF\nF\n20.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n25 rows. The resulting DataFrame closely resembles the original right DataFrame, but with information from the left DataFrame added to it, where applicable. Note that rows from the right DataFrame that have multiple matching rows in the left DataFrame are duplicated.\n\n\n# 4.\nright_join = pd.merge(left_df, right_df, left_on='species_id', right_on='species_id', \n                      how='right')\nright_join\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\ngenus\nspecies\ntaxa\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nAB\nNaN\nNaN\nNaN\nAmphispiza\nbilineata\nBird\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nAH\nNaN\nNaN\nNaN\nAmmospermophilus\nharrisi\nRodent\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nAS\nNaN\nNaN\nNaN\nAmmodramus\nsavannarum\nBird\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nBA\nNaN\nNaN\nNaN\nBaiomys\ntaylori\nRodent\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nCB\nNaN\nNaN\nNaN\nCampylorhynchus\nbrunneicapillus\nBird\n\n\n5\nNaN\nNaN\nNaN\nNaN\nNaN\nCM\nNaN\nNaN\nNaN\nCalamospiza\nmelanocorys\nBird\n\n\n6\nNaN\nNaN\nNaN\nNaN\nNaN\nCQ\nNaN\nNaN\nNaN\nCallipepla\nsquamata\nBird\n\n\n7\nNaN\nNaN\nNaN\nNaN\nNaN\nCS\nNaN\nNaN\nNaN\nCrotalus\nscutalatus\nReptile\n\n\n8\nNaN\nNaN\nNaN\nNaN\nNaN\nCT\nNaN\nNaN\nNaN\nCnemidophorus\ntigris\nReptile\n\n\n9\nNaN\nNaN\nNaN\nNaN\nNaN\nCU\nNaN\nNaN\nNaN\nCnemidophorus\nuniparens\nReptile\n\n\n10\nNaN\nNaN\nNaN\nNaN\nNaN\nCV\nNaN\nNaN\nNaN\nCrotalus\nviridis\nReptile\n\n\n11\n3.0\n7.0\n16.0\n1977.0\n2.0\nDM\nF\n37.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n12\n4.0\n7.0\n16.0\n1977.0\n7.0\nDM\nM\n36.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n13\n5.0\n7.0\n16.0\n1977.0\n3.0\nDM\nM\n35.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n14\n8.0\n7.0\n16.0\n1977.0\n1.0\nDM\nM\n37.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n15\n9.0\n7.0\n16.0\n1977.0\n1.0\nDM\nF\n34.0\nNaN\nDipodomys\nmerriami\nRodent\n\n\n16\nNaN\nNaN\nNaN\nNaN\nNaN\nDO\nNaN\nNaN\nNaN\nDipodomys\nordii\nRodent\n\n\n17\nNaN\nNaN\nNaN\nNaN\nNaN\nDS\nNaN\nNaN\nNaN\nDipodomys\nspectabilis\nRodent\n\n\n18\nNaN\nNaN\nNaN\nNaN\nNaN\nDX\nNaN\nNaN\nNaN\nDipodomys\nsp.\nRodent\n\n\n19\nNaN\nNaN\nNaN\nNaN\nNaN\nEO\nNaN\nNaN\nNaN\nEumeces\nobsoletus\nReptile\n\n\n20\nNaN\nNaN\nNaN\nNaN\nNaN\nGS\nNaN\nNaN\nNaN\nGambelia\nsilus\nReptile\n\n\n21\n1.0\n7.0\n16.0\n1977.0\n2.0\nNL\nM\n32.0\nNaN\nNeotoma\nalbigula\nRodent\n\n\n22\n2.0\n7.0\n16.0\n1977.0\n3.0\nNL\nM\n33.0\nNaN\nNeotoma\nalbigula\nRodent\n\n\n23\nNaN\nNaN\nNaN\nNaN\nNaN\nNX\nNaN\nNaN\nNaN\nNeotoma\nsp.\nRodent\n\n\n24\nNaN\nNaN\nNaN\nNaN\nNaN\nOL\nNaN\nNaN\nNaN\nOnychomys\nleucogaster\nRodent\n\n\n\n\n\n\n\n\n\n\n11.0.12 Exercise 11\nTime to play with plots! Create a multiplot following these instructions: - Using the matplotlib.pyplot function subplots(), create a single figure (10x10 inches) with four subplots organized in two rows and two columns; - In the top row plot hindfoot_length VS weight for female and male in two different plots with two different colors; - In the bottom row, plot the same data of the top row, but using data collected before (left plot) and after (right plot) 1990; - Give to each plot an appropriate descriptive title and customize the plot labels. \nFeel free to use the DataFrame plot method or plt.scatter function to plot data points, but be awave that, in any case, the first thing to do is creating Figure and Axes. EXTRA: The four plots have same x and y axes spanning the same range. Can you remove the space between the four plots? Try it!\n\nfrom matplotlib import pyplot as plt\n\nfig, axes = plt.subplots(2,2,figsize=(10,10)) # prepare a matplotlib figure\n\n# Top left plot, male data\nsurveys_df[surveys_df['sex']=='M'].plot(\"hindfoot_length\", \"weight\", kind=\"scatter\", ax=axes[0][0], color='blue')\naxes[0][0].set_title('Male data')\naxes[0][0].grid()\n\n# Top right plot, female data\nsurveys_df[surveys_df['sex']=='F'].plot(\"hindfoot_length\", \"weight\", kind=\"scatter\", ax=axes[0][1], color='red')\naxes[0][1].set_title('Female data')\naxes[0][1].grid()\n\nyear = 2000\n\n# Bottom left plot, male data\nsurveys_df[(surveys_df['sex']=='M') & (surveys_df['year'] &lt; year)].plot(\"hindfoot_length\", \"weight\", kind=\"scatter\", ax=axes[1][0], color='blue')\naxes[1][0].set_title(f'Male data (&lt; {year})')\naxes[1][0].grid()\n\n# Bottom right plot, male data\nsurveys_df[(surveys_df['sex']=='F') & (surveys_df['year'] &gt;= year)].plot(\"hindfoot_length\", \"weight\", kind=\"scatter\", ax=axes[1][1], color='red')\naxes[1][1].set_title(f'Female data (&gt;= {year})')\naxes[1][1].grid()\n\n# Removing individual plot labels\nfor i in range(2):\n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].set_ylabel('')\n\n# Initializing figure labels\nfig.supxlabel(\"Hindfoot Length [cm]\",fontsize=14)\nfig.supylabel(\"Weight [Kg]\",fontsize=14)\nfig.suptitle('Scatter plot of weight versus hindfoot length', fontsize=15)\n\nText(0.5, 0.98, 'Scatter plot of weight versus hindfoot length')",
    "crumbs": [
      "Exercises solutions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Afternoon Exercises: Working with data</span>"
    ]
  },
  {
    "objectID": "what-next.html",
    "href": "what-next.html",
    "title": "What Next?",
    "section": "",
    "text": "Libraries:\nThis is a short list of popular Python libraries that might be useful for your next steps:\nThe documentation sites of these libraries often contain many examples and tutorials to get you started.\nMany more libraries for working with various data types (text, images, audio, video, etc.) and a wide variety of other tasks/applications (web scraping, parallel computing, large language models, image classification, speech recognition, etc.) are available.",
    "crumbs": [
      "What Next?"
    ]
  },
  {
    "objectID": "what-next.html#libraries",
    "href": "what-next.html#libraries",
    "title": "What Next?",
    "section": "",
    "text": "numpy for numerical computing\npandas for data analysis\nscipy and statsmodels for statistics\nscikit-learn for machine learning\ngeopandas and rasterio for geospatial data\nmatplotlib and seaborn for plotting",
    "crumbs": [
      "What Next?"
    ]
  },
  {
    "objectID": "what-next.html#courses",
    "href": "what-next.html#courses",
    "title": "What Next?",
    "section": "Courses:",
    "text": "Courses:\n\nBest practices for writing reproducible code by UU RDM support\nVarious intermediate and advanced programming courses by the eScience Center\nSoftware Carpentries\nPython for Data Science and Data Wrangling (online book)\nPython Data Science Handbook (online book)",
    "crumbs": [
      "What Next?"
    ]
  },
  {
    "objectID": "what-next.html#find-us",
    "href": "what-next.html#find-us",
    "title": "What Next?",
    "section": "Find us:",
    "text": "Find us:\nWe are happy to help you in your journey to master Python and use it in your own projects. You can find us at the following places:\n\nWalk-In Hours, come with your questions!\nProgramming Cafe, informal meetup about programming. Bring your laptop, work on your project and get help when you need it!\nUU Research Engineers\nUU RDM consultants",
    "crumbs": [
      "What Next?"
    ]
  }
]